
var json_documents = [
  
    {
      "id": "0",
      "title": "API",
      "content": "Performance Analyzer API Performance Analyzer uses a single HTTP method and URI for all requests:  GET &amp;lt;endpoint&amp;gt;:9600/_opendistro/_performanceanalyzer/metrics Note the use of port 9600. Then you provide parameters for metrics, aggregations, dimensions, and nodes (optional):  ?metrics=&amp;lt;metrics&amp;gt;&amp;amp;agg=&amp;lt;aggregations&amp;gt;&amp;amp;dim=&amp;lt;dimensions&amp;gt;&amp;amp;nodes=all&quot; For a full list of metrics, see Metrics reference. Performance Analyzer updates its data every five seconds. If you create a custom client, we recommend using that same interval for calls to the API.  Sample request  GET _opendistro/_performanceanalyzer/metrics?metrics=Latency,CPU_Utilization&amp;amp;agg=avg,max&amp;amp;dim=ShardID&amp;amp;nodes=all Sample response  { &quot;keHlhQbbTpm1BYicficEQg&quot;: {   &quot;timestamp&quot;: 1554940530000,   &quot;data&quot;: {   &quot;fields&quot;: [{  &quot;name&quot;: &quot;ShardID&quot;,  &quot;type&quot;: &quot;VARCHAR&quot;   },   {  &quot;name&quot;: &quot;Latency&quot;,  &quot;type&quot;: &quot;DOUBLE&quot;   },   {  &quot;name&quot;: &quot;CPU_Utilization&quot;,  &quot;type&quot;: &quot;DOUBLE&quot;   }   ],   &quot;records&quot;: [   [  null,  null,  0.012552206029147535   ],   [  &quot;1&quot;,  4.8,  0.0009780939762972104   ]   ]   } }, &quot;bHdpbMJZTs-TKtZro2SmYA&quot;: {   &quot;timestamp&quot;: 1554940530000,   &quot;data&quot;: {   &quot;fields&quot;: [{  &quot;name&quot;: &quot;ShardID&quot;,  &quot;type&quot;: &quot;VARCHAR&quot;   },   {  &quot;name&quot;: &quot;Latency&quot;,  &quot;type&quot;: &quot;DOUBLE&quot;   },   {  &quot;name&quot;: &quot;CPU_Utilization&quot;,  &quot;type&quot;: &quot;DOUBLE&quot;   }   ],   &quot;records&quot;: [   [  null,  18.2,  0.011966493817311527   ],   [  &quot;1&quot;,  14.8,  0.0007670829370071493   ]   ]   } } } In this case, each top-level object represents a node. The API returns names and data types for the metrics and dimensions that you specified, along with values from five seconds ago and current values (if different). Null values represent inactivity during that time period. ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/pa/api/",
      "relUrl": "/docs/pa/api/"
    },
  
    {
      "id": "1",
      "title": "API",
      "content": "Alerting API Use the alerting API to programmatically manage monitors and alerts. Create monitor Update monitor Get monitor Monitor stats Delete monitor Search monitors Run monitor Acknowledge alert Create destination Update destination Delete destination   Create monitor  Request  POST _opendistro/_alerting/monitors { &quot;name&quot;: &quot;test-monitor&quot;, &quot;type&quot;: &quot;monitor&quot;, &quot;enabled&quot;: true, &quot;schedule&quot;: {   &quot;period&quot;: {   &quot;interval&quot;: 1,   &quot;unit&quot;: &quot;MINUTES&quot;   } }, &quot;inputs&quot;: [{   &quot;search&quot;: {   &quot;indices&quot;: [&quot;movies&quot;],   &quot;query&quot;: {   &quot;size&quot;: 0,   &quot;aggregations&quot;: {},   &quot;query&quot;: {  &quot;bool&quot;: {  &quot;filter&quot;: {  &quot;range&quot;: {    &quot;@timestamp&quot;: {    &quot;gte&quot;: &quot;||-1h&quot;,    &quot;lte&quot;: &quot;&quot;,    &quot;format&quot;: &quot;epoch_millis&quot;    }  }  }  }   }   }   } }], &quot;triggers&quot;: [] } Sample response  { &quot;_id&quot;: &quot;ZO5TOmkBi6P41RVAsNwh&quot;, &quot;_version&quot;: 1, &quot;monitor&quot;: {   &quot;type&quot;: &quot;monitor&quot;,   &quot;name&quot;: &quot;test-monitor&quot;,   &quot;enabled&quot;: true,   &quot;enabled_time&quot;: 1551461756953,   &quot;schedule&quot;: {   &quot;period&quot;: {   &quot;interval&quot;: 1,   &quot;unit&quot;: &quot;MINUTES&quot;   }   },   &quot;inputs&quot;: [{   &quot;search&quot;: {   &quot;indices&quot;: [&quot;movies&quot;],   &quot;query&quot;: {  &quot;size&quot;: 0,  &quot;query&quot;: {  &quot;bool&quot;: {  &quot;filter&quot;: [{    &quot;range&quot;: {    &quot;@timestamp&quot;: {    &quot;from&quot;: &quot;||-1h&quot;,    &quot;to&quot;: &quot;&quot;,    &quot;include_lower&quot;: true,    &quot;include_upper&quot;: true,    &quot;format&quot;: &quot;epoch_millis&quot;,    &quot;boost&quot;: 1.0    }    }  }],  &quot;adjust_pure_negative&quot;: true,  &quot;boost&quot;: 1.0  }  },  &quot;aggregations&quot;: {}   }   }   }],   &quot;triggers&quot;: [],   &quot;last_update_time&quot;: 1551461756953 } }   Update monitor  When you update a monitor, include the current version number as a parameter. Open Distro for Elasticsearch increments the version number automatically (see the sample response).  Request  PUT _opendistro/_alerting/monitors/&amp;lt;monitor_id&amp;gt; { &quot;type&quot;: &quot;monitor&quot;, &quot;name&quot;: &quot;test-monitor&quot;, &quot;enabled&quot;: true, &quot;enabled_time&quot;: 1551466220455, &quot;schedule&quot;: {   &quot;period&quot;: {   &quot;interval&quot;: 1,   &quot;unit&quot;: &quot;MINUTES&quot;   } }, &quot;inputs&quot;: [{   &quot;search&quot;: {   &quot;indices&quot;: [   &quot;*&quot;   ],   &quot;query&quot;: {   &quot;query&quot;: {  &quot;match_all&quot;: {  &quot;boost&quot;: 1  }   }   }   } }], &quot;triggers&quot;: [{   &quot;id&quot;: &quot;StaeOmkBC25HCRGmL_y-&quot;,   &quot;name&quot;: &quot;test-trigger&quot;,   &quot;severity&quot;: &quot;1&quot;,   &quot;condition&quot;: {   &quot;script&quot;: {   &quot;source&quot;: &quot;return true&quot;,   &quot;lang&quot;: &quot;painless&quot;   }   },   &quot;actions&quot;: [{   &quot;name&quot;: &quot;test-action&quot;,   &quot;destination_id&quot;: &quot;RtaaOmkBC25HCRGm0fxi&quot;,   &quot;subject_template&quot;: {   &quot;source&quot;: &quot;My Message Subject&quot;,   &quot;lang&quot;: &quot;mustache&quot;   },   &quot;message_template&quot;: {   &quot;source&quot;: &quot;This is my message body.&quot;,   &quot;lang&quot;: &quot;mustache&quot;   }   }] }], &quot;last_update_time&quot;: 1551466639295 } Sample response  { &quot;_id&quot;: &quot;Q9aXOmkBC25HCRGmzfw-&quot;, &quot;_version&quot;: 4, &quot;monitor&quot;: {   &quot;type&quot;: &quot;monitor&quot;,   &quot;name&quot;: &quot;test-monitor&quot;,   &quot;enabled&quot;: true,   &quot;enabled_time&quot;: 1551466220455,   &quot;schedule&quot;: {   &quot;period&quot;: {   &quot;interval&quot;: 1,   &quot;unit&quot;: &quot;MINUTES&quot;   }   },   &quot;inputs&quot;: [{   &quot;search&quot;: {   &quot;indices&quot;: [  &quot;*&quot;   ],   &quot;query&quot;: {  &quot;query&quot;: {  &quot;match_all&quot;: {  &quot;boost&quot;: 1  }  }   }   }   }],   &quot;triggers&quot;: [{   &quot;id&quot;: &quot;StaeOmkBC25HCRGmL_y-&quot;,   &quot;name&quot;: &quot;test-trigger&quot;,   &quot;severity&quot;: &quot;1&quot;,   &quot;condition&quot;: {   &quot;script&quot;: {  &quot;source&quot;: &quot;return true&quot;,  &quot;lang&quot;: &quot;painless&quot;   }   },   &quot;actions&quot;: [{   &quot;name&quot;: &quot;test-action&quot;,   &quot;destination_id&quot;: &quot;RtaaOmkBC25HCRGm0fxi&quot;,   &quot;subject_template&quot;: {  &quot;source&quot;: &quot;My Message Subject&quot;,  &quot;lang&quot;: &quot;mustache&quot;   },   &quot;message_template&quot;: {  &quot;source&quot;: &quot;This is my message body.&quot;,  &quot;lang&quot;: &quot;mustache&quot;   }   }]   }],   &quot;last_update_time&quot;: 1551466761596 } }   Get monitor  Request  GET _opendistro/_alerting/monitors/&amp;lt;monitor_id&amp;gt; Sample response  { &quot;_id&quot;: &quot;Q9aXOmkBC25HCRGmzfw-&quot;, &quot;_version&quot;: 3, &quot;monitor&quot;: {   &quot;type&quot;: &quot;monitor&quot;,   &quot;name&quot;: &quot;test-monitor&quot;,   &quot;enabled&quot;: true,   &quot;enabled_time&quot;: 1551466220455,   &quot;schedule&quot;: {   &quot;period&quot;: {   &quot;interval&quot;: 1,   &quot;unit&quot;: &quot;MINUTES&quot;   }   },   &quot;inputs&quot;: [{   &quot;search&quot;: {   &quot;indices&quot;: [  &quot;*&quot;   ],   &quot;query&quot;: {  &quot;query&quot;: {  &quot;match_all&quot;: {  &quot;boost&quot;: 1  }  }   }   }   }],   &quot;triggers&quot;: [{   &quot;id&quot;: &quot;StaeOmkBC25HCRGmL_y-&quot;,   &quot;name&quot;: &quot;test-trigger&quot;,   &quot;severity&quot;: &quot;1&quot;,   &quot;condition&quot;: {   &quot;script&quot;: {  &quot;source&quot;: &quot;return true&quot;,  &quot;lang&quot;: &quot;painless&quot;   }   },   &quot;actions&quot;: [{   &quot;name&quot;: &quot;test-action&quot;,   &quot;destination_id&quot;: &quot;RtaaOmkBC25HCRGm0fxi&quot;,   &quot;subject_template&quot;: {  &quot;source&quot;: &quot;My Message Subject&quot;,  &quot;lang&quot;: &quot;mustache&quot;   },   &quot;message_template&quot;: {  &quot;source&quot;: &quot;This is my message body.&quot;,  &quot;lang&quot;: &quot;mustache&quot;   }   }]   }],   &quot;last_update_time&quot;: 1551466639295 } }   Monitor stats  Returns statistics about the alerting feature. Use _opendistro/_alerting/stats to find node IDs and metrics. Then you can drill down using those values.  Request  GET _opendistro/_alerting/stats GET _opendistro/_alerting/stats/&amp;lt;metric&amp;gt; GET _opendistro/_alerting/&amp;lt;node-id&amp;gt;/stats GET _opendistro/_alerting/&amp;lt;node-id&amp;gt;/stats/&amp;lt;metric&amp;gt; Sample response  { &quot;_nodes&quot;: {   &quot;total&quot;: 9,   &quot;successful&quot;: 9,   &quot;failed&quot;: 0 }, &quot;cluster_name&quot;: &quot;475300751431:alerting65-dont-delete&quot;, &quot;opendistro.scheduled_jobs.enabled&quot;: true, &quot;scheduled_job_index_exists&quot;: true, &quot;scheduled_job_index_status&quot;: &quot;green&quot;, &quot;nodes_on_schedule&quot;: 9, &quot;nodes_not_on_schedule&quot;: 0, &quot;nodes&quot;: {   &quot;qWcbKbb-TVyyI-Q7VSeOqA&quot;: {   &quot;name&quot;: &quot;qWcbKbb&quot;,   &quot;schedule_status&quot;: &quot;green&quot;,   &quot;roles&quot;: [   &quot;MASTER&quot;   ],   &quot;job_scheduling_metrics&quot;: {   &quot;last_full_sweep_time_millis&quot;: 207017,   &quot;full_sweep_on_time&quot;: true   },   &quot;jobs_info&quot;: {}   },   &quot;Do-DX9ZcS06Y9w1XbSJo1A&quot;: {   &quot;name&quot;: &quot;Do-DX9Z&quot;,   &quot;schedule_status&quot;: &quot;green&quot;,   &quot;roles&quot;: [   &quot;DATA&quot;,   &quot;INGEST&quot;   ],   &quot;job_scheduling_metrics&quot;: {   &quot;last_full_sweep_time_millis&quot;: 230516,   &quot;full_sweep_on_time&quot;: true   },   &quot;jobs_info&quot;: {}   },   &quot;n5phkBiYQfS5I0FDzcqjZQ&quot;: {   &quot;name&quot;: &quot;n5phkBi&quot;,   &quot;schedule_status&quot;: &quot;green&quot;,   &quot;roles&quot;: [   &quot;MASTER&quot;   ],   &quot;job_scheduling_metrics&quot;: {   &quot;last_full_sweep_time_millis&quot;: 228406,   &quot;full_sweep_on_time&quot;: true   },   &quot;jobs_info&quot;: {}   },   &quot;Tazzo8cQSY-g3vOjgYYLzA&quot;: {   &quot;name&quot;: &quot;Tazzo8c&quot;,   &quot;schedule_status&quot;: &quot;green&quot;,   &quot;roles&quot;: [   &quot;DATA&quot;,   &quot;INGEST&quot;   ],   &quot;job_scheduling_metrics&quot;: {   &quot;last_full_sweep_time_millis&quot;: 211722,   &quot;full_sweep_on_time&quot;: true   },   &quot;jobs_info&quot;: {   &quot;i-wsFmkB8NzS6aXjQSk0&quot;: {  &quot;last_execution_time&quot;: 1550864912882,  &quot;running_on_time&quot;: true   }   }   },   &quot;Nyf7F8brTOSJuFPXw6CnpA&quot;: {   &quot;name&quot;: &quot;Nyf7F8b&quot;,   &quot;schedule_status&quot;: &quot;green&quot;,   &quot;roles&quot;: [   &quot;DATA&quot;,   &quot;INGEST&quot;   ],   &quot;job_scheduling_metrics&quot;: {   &quot;last_full_sweep_time_millis&quot;: 223300,   &quot;full_sweep_on_time&quot;: true   },   &quot;jobs_info&quot;: {   &quot;NbpoFmkBeSe-hD59AKgE&quot;: {  &quot;last_execution_time&quot;: 1550864928354,  &quot;running_on_time&quot;: true   },   &quot;-LlLFmkBeSe-hD59Ydtb&quot;: {  &quot;last_execution_time&quot;: 1550864732727,  &quot;running_on_time&quot;: true   },   &quot;pBFxFmkBNXkgNmTBaFj1&quot;: {  &quot;last_execution_time&quot;: 1550863325024,  &quot;running_on_time&quot;: true   },   &quot;hfasEmkBNXkgNmTBrvIW&quot;: {  &quot;last_execution_time&quot;: 1550862000001,  &quot;running_on_time&quot;: true   }   }   },   &quot;oOdJDIBVT5qbbO3d8VLeEw&quot;: {   &quot;name&quot;: &quot;oOdJDIB&quot;,   &quot;schedule_status&quot;: &quot;green&quot;,   &quot;roles&quot;: [   &quot;DATA&quot;,   &quot;INGEST&quot;   ],   &quot;job_scheduling_metrics&quot;: {   &quot;last_full_sweep_time_millis&quot;: 227570,   &quot;full_sweep_on_time&quot;: true   },   &quot;jobs_info&quot;: {   &quot;4hKRFmkBNXkgNmTBKjYX&quot;: {  &quot;last_execution_time&quot;: 1550864806101,  &quot;running_on_time&quot;: true   }   }   },   &quot;NRDG6JYgR8m0GOZYQ9QGjQ&quot;: {   &quot;name&quot;: &quot;NRDG6JY&quot;,   &quot;schedule_status&quot;: &quot;green&quot;,   &quot;roles&quot;: [   &quot;MASTER&quot;   ],   &quot;job_scheduling_metrics&quot;: {   &quot;last_full_sweep_time_millis&quot;: 227652,   &quot;full_sweep_on_time&quot;: true   },   &quot;jobs_info&quot;: {}   },   &quot;URMrXRz3Tm-CB72hlsl93Q&quot;: {   &quot;name&quot;: &quot;URMrXRz&quot;,   &quot;schedule_status&quot;: &quot;green&quot;,   &quot;roles&quot;: [   &quot;DATA&quot;,   &quot;INGEST&quot;   ],   &quot;job_scheduling_metrics&quot;: {   &quot;last_full_sweep_time_millis&quot;: 231048,   &quot;full_sweep_on_time&quot;: true   },   &quot;jobs_info&quot;: {   &quot;m7uKFmkBeSe-hD59jplP&quot;: {  &quot;running_on_time&quot;: true   }   }   },   &quot;eXgt1k9oTRCLmx2HBGElUw&quot;: {   &quot;name&quot;: &quot;eXgt1k9&quot;,   &quot;schedule_status&quot;: &quot;green&quot;,   &quot;roles&quot;: [   &quot;DATA&quot;,   &quot;INGEST&quot;   ],   &quot;job_scheduling_metrics&quot;: {   &quot;last_full_sweep_time_millis&quot;: 229234,   &quot;full_sweep_on_time&quot;: true   },   &quot;jobs_info&quot;: {   &quot;wWkFFmkBc2NG-PeLntxk&quot;: {  &quot;running_on_time&quot;: true   },   &quot;3usNFmkB8NzS6aXjO1Gs&quot;: {  &quot;last_execution_time&quot;: 1550863959848,  &quot;running_on_time&quot;: true   }   }   } } }   Delete monitor  Request  DELETE _opendistro/_alerting/monitors/&amp;lt;monitor_id&amp;gt; Sample response  { &quot;_index&quot;: &quot;.opendistro-scheduled-jobs&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;OYAHOmgBl3cmwnqZl_yH&quot;, &quot;_version&quot;: 2, &quot;result&quot;: &quot;deleted&quot;, &quot;forced_refresh&quot;: true, &quot;_shards&quot;: {   &quot;total&quot;: 2,   &quot;successful&quot;: 2,   &quot;failed&quot;: 0 }, &quot;_seq_no&quot;: 11, &quot;_primary_term&quot;: 1 }   Search monitors  Request  GET _opendistro/_alerting/monitors/_search { &quot;query&quot;: {   &quot;match&quot; : {   &quot;monitor.name&quot;: &quot;my-monitor-name&quot;   } } } Sample response  { &quot;took&quot;: 17, &quot;timed_out&quot;: false, &quot;_shards&quot;: {   &quot;total&quot;: 5,   &quot;successful&quot;: 5,   &quot;skipped&quot;: 0,   &quot;failed&quot;: 0 }, &quot;hits&quot;: {   &quot;total&quot;: 1,   &quot;max_score&quot;: 0.6931472,   &quot;hits&quot;: [{   &quot;_index&quot;: &quot;.opendistro-scheduled-jobs&quot;,   &quot;_type&quot;: &quot;_doc&quot;,   &quot;_id&quot;: &quot;eGQi7GcBRS7-AJEqfAnr&quot;,   &quot;_score&quot;: 0.6931472,   &quot;_source&quot;: {   &quot;type&quot;: &quot;monitor&quot;,   &quot;name&quot;: &quot;my-monitor-name&quot;,   &quot;enabled&quot;: true,   &quot;enabled_time&quot;: 1545854942426,   &quot;schedule&quot;: {  &quot;period&quot;: {  &quot;interval&quot;: 1,  &quot;unit&quot;: &quot;MINUTES&quot;  }   },   &quot;inputs&quot;: [{  &quot;search&quot;: {  &quot;indices&quot;: [  &quot;*&quot;  ],  &quot;query&quot;: {  &quot;size&quot;: 0,  &quot;query&quot;: {    &quot;bool&quot;: {    &quot;filter&quot;: [{    &quot;range&quot;: {   &quot;@timestamp&quot;: {   &quot;from&quot;: &quot;||-1h&quot;,   &quot;to&quot;: &quot;&quot;,   &quot;include_lower&quot;: true,   &quot;include_upper&quot;: true,   &quot;format&quot;: &quot;epoch_millis&quot;,   &quot;boost&quot;: 1   }    }    }],    &quot;adjust_pure_negative&quot;: true,    &quot;boost&quot;: 1    }  },  &quot;aggregations&quot;: {}  }  }   }],   &quot;triggers&quot;: [{  &quot;id&quot;: &quot;Sooi7GcB53a0ewuj_6MH&quot;,  &quot;name&quot;: &quot;Over&quot;,  &quot;severity&quot;: &quot;1&quot;,  &quot;condition&quot;: {  &quot;script&quot;: {  &quot;source&quot;: &quot;_ctx.results[0].hits.total &amp;gt; 400000&quot;,  &quot;lang&quot;: &quot;painless&quot;  }  },  &quot;actions&quot;: []   }],   &quot;last_update_time&quot;: 1545854975758   }   }] } }   Run monitor  You can add the optional ?dryrun=true parameter to the URL to show the results of a run without actions sending any message.  Request  POST _opendistro/_alerting/monitors/&amp;lt;monitor_id&amp;gt;/_execute Sample response  { &quot;monitor_name&quot;: &quot;logs&quot;, &quot;period_start&quot;: 1547161872322, &quot;period_end&quot;: 1547161932322, &quot;error&quot;: null, &quot;trigger_results&quot;: {   &quot;Sooi7GcB53a0ewuj_6MH&quot;: {   &quot;name&quot;: &quot;Over&quot;,   &quot;triggered&quot;: true,   &quot;error&quot;: null,   &quot;action_results&quot;: {}   } } }   Acknowledge alert  To get the alert ID, query the .opendistro-alerts index. See Alerting indices. You can acknowledge any number of alerts in one call. If the alert is already in an ERROR, COMPLETED, or ACKNOWLEDGED state, it will appear in the failed array.  Request  POST _opendistro/_alerting/monitors/&amp;lt;monitor-id&amp;gt;/_acknowledge/alerts { &quot;alerts&quot;: [&quot;bn0_PmgBoCvkhulGF2K8&quot;] } Sample response  { &quot;success&quot;: [   &quot;bn0_PmgBoCvkhulGF2K8&quot; ], &quot;failed&quot;: [] }   Create destination  Request  POST _opendistro/_alerting/destinations { &quot;name&quot;: &quot;my-destination&quot;, &quot;type&quot;: &quot;slack&quot;, &quot;slack&quot;: {   &quot;url&quot;: &quot;http://www.example.com&quot; } } Sample response  { &quot;_id&quot;: &quot;nO-yFmkB8NzS6aXjJdiI&quot;, &quot;_version&quot;: 1, &quot;destination&quot;: {   &quot;type&quot;: &quot;slack&quot;,   &quot;name&quot;: &quot;my-destination&quot;,   &quot;last_update_time&quot;: 1550863967624,   &quot;slack&quot;: {   &quot;url&quot;: &quot;http://www.example.com&quot;   } } }   Update destination  Request  PUT _opendistro/_alerting/destinations/&amp;lt;destination-id&amp;gt; { &quot;name&quot;: &quot;my-updated-destination&quot;, &quot;type&quot;: &quot;slack&quot;, &quot;slack&quot;: {   &quot;url&quot;: &quot;http://www.example.com&quot; } } Sample response  { &quot;_id&quot;: &quot;pe-1FmkB8NzS6aXjqvVY&quot;, &quot;_version&quot;: 4, &quot;destination&quot;: {   &quot;type&quot;: &quot;slack&quot;,   &quot;name&quot;: &quot;my-updated-destination&quot;,   &quot;last_update_time&quot;: 1550864289375,   &quot;slack&quot;: {   &quot;url&quot;: &quot;http://www.example.com&quot;   } } }   Delete destination  Request  _opendistro/_alerting/destinations/&amp;lt;destination-id&amp;gt; Sample response  { &quot;_index&quot;: &quot;.opendistro-alerting-config&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;Zu-zFmkB8NzS6aXjLeBI&quot;, &quot;_version&quot;: 2, &quot;result&quot;: &quot;deleted&quot;, &quot;forced_refresh&quot;: true, &quot;_shards&quot;: {   &quot;total&quot;: 2,   &quot;successful&quot;: 2,   &quot;failed&quot;: 0 }, &quot;_seq_no&quot;: 8, &quot;_primary_term&quot;: 1 }  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/alerting/api/",
      "relUrl": "/docs/alerting/api/"
    },
  
    {
      "id": "2",
      "title": "API",
      "content": "API The Security plugin REST API lets you programmatically create and manage users, roles, role mappings, and action groups. Access control for the API Read-only and hidden resources Action groups  Get action group   Get action groups   Delete action group   Create action group   Patch action group   Patch action groups  Users  Get user   Get users   Delete user   Create user   Patch user   Patch users  Roles  Get role   Get roles   Delete role   Create role   Patch role   Patch roles  Role mappings  Get role mapping   Get role mappings   Delete role mapping   Create role mapping   Patch role mapping   Patch role mappings  Authentication  Get authentication details  Cache  Flush cache  Health  Health check  Access control for the API  Just like Elasticsearch permissions, you control access to the Security plugin REST API using roles. Specify roles in elasticsearch.yml:  opendistro_security.restapi.roles_enabled: [&quot;&amp;lt;role&amp;gt;&quot;, ...] These roles can now access all APIs. To prevent access to certain APIs:  opendistro_security.restapi.endpoints_disabled.&amp;lt;role&amp;gt;.&amp;lt;endpoint&amp;gt;: [&quot;&amp;lt;method&amp;gt;&quot;, ...] Possible values for endpoint are:   ACTIONGROUPS ROLES ROLESMAPPING INTERNALUSERS CONFIG CACHE LICENSE SYSTEMINFO Possible values for method are:   GET PUT POST DELETE PATCH For example, the following line grants the my-role role access to the PUT, POST, and DELETE methods of the ROLES URIs.  opendistro_security.restapi.endpoints_disabled.my-role.ROLES: [&quot;PUT&quot;, &quot;POST&quot;, &quot;DELETE&quot;] Read-only and hidden resources  You can mark users, role, role mappings, and action groups as read-only in their respective configuration files. Resources that have this flag set to true can’t be changed using the REST API and are marked as reserved in Kibana.  To mark a resource readonly, add the following flag:  kibana_user: readonly: true Likewise, you can mark users, role, role mappings, and action groups as hidden. Resources that have this flag set to true are not returned by the REST API and cannot be changed nor deleted:  kibana_user: hidden: true Hidden resources are read-only by definition.  To add or remove these flags, you need to modify plugins/opendistro_security/securityconfig/internal_users.yml and run plugins/opendistro_security/tools/securityadmin.sh.  Action groups  Get action group  Retrieves one action group.  Request  GET _opendistro/_security/api/actiongroups/&amp;lt;action-group&amp;gt; Sample response  { &quot;SEARCH&quot; : [ &quot;indices:data/read/search*&quot;, &quot;indices:data/read/msearch*&quot;, &quot;SUGGEST&quot; ] } Get action groups  Retrieves all action groups.  Request  GET _opendistro/_security/api/actiongroups/ Sample response  asdf Delete action group  Request  DELETE _opendistro/_security/api/actiongroups/&amp;lt;action-group&amp;gt; Sample response  { &quot;status&quot;:&quot;OK&quot;, &quot;message&quot;:&quot;actiongroup SEARCH deleted.&quot; } Create action group  Creates or replaces the specified action group.  Request  PUT _opendistro/_security/api/actiongroups/&amp;lt;action-group&amp;gt; { &quot;permissions&quot;: [&quot;indices:data/read/search*&quot;, &quot;indices:data/read/msearch*&quot;, &quot;SUGGEST&quot; ] } Sample response  { &quot;status&quot;:&quot;CREATED&quot;, &quot;message&quot;:&quot;action group SEARCH created&quot; } Patch action group  Updates individual attributes of an action group.  Request  PATCH _opendistro/_security/api/actiongroups/&amp;lt;action-group&amp;gt; [ {   &quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/permissions&quot;, &quot;value&quot;: [&quot;indices:admin/create&quot;, &quot;indices:admin/mapping/put&quot;] } ] Sample response  { &quot;status&quot;:&quot;OK&quot;, &quot;message&quot;:&quot;actiongroup SEARCH deleted.&quot; } Patch action groups  Creates, updates, or deletes multiple action groups in a single call.  Request  PATCH _opendistro/_security/api/actiongroups [ {   &quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/CREATE_INDEX&quot;, &quot;value&quot;: [&quot;indices:admin/create&quot;, &quot;indices:admin/mapping/put&quot;] }, {   &quot;op&quot;: &quot;delete&quot;, &quot;path&quot;: &quot;/CRUD&quot; } ] Sample response  { &quot;status&quot;:&quot;OK&quot;, &quot;message&quot;:&quot;actiongroup SEARCH deleted.&quot; }   Users  These calls let you create, update, and delete internal users. If you use an external authentication backend, you probably don’t need to worry about internal users.  Get user  Request  GET _opendistro/_security/api/internalusers/&amp;lt;username&amp;gt; Sample response  { &quot;kirk&quot;: {   &quot;hash&quot;: &quot;&quot;,   &quot;roles&quot;: [ &quot;captains&quot;, &quot;starfleet&quot; ],   &quot;attributes&quot;: { &quot;attribute1&quot;: &quot;value1&quot;, &quot;attribute2&quot;: &quot;value2&quot;,   } } } Get users  Request  GET _opendistro/_security/api/internalusers/ Sample response  { &quot;kirk&quot;: {   &quot;hash&quot;: &quot;&quot;,   &quot;roles&quot;: [ &quot;captains&quot;, &quot;starfleet&quot; ],   &quot;attributes&quot;: { &quot;attribute1&quot;: &quot;value1&quot;, &quot;attribute2&quot;: &quot;value2&quot;,   } } } Delete user  Request  DELETE _opendistro/_security/api/internalusers/&amp;lt;username&amp;gt; Sample response  { &quot;status&quot;:&quot;OK&quot;, &quot;message&quot;:&quot;user kirk deleted.&quot; } Create user  Creates or replaces the specified user. You must specify either password (plain text) or hash (the hashed user password). If you specify password, the Security plugin automatically hashes the password before storing it.  Request  PUT _opendistro/_security/api/internalusers/&amp;lt;username&amp;gt; { &quot;password&quot;: &quot;kirk&quot;, &quot;roles&quot;: [&quot;captains&quot;, &quot;starfleet&quot;],  &quot;attributes&quot;: {  &quot;attribute1&quot;: &quot;value1&quot;,  &quot;attribute2&quot;: &quot;value2&quot;,  } } Sample response  { &quot;status&quot;:&quot;CREATED&quot;, &quot;message&quot;:&quot;User kirk created&quot; } Patch user  Updates individual attributes of an internal user.  Request  PATCH _opendistro/_security/api/internalusers/&amp;lt;username&amp;gt; [ {   &quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/roles&quot;, &quot;value&quot;: [&quot;klingons&quot;] }, {   &quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/attributes&quot;, &quot;value&quot;: {&quot;newattribute&quot;: &quot;newvalue&quot;} } ] Sample response  { &quot;status&quot;:&quot;CREATED&quot;, &quot;message&quot;:&quot;User kirk created&quot; } Patch users  Creates, updates, or deletes multiple internal users in a single call.  Request  PATCH _opendistro/_security/api/internalusers [ {   &quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spock&quot;, &quot;value&quot;: { &quot;password&quot;: &quot;testpassword1&quot;, &quot;roles&quot;: [&quot;testrole1&quot;] } }, {   &quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/worf&quot;, &quot;value&quot;: { &quot;password&quot;: &quot;testpassword2&quot;, &quot;roles&quot;: [&quot;testrole2&quot;] } }, {   &quot;op&quot;: &quot;delete&quot;, &quot;path&quot;: &quot;/riker&quot; } ] Sample response  { &quot;status&quot;:&quot;CREATED&quot;, &quot;message&quot;:&quot;User kirk created&quot; }   Roles  Get role  Retrieves one role.  Request  GET _opendistro/_security/api/roles/&amp;lt;role&amp;gt; Sample response  { &quot;role_starfleet&quot; : {   &quot;indices&quot; : {   &quot;pub*&quot; : {   &quot;*&quot; : [ &quot;READ&quot; ],   &quot;_dls_&quot;: &quot;{  &quot;bool &quot;: {  &quot;must_not &quot;: {  &quot;match &quot;: {  &quot;Designation &quot;:  &quot;CEO &quot;  }}}}&quot;,   &quot;_fls_&quot;: [  &quot;Designation&quot;,  &quot;FirstName&quot;,  &quot;LastName&quot;,  &quot;Salary&quot;   ]   }   } } } Get roles  Retrieves all roles.  Request  GET _opendistro/_security/api/roles/ Sample response  { &quot;role_starfleet&quot; : {   &quot;indices&quot; : {   &quot;pub*&quot; : {   &quot;*&quot; : [ &quot;READ&quot; ],   &quot;_dls_&quot;: &quot;{  &quot;bool &quot;: {  &quot;must_not &quot;: {  &quot;match &quot;: {  &quot;Designation &quot;:  &quot;CEO &quot;  }}}}&quot;,   &quot;_fls_&quot;: [  &quot;Designation&quot;,  &quot;FirstName&quot;,  &quot;LastName&quot;,  &quot;Salary&quot;   ]   }   } } } Delete role  Request  DELETE _opendistro/_security/api/roles/&amp;lt;role&amp;gt; Sample response  { &quot;status&quot;:&quot;OK&quot;, &quot;message&quot;:&quot;role role_starfleet deleted.&quot; } Create role  Creates or replaces the specified role.  Request  PUT _opendistro/_security/api/roles/&amp;lt;role&amp;gt; { &quot;cluster&quot; : [ &quot;*&quot; ], &quot;indices&quot; : {   &quot;pub*&quot; : {   &quot;*&quot; : [ &quot;READ&quot; ],   &quot;_dls_&quot;: &quot;{  &quot;bool &quot;: {  &quot;must_not &quot;: {  &quot;match &quot;: {  &quot;Designation &quot;:  &quot;CEO &quot;}}}}&quot;,   &quot;_fls_&quot;: [&quot;field1&quot;, &quot;field2&quot;]   } }, &quot;tenants&quot;: {   &quot;tenant1&quot;: &quot;RW&quot;,   &quot;tenant2&quot;: &quot;RO&quot; } } Sample response  { &quot;status&quot;:&quot;OK&quot;, &quot;message&quot;:&quot;role role_starfleet created.&quot; } Patch role  Updates individual attributes of a role.  Request  PATCH _opendistro/_security/api/roles/&amp;lt;role&amp;gt; [ {   &quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/indices/public/_fls_&quot;, &quot;value&quot;: [&quot;field1&quot;] }, {   &quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/indices/public/_dls_&quot; }  ] Sample response  { &quot;status&quot;:&quot;OK&quot;, &quot;message&quot;:&quot;role role_starfleet created.&quot; } Patch roles  Creates, updates, or deletes multiple roles in a single call.  Request  PATCH _opendistro/_security/api/roles [ {   &quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/klingons&quot;,  &quot;value&quot;: { &quot;indices&quot; : { &quot;klingonindex&quot; : { &quot;*&quot; : [ &quot;READ&quot; ] }  } } }, {   &quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/romulans&quot;,  &quot;value&quot;: { &quot;indices&quot; : { &quot;romulansindex&quot; : { &quot;*&quot; : [ &quot;READ&quot; ] }  } } } ] Sample response  { &quot;status&quot;:&quot;OK&quot;, &quot;message&quot;:&quot;role role_starfleet created.&quot; }   Role mappings  Get role mapping  Retrieves one role mapping.  Request  GET _opendistro/_security/api/rolesmapping/&amp;lt;role&amp;gt; Sample response  { &quot;role_starfleet&quot; : {   &quot;backendroles&quot; : [ &quot;starfleet&quot;, &quot;captains&quot;, &quot;defectors&quot;, &quot;cn=ldaprole,ou=groups,dc=example,dc=com&quot; ],   &quot;hosts&quot; : [ &quot;*.starfleetintranet.com&quot; ],   &quot;users&quot; : [ &quot;worf&quot; ] } } Get role mappings  Retrieves all role mappings.  Request  GET _opendistro/_security/api/rolesmapping Sample response  { &quot;role_starfleet&quot; : {   &quot;backendroles&quot; : [ &quot;starfleet&quot;, &quot;captains&quot;, &quot;defectors&quot;, &quot;cn=ldaprole,ou=groups,dc=example,dc=com&quot; ],   &quot;hosts&quot; : [ &quot;*.starfleetintranet.com&quot; ],   &quot;users&quot; : [ &quot;worf&quot; ] } } Delete role mapping  Request  DELETE _opendistro/_security/api/rolesmapping/&amp;lt;role&amp;gt; Sample response  { &quot;status&quot;: &quot;OK&quot;, &quot;message&quot;: &quot;&#39;my-role&#39; deleted.&quot; } Create role mapping  Creates or replaces the specified role mapping.  Request  PUT _opendistro/_security/api/rolesmapping/&amp;lt;role&amp;gt; { &quot;backendroles&quot; : [ &quot;starfleet&quot;, &quot;captains&quot;, &quot;defectors&quot;, &quot;cn=ldaprole,ou=groups,dc=example,dc=com&quot; ], &quot;hosts&quot; : [ &quot;*.starfleetintranet.com&quot; ], &quot;users&quot; : [ &quot;worf&quot; ] } Sample response  { &quot;status&quot;: &quot;CREATED&quot;, &quot;message&quot;: &quot;&#39;my-role&#39; created.&quot; } Patch role mapping  Updates individual attributes of a role mapping.  Request  PATCH _opendistro/_security/api/rolesmapping/&amp;lt;role&amp;gt; [ {   &quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/users&quot;, &quot;value&quot;: [&quot;myuser&quot;] }, {   &quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/backendroles&quot;, &quot;value&quot;: [&quot;mybackendrole&quot;] } ] Sample response  { &quot;status&quot;: &quot;OK&quot;, &quot;message&quot;: &quot;&#39;my-role&#39; updated.&quot; } Patch role mappings  Creates or updates multiple role mappings in a single call.  Request  PATCH _opendistro/_security/api/rolesmapping [ {   &quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/human_resources&quot;, &quot;value&quot;: { &quot;users&quot;: [&quot;user1&quot;], &quot;backendroles&quot;: [&quot;backendrole2&quot;] } }, {   &quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/finance&quot;, &quot;value&quot;: { &quot;users&quot;: [&quot;user2&quot;], &quot;backendroles&quot;: [&quot;backendrole2&quot;] } } ] Sample response  { &quot;status&quot;: &quot;OK&quot;, &quot;message&quot;: &quot;Resource updated.&quot; }   Authentication  Get authentication details  Request  GET _opendistro/_security/api/config   Cache  Flush cache  Flushes the Security plugin user, authentication, and authorization cache.  Request  DELETE _opendistro/_security/api/cache Sample response  { &quot;status&quot;: &quot;OK&quot;, &quot;message&quot;: &quot;Cache flushed successfully.&quot; }   Health  Health check  Checks to see if the Security plugin is up and running.  Request  GET _opendistro/_security/health Sample response  { &quot;message&quot;: null, &quot;mode&quot;: &quot;strict&quot;, &quot;status&quot;: &quot;UP&quot; }  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/security/api/",
      "relUrl": "/docs/security/api/"
    },
  
    {
      "id": "3",
      "title": "Audit Log Storage Types",
      "content": "Audit log storage types Audit logs can take up quite a bit of space, so the Security plugin offers several options for storage locations.    Setting   Description   debug   Outputs to stdout. Useful for testing and debugging.    internal_elasticsearch   Writes to an audit index on the current Elasticsearch cluster.    external_elasticsearch   Writes to an audit index on a remote Elasticsearch cluster.    webhook   Sends events to an arbitrary HTTP endpoint.    log4j   Writes the events to a Log4j logger. You can use any Log4j appender, such as SNMP, JDBC, Cassandra, and Kafka.  You configure the output location in elasticsearch.yml:  opendistro_security.audit.type: &amp;lt;debug|internal_elasticsearch|external_elasticsearch|webhook|log4j&amp;gt; external_elasticsearch, webhook, and log4j all have additional configuration options. Details follow.  External Elasticsearch  The external_elasticsearch storage type requires one or more Elasticsearch endpoints with a host/IP address and port. Optionally, provide the index name and a document type.  opendistro_security.audit.type: external_elasticsearch opendistro_security.audit.config.http_endpoints: [&amp;lt;endpoints&amp;gt;] opendistro_security.audit.config.index: &amp;lt;indexname&amp;gt; opendistro_security.audit.config.type: _doc The Security plugin uses the Elasticsearch REST API to send events, just like any other indexing request. For opendistro_security.audit.config.http_endpoints, use a comma-separated list of hosts/IP addresses and the REST port (default 9200).  opendistro_security.audit.config.http_endpoints: [192.168.178.1:9200,192.168.178.2:9200] If you use external_elasticsearch and the remote cluster also uses the Security plugin, you must supply some additional parameters for authentication. These parameters depend on which authentication type you configured for the remote cluster.  TLS settings    Name   Data Type   Description   opendistro_security.audit.config.enable_ssl   Boolean   If you enabled SSL/TLS on the receiving cluster, set to true. The default is false.    opendistro_security.audit.config.verify_hostnames   Boolean   Whether to verify the hostname of the SSL/TLS certificate of the receiving cluster. Default is true.    opendistro_security.audit.config.pemtrustedcas_filepath   String   The trusted root certificate of the external Elasticsearch cluster, relative to the config directory.    opendistro_security.audit.config.pemtrustedcas_content   String   Instead of specifying the path (opendistro_security.audit.config.pemtrustedcas_filepath), you can configure the Base64-encoded certificate content directly.    opendistro_security.audit.config.enable_ssl_client_auth   Boolean   Whether to enable SSL/TLS client authentication. If you set this to true, the audit log module sends the node’s certificate along with the request. The receiving cluster can use this certificate to verify the identity of the caller.    opendistro_security.audit.config.pemcert_filepath   String   The path to the TLS certificate to send to the external Elasticsearch cluster, relative to the config directory.    opendistro_security.audit.config.pemcert_content   String   Instead of specifying the path (opendistro_security.audit.config.pemcert_filepath), you can configure the Base64-encoded certificate content directly.    opendistro_security.audit.config.pemkey_filepath   String   The path to the private key of the TLS certificate to send to the external Elasticsearch cluster, relative to the config directory.    opendistro_security.audit.config.pemkey_content   String   Instead of specifying the path (opendistro_security.audit.config.pemkey_filepath), you can configure the Base64-encoded certificate content directly.    opendistro_security.audit.config.pemkey_password   String   The password of the private key.  Basic auth settings  If you enabled HTTP basic authentication on the receiving cluster, use these settings to specify the username and password:  opendistro_security.audit.config.username: &amp;lt;username&amp;gt; opendistro_security.audit.config.password: &amp;lt;password&amp;gt; Webhook  Use the following keys to configure the webhook storage type.    Name   Data Type   Description   opendistro_security.audit.config.webhook.url   String   The HTTP or HTTPS URL to send the logs to.    opendistro_security.audit.config.webhook.ssl.verify   Boolean   If true, the TLS certificate provided by the endpoint (if any) will be verified. If set to false, no verification is performed. You can disable this check if you use self-signed certificates.    opendistro_security.audit.config.webhook.ssl.pemtrustedcas_filepath   String   The path to the trusted certificate against which the webhook’s TLS certificate is validated.    opendistro_security.audit.config.webhook.ssl.pemtrustedcas_content   String   Same as opendistro_security.audit.config.webhook.ssl.pemtrustedcas_content, but you can configure the base 64 encoded certificate content directly.    opendistro_security.audit.config.webhook.format   String   The format in which the audit log message is logged, can be one of URL_PARAMETER_GET, URL_PARAMETER_POST, TEXT, JSON, SLACK. See Formats.  Formats    Format   Description   URL_PARAMETER_GET   Uses HTTP GET to send logs to the webhook URL. All logged information is appended to the URL as request parameters.    URL_PARAMETER_POST   Uses HTTP POST to send logs to the webhook URL. All logged information is appended to the URL as request parameters.    TEXT   Uses HTTP POST to send logs to the webhook URL. The request body contains the audit log message in plain text format.    JSON   Uses HTTP POST to send logs to the webhook URL. The request body contains the audit log message in JSON format.    SLACK   Uses HTTP POST to send logs to the webhook URL. The request body contains the audit log message in JSON format suitable for consumption by Slack. The default implementation returns &quot;text&quot;: &quot;&amp;lt;AuditMessage#toText&amp;gt;&quot;.  Log4j  The log4j storage type lets you specify the name of the logger and log level.  opendistro_security.audit.config.log4j.logger_name: audit opendistro_security.audit.config.log4j.level: INFO By default, the Security plugin uses the logger name audit and logs the events on INFO level. Audit events are stored in JSON format. ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/security/audit-log-storage-types/",
      "relUrl": "/docs/security/audit-log-storage-types/"
    },
  
    {
      "id": "4",
      "title": "Audit Log Field Reference",
      "content": "Audit log field reference This page contains descriptions for all audit log fields.  Common attributes  The following attributes are logged for all event categories, independent of the layer.    Name   Description   audit_format_version   The audit log message format version.    audit_utc_timestamp   The UTC timestamp for the event.    audit_category   The audit log category, one of FAILED_LOGIN, MISSING_PRIVILEGES, BAD_HEADERS, SSL_EXCEPTION, OPENDISTRO_SECURITY_INDEX_ATTEMPT, AUTHENTICATED or GRANTED_PRIVILEGES.    audit_node_id The ID of the node where the event was generated.    audit_node_name   The name of the node where the event was generated.    audit_node_host_address   The host address of the node where the event was generated.    audit_node_host_name   The host name of the node where the event was generated.    audit_request_layer   The layer on which the event has been generated, either TRANSPORT or REST.    audit_request_origin   The layer from which the event originated, either TRANSPORT or REST.    audit_request_effective_user_is_admin   True if the request was made with a TLS admin certificate, otherwise false.  REST FAILED_LOGIN attributes    Name   Description   audit_request_effective_user   The username that failed to authenticate.    audit_rest_request_path   The REST endpoint URI.    audit_rest_request_params   The HTTP request parameters, if any.    audit_rest_request_headers   The HTTP headers, if any.    audit_request_initiating_user   The user that initiated the request. Only logged if it differs from the effective user.    audit_request_body   The HTTP request body, if any (and if request body logging is enabled).  REST AUTHENTICATED attributes    Name   Description   audit_request_effective_user   The username that failed to authenticate.    audit_request_initiating_user   The user that initiated the request. Only logged if it differs from the effective user.    audit_rest_request_path   The REST endpoint URI.    audit_rest_request_params   The HTTP request parameters, if any.    audit_rest_request_headers   The HTTP headers, if any.    audit_request_body   The HTTP request body, if any (and if request body logging is enabled).  REST SSL_EXCEPTION attributes    Name   Description   audit_request_exception_stacktrace   The stack trace of the SSL exception.  REST BAD_HEADERS attributes    Name   Description   audit_rest_request_path   The REST endpoint URI.    audit_rest_request_params   The HTTP request parameters, if any.    audit_rest_request_headers   The HTTP headers, if any.    audit_request_body   The HTTP request body, if any (and if request body logging is enabled).  Transport FAILED_LOGIN attributes    Name   Description   audit_trace_task_id   The ID of the request.    audit_transport_headers   The headers of the request, if any.    audit_request_effective_user   The username that failed to authenticate.    audit_request_initiating_user   The user that initiated the request. Only logged if it differs from the effective user.    audit_transport_request_type   The type of request (e.g. IndexRequest).    audit_request_body   The HTTP request body, if any (and if request body logging is enabled).    audit_trace_indices   The index name(s) included in the request. Can contain wildcards, date patterns, and aliases. Only logged if resolve_indices is true.    audit_trace_resolved_indices   The resolved index name(s) affected by the request. Only logged if resolve_indices is true.    audit_trace_doc_types   The document types affected by the request. Only logged if resolve_indices is true.  Transport AUTHENTICATED attributes    Name   Description   audit_trace_task_id   The ID of the request.    audit_transport_headers   The headers of the request, if any.    audit_request_effective_user   The username that failed to authenticate.    audit_request_initiating_user   The user that initiated the request. Only logged if it differs from the effective user.    audit_transport_request_type   The type of request (e.g. IndexRequest).    audit_request_body   The HTTP request body, if any (and if request body logging is enabled).    audit_trace_indices   The index name(s) included in the request. Can contain wildcards, date patterns, and aliases. Only logged if resolve_indices is true.    audit_trace_resolved_indices   The resolved index name(s) affected by the request. Only logged if resolve_indices is true.    audit_trace_doc_types   The document types affected by the request. Only logged if resolve_indices is true.  Transport MISSING_PRIVILEGES attributes    Name   Description   audit_trace_task_id   The ID of the request.    audit_trace_task_parent_id   The parent ID of this request, if any.    audit_transport_headers   The headers of the request, if any.    audit_request_effective_user   The username that failed to authenticate.    audit_request_initiating_user   The user that initiated the request. Only logged if it differs from the effective user.    audit_transport_request_type   The type of request (e.g. IndexRequest).    audit_request_privilege   The required privilege of the request (e.g. indices:data/read/search).    audit_request_body   The HTTP request body, if any (and if request body logging is enabled).    audit_trace_indices   The index name(s) included in the request. Can contain wildcards, date patterns, and aliases. Only logged if resolve_indices is true.    audit_trace_resolved_indices   The resolved index name(s) affected by the request. Only logged if resolve_indices is true.    audit_trace_doc_types   The document types affected by the request. Only logged if resolve_indices is true.  Transport GRANTED_PRIVILEGES attributes    Name   Description   audit_trace_task_id   The ID of the request.    audit_trace_task_parent_id   The parent ID of this request, if any.    audit_transport_headers   The headers of the request, if any.    audit_request_effective_user   The username that failed to authenticate.    audit_request_initiating_user   The user that initiated the request. Only logged if it differs from the effective user.    audit_transport_request_type   The type of request (e.g. IndexRequest).    audit_request_privilege   The required privilege of the request (e.g. indices:data/read/search).    audit_request_body   The HTTP request body, if any (and if request body logging is enabled).    audit_trace_indices   The index name(s) included in the request. Can contain wildcards, date patterns, and aliases. Only logged if resolve_indices is true.    audit_trace_resolved_indices   The resolved index name(s) affected by the request. Only logged if resolve_indices is true.    audit_trace_doc_types   The document types affected by the request. Only logged if resolve_indices is true.  Transport SSL_EXCEPTION attributes    Name   Description   audit_request_exception_stacktrace   The stack trace of the SSL exception.  Transport BAD_HEADERS attributes    Name   Description   audit_trace_task_id   The ID of the request.    audit_trace_task_parent_id   The parent ID of this request, if any.    audit_transport_headers   The headers of the request, if any.    audit_request_effective_user   The username that failed to authenticate.    audit_request_initiating_user   The user that initiated the request. Only logged if it differs from the effective user.    audit_transport_request_type   The type of request (e.g. IndexRequest).    audit_request_body   The HTTP request body, if any (and if request body logging is enabled).    audit_trace_indices   The index name(s) included in the request. Can contain wildcards, date patterns, and aliases. Only logged if resolve_indices is true.    audit_trace_resolved_indices   The resolved index name(s) affected by the request. Only logged if resolve_indices is true.    audit_trace_doc_types   The document types affected by the request. Only logged if resolve_indices is true.  Transport OPENDISTRO_SECURITY_INDEX_ATTEMPT attributes    Name   Description   audit_trace_task_id   The ID of the request.    audit_transport_headers   The headers of the request, if any.    audit_request_effective_user   The username that failed to authenticate.    audit_request_initiating_user   The user that initiated the request. Only logged if it differs from the effective user.    audit_transport_request_type   The type of request (e.g. IndexRequest).    audit_request_body   The HTTP request body, if any (and if request body logging is enabled).    audit_trace_indices   The index name(s) included in the request. Can contain wildcards, date patterns, and aliases. Only logged if resolve_indices is true.    audit_trace_resolved_indices   The resolved index name(s) affected by the request. Only logged if resolve_indices is true.    audit_trace_doc_types   The document types affected by the request. Only logged if resolve_indices is true. ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/security/audit-logs-field-reference/",
      "relUrl": "/docs/security/audit-logs-field-reference/"
    },
  
    {
      "id": "5",
      "title": "Audit Logs",
      "content": "Enable audit logs Audit logs let you track access to your Elasticsearch cluster and are useful for compliance purposes or in the aftermath of a security breach. You can configure the categories to be logged, the detail level of the logged messages, and where to store the logs.  To enable audit logging: Add the following line to elasticsearch.yml on each node:  opendistro_security.audit.type: internal_elasticsearch  This setting stores audit logs on the current cluster. For other storage options, see Audit Log Storage Types.  Restart each node. Tracked events Exclude categories Disable REST or the transport layer Disable request body logging Log index names Configure bulk request handling Exclude requests Exclude users Configure the audit log index name (Advanced) Tune the thread pool   Tracked events  Audit logging records events in two ways: HTTP requests (REST) and the transport layer.    Event   Logged on REST   Logged on transport   Description   FAILED_LOGIN   Yes   Yes   The credentials of a request could not be validated, most likely because the user does not exist or the password is incorrect.    AUTHENTICATED   Yes   Yes   A user successfully authenticated.    MISSING_PRIVILEGES   No   Yes   The user does not have the required permissions to execute the request.    GRANTED_PRIVILEGES   No   Yes   A user made a successful request to Elasticsearch.    SSL_EXCEPTION   Yes   Yes   An attempt was made to access Elasticsearch without a valid SSL/TLS certificate.    OPENDISTRO_SECURITY_INDEX_ATTEMPT   No   Yes   An attempt was made to modify the Security plugin internal user and privileges index without the required permissions or TLS admin certificate.    BAD_HEADERS   Yes   Yes   An attempt was made to spoof a request to Elasticsearch with the Security plugin internal headers.  These default log settings work well for most use cases, but you can change settings to save storage space or adapt the information to your exact needs.  Exclude categories  To exclude categories, set:  opendistro_security.audit.config.disabled_rest_categories: &amp;lt;disabled categories&amp;gt; opendistro_security.audit.config.disabled_transport_categories: &amp;lt;disabled categories&amp;gt; For example:  opendistro_security.audit.config.disabled_rest_categories: AUTHENTICATED, OPENDISTRO_SECURITY_INDEX_ATTEMPT opendistro_security.audit.config.disabled_transport_categories: GRANTED_PRIVILEGES If you want to log events in all categories, use NONE:  opendistro_security.audit.config.disabled_rest_categories: NONE opendistro_security.audit.config.disabled_transport_categories: NONE Disable REST or the transport layer  By default, the Security plugin logs events on both REST and the transport layer. You can disable either type:  opendistro_security.audit.enable_rest: true opendistro_security.audit.enable_transport: false Disable request body logging  By default, the Security plugin includes the body of the request (if available) for both REST and the transport layer. If you do not want or need the request body, you can disable it:  opendistro_security.audit.log_request_body: false Log index names  By default, the Security plugin logs all indices affected by a request. Because index names can be an aliases and contain wildcards/date patterns, the Security plugin logs the index name that the user submitted and the actual index name to which it resolves.  For example, if you use an alias or a wildcard, the the audit event might look like:  audit_trace_indices: [ &quot;human*&quot; ], audit_trace_resolved_indices: [ &quot;humanresources&quot; ] You can disable this feature by setting:  opendistro_security.audit.resolve_indices: false Disabling this feature only takes effect if opendistro_security.audit.log_request_body is also set to false.  Configure bulk request handling  Bulk requests can contain many indexing operations. By default, the Security plugin only logs the single bulk request, not each individual operation.  The Security plugin can be configured to log each indexing operation as a separate event:  opendistro_security.audit.resolve_bulk_requests: true This change can create a massive number of events in the audit logs, so we don’t recommend enabling this setting if you make heavy use of the _bulk API.  Exclude requests  You can exclude certain requests from being logged completely, by either configuring actions (for transport requests) and/or HTTP request paths (REST):  opendistro_security.audit.ignore_requests: [&quot;indices:data/read/*&quot;, &quot;SearchRequest&quot;] Exclude users  By default, the Security plugin logs events from all users, but excludes the internal Kibana server user kibanaserver. You can exclude other users:  opendistro_security.audit.ignore_users: - kibanaserver - admin If requests from all users should be logged, use NONE:  opendistro_security.audit.ignore_users: NONE Configure the audit log index name  By default, the Security plugin stores audit events in a daily rolling index named auditlog-YYYY.MM.dd. You can configure the name of the index in elasticsearch.yml:  opendistro_security.audit.config.index: myauditlogindex Use a date pattern in the index name to configure daily, weekly, or monthly rolling indices:  opendistro_security.audit.config.index: &quot;&#39;auditlog-&#39;YYYY.MM.dd&quot; For a reference on the date pattern format, see the Joda DateTimeFormat documentation.  (Advanced) Tune the thread pool  The Search plugin logs events asynchronously, which keeps performance impact on your cluster minimal. The plugin uses a fixed thread pool to log events. You can define the number of threads in the pool in elasticsearch.yml:  opendistro_security.audit.threadpool.size: &amp;lt;integer&amp;gt; The default setting is 10. Setting this value to 0 disables the thread pool, which means the plugin logs events synchronously. To set the maximum queue length per thread:  opendistro_security.audit.threadpool.max_queue_len: 100000  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/security/audit-logs/",
      "relUrl": "/docs/security/audit-logs/"
    },
  
    {
      "id": "6",
      "title": "Basics",
      "content": "Security plugin basics Understanding key terms and the authentication flow is the best way to get started with the Security plugin for Open Distro for Elasticsearch.  Concepts    Term   Description   Permission   An individual action, such as creating an index (e.g. indices:admin/create). For a complete list, see Permissions.    Action group   A set of permissions. For example, the predefined SEARCH action group authorizes roles to use the _search and _msearch APIs.    Role   Security roles define the scope of a permission or action group: cluster, index, document, or field. For example, a role named delivery_analyst might have no cluster permissions, the READ action group for all indices that match the delivery-data-* pattern, access to all document types within those indices, and access to all fields except delivery_driver_name.    Backend role   (Optional) Additional, external roles that come from an authorization backend (e.g. LDAP/Active Directory).    User   Users make requests to Elasticsearch clusters. A user has credentials (e.g. a username and password), zero or more backend roles, and zero or more custom attributes.    Role mapping   Users assume roles after they successfully authenticate. Role mappings, well, map roles to users (or backend roles). For example, a mapping of kibana_user (role) to jdoe (user) means that John Doe gains all the permissions of kibana_user after authenticating. Likewise, a mapping of all_access (role) to admin (backend role) means that any user with the backend role of admin (from an LDAP/Active Directory server) gains all the permissions of all_access after authenticating. You can map individual roles to many users and/or backend roles.  The Security plugin comes with a number of predefined action groups, roles, mappings, and users. These entities serve as sensible defaults and are good examples of how to use the plugin.  Authentication flow In order to identify the user who wants to access the cluster, the Security plugin needs the user’s credentials.  These credentials differ depending on how you’ve configured the plugin. For example, if you use basic authentication, these credentials are a username and password. If you use a JSON web token, these credentials are stored within the token itself. If you use TLS certificates, the credentials are the distinguished name (DN) of the certificate.  The Security plugin authenticates the user’s credentials against a backend: the internal user database, Lightweight Directory Access Protocol (LDAP), Active Directory, Kerberos, or JSON web tokens.  The plugin supports chaining backends. If more than one backend is configured, the plugin tries to authenticate the user against all backends until one succeeds. A common use case is to combine the Security plugin’s internal user database with LDAP/Active Directory.  (Optional) After an authenticator verifies the user’s credentials, the plugin collects any backend roles. In most cases, this backend is LDAP/Active Directory.  Now that the user has authenticated and any backend roles have been retrieved, the Security plugin uses the role mapping to map security roles to the user (or to the user’s backend roles).  If the role mapping doesn’t include the user (or the user’s backend roles), the user successfully authenticates, but has no permissions.  The user can now perform actions as defined by the mapped security roles. For example, a user might map to the kibana_user role and thus have permissions to access Kibana.   ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/security/concepts/",
      "relUrl": "/docs/security/concepts/"
    },
  
    {
      "id": "7",
      "title": "Backend Configuration",
      "content": "Backend configuration One of the first steps to using the Security plugin is to decide on an authentication backend, which handles steps 2-3 of the authentication flow. The plugin has an internal user database, but many people prefer to use an existing authentication backend, such as an LDAP server, or some combination of the two.  The main configuration file for authentication and authorization modules  is plugins/opendistro_security/securityconfig/config.yml. It defines how the Security plugin retrieves the user credentials, how it verifies these credentials, and how additional user roles are fetched from backend systems (optional).  config.yml has three main parts:  opendistro_security: dynamic:   http:   ...   authc:   ...   authz:   ...   HTTP Authentication Authorization Examples HTTP basic Kerberos  Dynamic configuration   Authentication backend  JSON web token  Header   Payload   Signature   Configure JSON web tokens   Symmetric key algorithms: HMAC   Asymmetric key algorithms: RSA and ECDSA   Bearer authentication for HTTP requests   URL parameters for HTTP requests   Validated registered claims   Supported formats and algorithms  HTTP  The http section has the following format:  anonymous_auth_enabled: &amp;lt;true|false&amp;gt; xff: # optional section enabled: &amp;lt;true|false&amp;gt; internalProxies: &amp;lt;string&amp;gt; # Regex pattern remoteIpHeader: &amp;lt;string&amp;gt; # Name of the header in which to look. Typically: x-forwarded-for proxiesHeader: &amp;lt;string&amp;gt; trustedProxies: &amp;lt;string&amp;gt; # Regex pattern If you disable anonymous authentication, the Security plugin won’t initialize if you have not provided at least one authc.  Authentication  The authc section has the following format:  &amp;lt;name&amp;gt;: http_enabled: &amp;lt;true|false&amp;gt; transport_enabled: &amp;lt;true|false&amp;gt; order: &amp;lt;integer&amp;gt;   http_authenticator:   ...   authentication_backend:   ... An entry in the authc section is called an authentication domain. It specifies where to get the user credentials and against which backend they should be authenticated.  You can use more than one authentication domain. Each authentication domain has a name (e.g. basic_auth_internal), enabled flags, and an order. The order makes it possible to chain authentication domains together. The Security plugin uses them in the order you provide. If the user successfully authenticates with one domain, the Security plugin skips the remaining domains.  http_authenticator specifies which authentication method you want to use on the HTTP layer.  The syntax for defining an authenticator on the HTTP layer is:  http_authenticator: type: &amp;lt;type&amp;gt; challenge: &amp;lt;true|false&amp;gt; config:   ... Allowed values for type are:   basic: HTTP basic authentication. No additional configuration is needed. kerberos: Kerberos authentication. Additional, Kerberos-specific configuration is needed. jwt: JSON web token authentication. Additional, JWT-specific configuration is needed. clientcert: Authentication via a client TLS certificate. This certificate must be trusted by one of the root CAs in the truststore of your nodes. After setting an HTTP authenticator, you need to specify against which backend system you want to authenticate the user:  authentication_backend: type: &amp;lt;type&amp;gt; config:   ... Possible vales for type are:   noop: This setting means that no further authentication against any backend system is performed. Use noop if the HTTP authenticator has already authenticated the user completely, as in the case of JWT, Kerberos, or client certificate authentication. internal: Use the users and roles defined in internal_users.yml for authentication. ldap: Authenticate users against an LDAP server. This setting requires additional, LDAP-specific configuration settings. Authorization  After the user has been authenticated, the Security plugin can optionally collect additional user roles from backend systems. The authorization configuration has the following format:  authz: &amp;lt;name&amp;gt;:   http_enabled: &amp;lt;true|false&amp;gt;   transport_enabled: &amp;lt;true|false&amp;gt;   authorization_backend:   type: &amp;lt;type&amp;gt;   config:   ... You can define multiple entries in this section the same way as you can for authentication entries. In this case, execution order is not relevant, so there is no order field.  Possible vales for type are:   noop: Used to skip this step altogether ldap: Fetch additional roles from an LDAP server. This setting requires additional, LDAP-specific configuration settings. Examples  The default plugins/opendistro_security/securityconfig/config.yml that ships with Open Distro for Elasticsearch contains many configuration examples. Use these examples as a starting point, and customize them to your needs.  HTTP basic  In order to set up HTTP basic authentication, you just need to enable it in the http_authenticator section of the configuration:  http_authenticator: type: basic challenge: true In most cases, you want to set the challenge flag to true. The flag defines the behavior of the Security plugin if the Authorization field in the HTTP header is not set.  If challenge is set to true, the Security plugin sends a response with status UNAUTHORIZED (401) back to the client. If the client is accessing the cluster with a browser, this triggers the authentication dialog, and the user is prompted to enter username and password.  If challenge is set to false and no Authorization header field is set, the Security plugin does not send a WWW-Authenticate response back to the client, and authentication fails. You may want to use this setting if you have another challenge http_authenticator in your configured authentication domains. One such scenario is when you plan to use basic authentication and Kerberos together.  Kerberos  Due to the nature of Kerberos, you need to define some settings in elasticsearch.yml and some in config.yml.  In elasticsearch.yml, you need to define:  opendistro_security.kerberos.krb5_filepath: &#39;/etc/krb5.conf&#39; opendistro_security.kerberos.acceptor_keytab_filepath: &#39;eskeytab.tab&#39; opendistro_security.kerberos.krb5_filepath defines the path to your Kerberos configuration file. This file contains various settings regarding your Kerberos installation, for example, the realm name(s), hostname(s), and port(s) of the Kerberos key distribution center (KDC).  opendistro_security.kerberos.acceptor_keytab_filepath defines the path to the keytab file, which contains the principal that the Security plugin uses to issue requests against Kerberos.  acceptor_principal: &#39;HTTP/localhost&#39; defines the principal that the Security plugin will use to issue requests against Kerberos.  The acceptor_principal defines the acceptor/server principal name the Security plugin uses to issue requests against Kerberos. This value must be present in the keytab file.  Due to security restrictions, the keytab file must be placed in the &amp;lt;open-distro-install-dir&amp;gt;/conf or a subdirectory, and the path in elasticsearch.yml must be relative, not absolute.  Dynamic configuration  A typical Kerberos authentication domain in config.yml looks like this:  authc:   kerberos_auth_domain:   enabled: true   order: 1   http_authenticator:  type: kerberos  challenge: true  config:  krb_debug: false  strip_realm_from_principal: true   authentication_backend:  type: noop Authentication against Kerberos via a browser on HTTP level is achieved using SPNEGO. Kerberos/SPNEGO implementations vary, depending on your browser and operating system. This is important when deciding if you need to set the challenge flag to true or false.  As with HTTP Basic Authentication, this flag determines how the Security plugin should react when no Authorization header is found in the HTTP request or if this header does not equal negotiate.  If set to true, the Security plugin sends a response with status code 401 and a WWW-Authenticate header set to negotiate. This tells the client (browser) to resend the request with the Authorization header set. If set to false, the Security plugin cannot extract the credentials from the request, and authentication fails. Setting challenge to false thus only makes sense if the Kerberos credentials are sent in the initial request.  As the name implies, setting krb_debug to true will output Kerberos-specific debugging messages to stdout. Use this setting if you encounter problems with your Kerberos integration.  If you set strip_realm_from_principal to true, the Security plugin strips the realm from the user name.  Authentication backend  Since Kerberos/SPNEGO authenticates users on HTTP level, no additional authentication_backend is needed. Set this value to noop.  JSON web token  JSON web tokens (JWT) are JSON-based access tokens that assert one or more claims. They are commonly used to implement single sign-on (SSO) solutions and fall in the category of token-based authentication systems.   A user logs in to an authentication server by providing credentials (e.g. username and password). The authentication server validates the credentials. The authentication server creates an access token and signs it. The authentication server returns the token to the user. The user stores the access token. The user sends the access token alongside every request to the service it wants to use. The service verifies the token and grants or denies access. A JSON web token is self-contained in the sense that it carries all necessary information to verify a user within itself. The tokens are Base64-encoded, signed JSON objects.  JSON web tokens consist of three parts:   Header Payload Signature Header  The header contains information about the used signing mechanism, for example:  { &quot;alg&quot;: &quot;HS256&quot;, &quot;typ&quot;: &quot;JWT&quot; } In this case, the header states that the message was signed using HMAC-SHA256.  Payload  The payload of a JSON web token contains the so-called JWT Claims. A claim can be any piece of information about the user that the application that created the token has verified.  The specification defines a set of standard claims with reserved names (“registered claims”). These include, for example, the token issuer, the expiration date, or the creation date.  Public claims, on the other hand, can be created freely by the token issuer. They can contain arbitrary information, such as the user name and the roles of the user.  Example:  { &quot;iss&quot;: &quot;example.com&quot;, &quot;exp&quot;: 1300819380, &quot;name&quot;: &quot;John Doe&quot;, &quot;roles&quot;: &quot;admin, devops&quot; } Signature  The issuer of the token calculates the signature of the token by applying a cryptographic hash function on the Base64-encoded header and payload. These three parts are then concatenated using periods to form a complete JSON web token:  encoded = base64UrlEncode(header) + &quot;.&quot; + base64UrlEncode(payload) signature = HMACSHA256(encoded, &#39;secretkey&#39;); jwt = encoded + &quot;.&quot; + base64UrlEncode(signature) For example: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJsb2dnZWRJbkFzIjoiYWRtaW4iLCJpYXQiOjE0MjI3Nzk2Mzh9.gzSraSYS8EXBxLN_oWnFSRgCzcmJmMjLiuyu5CSpyHI Configure JSON web tokens  If JSON web tokens are the only authentication method you use, disable the user cache by setting opendistro_security.cache.ttl_minutes: 0.  Set up an authentication domain and choose jwt as HTTP authentication type. Since the tokens already contain all required information to verify the request, challenge must be set to false and authentication_backend to noop.  Example:  jwt_auth_domain: enabled: true order: 0 http_authenticator:   type: jwt   challenge: false   config:   signing_key: &quot;base64 encoded key&quot;   jwt_header: &quot;Authorization&quot;   jwt_url_parameter: null   subject_key: null   roles_key: null authentication_backend: I  type: noop Configuration parameter:    Name   Description   signing_key   The signing key to use when verifying the token. If you use a symmetric key algorithm, it is the Base64-encoded shared secret. If you use an asymmetric algorithm, it contains the public key.    jwt_header   The HTTP header in which the token is transmitted. Typically the Authorization header with the Bearer schema: Authorization: Bearer &amp;lt;token&amp;gt;. Default is Authorization.    jwt_url_parameter   If the token is not transmitted in the HTTP header, but as an URL parameter, define the name of this parameter here.    subject_key   The key in the JSON payload that stores the username. If not set, the subject registered claim is used.    roles_key   The key in the JSON payload that stores the user’s roles. The value of this key must be a comma-separated list of roles.  Since JSON web tokens are self-contained and the user is authenticated on HTTP level, no additional authentication_backend is needed. Set this value to noop.  Symmetric key algorithms: HMAC  Hash-based message authentication codes (HMACs) are a group of algorithms that provide a way of signing messages by means of a shared key. The key is shared between the authentication server and the Security plugin. It must be configured as a Base64-encoded value in the signing_key setting:  jwt_auth_domain: ...   config:   signing_key: &quot;a3M5MjEwamRqOTAxOTJqZDE=&quot;   ... Asymmetric key algorithms: RSA and ECDSA  RSA and ECDSA are asymmetric encryption and digital signature algorithms and use a public/private key pair to sign and verify tokens. This means that they use a private key for signing the token, while the Security plugin only needs to know the public key to verify it.  Since you cannot issue new tokens with the public key—and because you can make valid assumptions about the creator of the token—RSA and ECDSA are considered more secure than using HMAC.  In order to use RS256, you only need to configure the (non-Base64-encoded) public RSA key as signing_key in the JWT configuration:  jwt_auth_domain: ...   config:   signing_key: |-   --BEGIN PUBLIC KEY--   MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQK...   --END PUBLIC KEY--   ... The Security plugin automatically detects the algorithm (RSA/ECDSA), and if necessary, you can break the key into multiple lines.  Bearer authentication for HTTP requests  The most common way of transmitting a JSON web token in an HTTP request is to add it as an HTTP header with the bearer authentication schema.  Authorization: Bearer &amp;lt;JWT&amp;gt; The default name of the header is Authorization. If required by your authentication server or proxy, you can also use a different HTTP header name using the jwt_header configuration key.  As with HTTP basic authentication, you should use HTTPS instead of HTTP when transmitting JSON web tokens in HTTP requests.  URL parameters for HTTP requests  While the most common way to transmit JWTs in HTTP requests is to use a header field, the Security plugin also supports parameters. Configure the name of the GET parameter using the following key:  config:   signing_key: ...   jwt_url_parameter: &quot;parameter_name&quot;   subject_key: ...   roles_key: ... As with HTTP basic authentication, you should use HTTPS instead of HTTP.  Validated registered claims  The following registered claims are validated automatically:   “iat” (Issued At) Claim “nbf” (Not Before) Claim “exp” (Expiration Time) Claim Supported formats and algorithms  The Security plugin supports digitally-signed compact JSON web tokens with all standard algorithms:  HS256: HMAC using SHA-256 HS384: HMAC using SHA-384 HS512: HMAC using SHA-512 RS256: RSASSA-PKCS-v1_5 using SHA-256 RS384: RSASSA-PKCS-v1_5 using SHA-384 RS512: RSASSA-PKCS-v1_5 using SHA-512 PS256: RSASSA-PSS using SHA-256 and MGF1 with SHA-256 PS384: RSASSA-PSS using SHA-384 and MGF1 with SHA-384 PS512: RSASSA-PSS using SHA-512 and MGF1 with SHA-512 ES256: ECDSA using P-256 and SHA-256 ES384: ECDSA using P-384 and SHA-384 ES512: ECDSA using P-521 and SHA-512  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/security/configuration/",
      "relUrl": "/docs/security/configuration/"
    },
  
    {
      "id": "8",
      "title": "Configuration",
      "content": "Elasticsearch configuration Most Elasticsearch configuration can take place in the cluster settings API. Certain operations require you to modify elasticsearch.yml and restart the cluster.  Whenever possible, use the cluster settings API instead; elasticsearch.yml is local to each node, whereas the API applies the setting to all nodes in the cluster.  Cluster settings API  The first step in changing a setting is to view the current settings:  GET _cluster/settings?include_defaults=true For a more concise summary of non-default settings:  GET _cluster/settings Three categories of setting exist in the cluster settings API: persistent, transient, and default. Persistent settings, well, persist after a cluster restart. After a restart, Elasticsearch clears transient settings.  If you specify the same setting in multiple places, Elasticsearch uses the following precedence:   Transient settings Persistent settings Settings from elasticsearch.yml Default settings To change a setting, just specify the new one as either persistent or transient. This example shows the flat settings form:  PUT /_cluster/settings { &quot;persistent&quot; : {   &quot;action.auto_create_index&quot; : false } } You can also use the expanded form, which lets you copy and paste from the GET response and change existing values:  PUT /_cluster/settings { &quot;persistent&quot;: {   &quot;action&quot;: {   &quot;auto_create_index&quot;: false   } } }   Configuration file  You can find elasticsearch.yml in /usr/share/elasticsearch/config/elasticsearch.yml (Docker) or /etc/elasticsearch/elasticsearch.yml (RPM) on each node. Out of the box, it contains a number of default settings for the Security plugin that you should modify before using Open Distro for Elasticsearch for a production workload. To learn more, see Security.  Sample configuration file  cluster.name: &quot;docker-cluster&quot; network.host: 0.0.0.0  # minimum_master_nodes need to be explicitly set when bound on a public IP # set to 1 to allow single node clusters discovery.zen.minimum_master_nodes: 1   Start OpenDistro for Elasticsearch Security Demo Configuration  # WARNING: revise all the lines below before you go into production opendistro_security.ssl.transport.pemcert_filepath: esnode.pem opendistro_security.ssl.transport.pemkey_filepath: esnode-key.pem opendistro_security.ssl.transport.pemtrustedcas_filepath: root-ca.pem opendistro_security.ssl.transport.enforce_hostname_verification: false opendistro_security.ssl.http.enabled: true opendistro_security.ssl.http.pemcert_filepath: esnode.pem opendistro_security.ssl.http.pemkey_filepath: esnode-key.pem opendistro_security.ssl.http.pemtrustedcas_filepath: root-ca.pem opendistro_security.allow_unsafe_democertificates: true opendistro_security.allow_default_init_securityindex: true opendistro_security.authcz.admin_dn: - CN=kirk,OU=client,O=client,L=test, C=de  opendistro_security.audit.type: internal_elasticsearch opendistro_security.enable_snapshot_restore_privilege: true opendistro_security.check_snapshot_restore_write_privileges: true opendistro_security.restapi.roles_enabled: [&quot;all_access&quot;, &quot;security_rest_api_access&quot;] cluster.routing.allocation.disk.threshold_enabled: false node.max_local_storage_nodes: 3  End OpenDistro for Elasticsearch Security Demo Configuration   ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/elasticsearch/configuration/",
      "relUrl": "/docs/elasticsearch/configuration/"
    },
  
    {
      "id": "9",
      "title": "Cron",
      "content": "Cron expression reference Monitors can run at a variety of fixed intervals (e.g. hourly, daily, etc.), but you can also define custom cron expressions for when they should run. Monitors use the Unix cron syntax and support five fields:    Field   Valid values   Minute   0-59    Hour   0-23    Day of month   1-31    Month   1-12    Day of week   0-7 (0 and 7 are both Sunday)  For example, the following expression translates to “every Monday through Friday at 11:30 AM”:  30 11 * * 1-5 Features    Feature   Description   *   Wildcard. Specifies all valid values.    ,   List. Use to specify several values (e.g. 1,15,30).    -   Range. Use to specify a range of values (e.g. 1-15).    /   Step. Use after a wildcard or range to specify the “step” between values. For example, 0-11/2 is equivalent to 0,2,4,6,8,10.  Note that you can specify the day using two fields: day of month and day of week. For most situations, we recommend that you use just one of these fields and leave the other as *.  If you use a non-wildcard value in both fields, the monitor runs when either field matches the time. For example, 15 2 1,15 * 1 causes the monitor to run at 2:15 AM on the 1st of the month, the 15th of the month, and every Monday.  Sample expressions  Every other day at 1:45 PM:  45 13 1-31/2 * * Every 10 minutes on Saturday and Sunday:  0/10 * * * 6-7 Every three hours on the first day of every other month:  0 0-23/3 1 1-12/2 *  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/alerting/cron/",
      "relUrl": "/docs/alerting/cron/"
    },
  
    {
      "id": "10",
      "title": "Cross-Cluster Search",
      "content": "Cross-cluster search Cross-cluster search is exactly what it sounds like: it lets any node in a cluster execute search requests across other clusters. The Security plugin supports cross-cluster search out of the box.  Authentication flow  When accessing a remote cluster from a coordinating cluster using cross-cluster search:   The Security plugin authenticates the user on the coordinating cluster. The Security plugin fetches the users backend roles on the coordinating cluster. The call including the authenticated user is forwarded to the remote cluster. The user’s permissions are evaluated on the remote cluster. While you can have different authentication and authorization configurations on the remote and coordinating cluster, we recommend using the same settings on both.  Permissions  To query indices on remote clusters, users need to have the following permissions for the index, in addition to READ or SEARCH permissions:  indices:admin/shards/search_shards Sample configuration  humanresources: cluster:   - CLUSTER_COMPOSITE_OPS_RO indices:   &#39;humanresources&#39;:   &#39;*&#39;:   - READ   - indices:admin/shards/search_shards # needed for CCS  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/security/cross-cluster-search/",
      "relUrl": "/docs/security/cross-cluster-search/"
    },
  
    {
      "id": "11",
      "title": "Create Dashboards",
      "content": "PerfTop dashboards Dashboards are defined in JSON and composed of three main elements: tables, line graphs, and bar graphs. You define a grid of rows and columns and then place elements within that grid, with each element spanning as many rows and columns as you specify.  The best way to get started with building custom dashboards is to duplicate and modify one of the existing JSON files in the dashboards directory. Summary of elements Position elements Add queries Add options  All elements   Tables   Bars   Lines  Summary of elements   Tables show metrics per dimension. For example, if your metric is CPU_Utilization and your dimension ShardID, a PerfTop table shows a row for each shard on each node. Bar graphs are aggregated for the cluster, unless you add nodeName to the dashboard. See the options for all elements. Line graphs are aggregated for each node. Each line represents a node. Position elements  PerfTop positions elements within a grid. For example, consider this 12 * 12 grid.  The upper-left of the grid represents row 0, column 0, so the starting positions for the three boxes are:   Orange: row 0, column 0 Purple: row 2, column 2 Green: row 1, column 6 These boxes span a number of rows and columns. In this case:   Orange: 2 rows, 4 columns Purple: 1 row, 4 columns Green: 3 rows, 2 columns In JSON form, we have the following:  { &quot;gridOptions&quot;: {   &quot;rows&quot;: 12,   &quot;cols&quot;: 12 }, &quot;graphs&quot;: {   &quot;tables&quot;: [{   &quot;options&quot;: {  &quot;gridPosition&quot;: {  &quot;row&quot;: 0,  &quot;col&quot;: 0,  &quot;rowSpan&quot;: 2,  &quot;colSpan&quot;: 4  }   }   },   {   &quot;options&quot;: {  &quot;gridPosition&quot;: {  &quot;row&quot;: 2,  &quot;col&quot;: 2,  &quot;rowSpan&quot;: 1,  &quot;colSpan&quot;: 4  }   }   },   {   &quot;options&quot;: {  &quot;gridPosition&quot;: {  &quot;row&quot;: 1,  &quot;col&quot;: 6,  &quot;rowSpan&quot;: 3,  &quot;colSpan&quot;: 2  }   }   }   ] } } At this point, however, all the JSON does is define the size and position of three tables.  To fill elements with data, you specify a query.  Add queries  Queries use the same elements as the REST API, just in JSON form:  { &quot;queryParams&quot;: {   &quot;metrics&quot;: &quot;estimated,limitConfigured&quot;,   &quot;aggregates&quot;: &quot;avg,avg&quot;,   &quot;dimensions&quot;: &quot;type&quot;,   &quot;sortBy&quot;: &quot;estimated&quot; } } For details on available metrics, see Metrics reference.  Add options  Options include labels, colors, and a refresh interval. Different elements types have different options.  Dashboards support the 16 ANSI colors: black, red, green, yellow, blue, magenta, cyan, and white. For the “bright” variants of these colors, use the numbers 8–15. If your terminal supports 256 colors, you can also use hex codes (e.g. #6D40ED).  All elements    Option   Type   Description   label   String or integer   The text in the upper-left corner of the box.    labelColor   String or integer   The color of the label.    refreshInterval   Integer   The number of milliseconds between calls to the Performance Analyzer API for new data. Minimum value is 5000.    dimensionFilters   String array   The dimension value to diplay for the graph. For example, if you query for metric=Net_Throughput&amp;amp;agg=sum&amp;amp;dim=Direction and the possible dimension values are in and out, you can define dimensionFilters: [&quot;in&quot;] to only display the metric data for in dimension    nodeName   String   If non-null, lets you restrict elements to individual nodes. You can specify the node name directly in the dashboard file, but the better approach is to use &quot;nodeName&quot;: &quot;#nodeName&quot; in the dashboard and include the --nodename &amp;lt;node_name&amp;gt; argument when starting PerfTop.  Tables    Option   Type   Description   bg   String or integer   The background color.    fg   String or integer   The text color.    selectedFg   String or integer   The text color for focused text.    selectedBg   String or integer   The background color for focused text.    columnSpacing   Integer   The amount of space (measured in characters) between columns.    keys   Boolean   Has no impact at this time.  Bars    Option   Type   Description   barWidth   Integer   The width of each bar (measured in characters) in the graph.    xOffset   Integer   The amount of space (measured in characters) between the y-axis and the first bar in the graph.    maxHeight   Integer   The maximum height of each bar (measured in characters) in the graph.  Lines    Option   Type   Description   showNthLabel   Integer   Which of the xAxis labels to show. For example, &quot;showNthLabel&quot;: 2 shows every other label.    showLegend   Boolean   Whether or not to display a legend for the line graph.    legend.width   Integer   The width of the legend (measured in characters) in the graph.    xAxis   String array   Array of labels for the x-axis. For example, [&quot;0:00&quot;, &quot;0:10&quot;, &quot;0:20&quot;, &quot;0:30&quot;, &quot;0:40&quot;, &quot;0:50&quot;].    colors   String array   Array of line colors to choose from. For example, [&quot;magenta&quot;, &quot;cyan&quot;]. If you don’t provide this value, PerfTop chooses random colors for each line. ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/pa/dashboards/",
      "relUrl": "/docs/pa/dashboards/"
    },
  
    {
      "id": "12",
      "title": "Debian Package",
      "content": "Debian package Installing and running Open Distro for Elasticsearch from an Debian package is a more manual process than the Docker image. We recommend Ubuntu 16.04 or 18.04, but any Debian-based distribution that uses systemd should work.  These steps assume you’re using Ubuntu 18.04. Install Java 11:  sudo add-apt-repository ppa:openjdk-r/ppa sudo apt update sudo apt install openjdk-11-jdk   Download and add signing keys for the repositories:  wget -qO - https://d3g5vo6xdbdb9a.cloudfront.net/GPG-KEY-opendistroforelasticsearch | sudo apt-key add -   Add the repositories:  echo &quot;deb https://d3g5vo6xdbdb9a.cloudfront.net/apt stable main&quot; | sudo tee -a /etc/apt/sources.list.d/opendistroforelasticsearch.list   Install Elasticsearch OSS:  wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-6.7.1.deb sudo dpkg  -i elasticsearch-oss-6.7.1.deb   Install Open Distro for Elasticsearch:  sudo apt-get update sudo apt install opendistroforelasticsearch   To start Open Distro for Elasticsearch:  sudo systemctl start elasticsearch.service   Send requests to the server to verify that Elasticsearch is up and running:  curl -XGET https://localhost:9200 -u admin:admin --insecure curl -XGET https://localhost:9200/_cat/nodes?v -u admin:admin --insecure curl -XGET https://localhost:9200/_cat/plugins?v -u admin:admin --insecure   For instructions on installing and running Kibana, see Kibana.  To check the status of the service:  systemctl status elasticsearch.service   To stop Open Distro for Elasticsearch:  sudo systemctl stop elasticsearch.service  Configuration  To run Open Distro for Elasticsearch when the system starts:  sudo /bin/systemctl daemon-reload sudo /bin/systemctl enable elasticsearch.service You can also modify the values in /etc/default/elasticsearch (JAVA_HOME, most notably), /etc/elasticsearch/elasticsearch.yml, and /etc/elasticsearch/jvm.options (to set the heap size, most notably). To learn more, see Elasticsearch configuration and Important Settings on the Docker page.  Where are the files?  The Debian package installs files to the following locations:    File type   Location   Elasticsearch home, management scripts, and plugins   /usr/share/elasticsearch/    Configuration files   /etc/elasticsearch    Environment variables   /etc/default/elasticsearch    Logs   /var/log/elasticsearch    Shard data   /var/lib/elasticsearch  Notes on Debian  If you are using Debian rather than Ubuntu, you likely need to make some modifications to the install process. When installing Java 11, rather than sudo add-apt-repository ppa:openjdk-r/ppa, run:  sudo echo &#39;deb http://deb.debian.org/debian stretch-backports main&#39; &amp;gt; /etc/apt/sources.list.d/backports.list   Before installing Open Distro for Elasticsearch, run:  apt install apt-transport-https ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/install/deb/",
      "relUrl": "/docs/install/deb/"
    },
  
    {
      "id": "13",
      "title": "Default Action Groups",
      "content": "Default action groups This page catalogs all default action groups. Often, the most coherent way to create new action groups is to use a combination of these default groups and individual permissions.  General    Name   Description   UNLIMITED   Grants complete access. Can be used on an cluster- or index-level. Equates to &quot;*&quot;.  Cluster-level    Name   Description   CLUSTER_ALL   Grants all cluster permissions. Equates to cluster:*.    CLUSTER_MONITOR   Grants all cluster monitoring permissions. Equates to cluster:monitor/*.    CLUSTER_COMPOSITE_OPS_RO   Grants read-only permissions to execute requests like mget, msearch, or mtv, plus permissions to query for aliases.    CLUSTER_COMPOSITE_OPS   Same as CLUSTER_COMPOSITE_OPS_RO, but also grants bulk permissions and all aliases permissions.    MANAGE_SNAPSHOTS   Grants permissions to manage snapshots and repositories.  Index-level    Name   Description   INDICES_ALL   Grants all permissions on the index. Equates to indices:*.    GET   Grants permissions to use get and mget actions only.    READ   Grants read permissions such as search, get field mappings, get, and mget.    WRITE   Grants write permissions to documents.    DELETE   Grants permissions to delete documents.    CRUD   Combines the READ, WRITE and DELETE action groups.    SEARCH   Grants permissions to search documents. Includes SUGGEST.    SUGGEST   Grants permissions to use the suggest API. Included in the READ action group.    CREATE_INDEX   Grants permissions to create indices and mappings.    INDICES_MONITOR   Grants permissions to execute all index monitoring actions (e.g. recovery, segments info, index stats, and status).    MANAGE_ALIASES   Grants permissions to manage aliases.    MANAGE   Grants all monitoring and administration permissions for indices. ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/security/default-action-groups/",
      "relUrl": "/docs/security/default-action-groups/"
    },
  
    {
      "id": "14",
      "title": "Disable Security",
      "content": "Disable security You might want to temporarily disable the Security plugin to make testing or internal usage more straightforward. To disable the plugin, add the following line in elasticsearch.yml:  opendistro_security.disabled: true A more permanent option is to remove the Security plugin entirely. Delete the plugins/opendistro_security folder on all nodes, and delete the opendistro_security configuration entries from elasticsearch.yml.  Disabling or removing the plugin exposes the configuration index for the Security plugin. If the index contains sensitive information, be sure to protect it through some other means. If you no longer need the index, delete it.  Remove Kibana plugin  The Security plugin is actually two plugins: one for Elasticsearch and one for Kibana. You can use the Elasticsearch plugin independently, but the Kibana plugin depends on a secured Elasticsearch cluster.  If you disable the Security plugin in elasticsearch.yml (or delete the plugin entirely) and still want to use Kibana, you must remove the corresponding Kibana plugin. To learn more, see Standalone Kibana plugin install.  RPM   Remove all opendistro_security lines from kibana.yml. Change elasticsearch.url in kibana.yml to http:// rather than https://. sudo /usr/share/kibana/bin/kibana-plugin remove opendistro_security. sudo systemctl restart kibana.service Docker Create a new Dockerfile:  FROM amazon/opendistro-for-elasticsearch-kibana:0.9.0 RUN /usr/share/kibana/bin/kibana-plugin remove opendistro_security   To build the new Docker image, run:  docker build --tag=kibana-no-security .  In docker-compose.yml, change amazon/opendistro-for-elasticsearch-kibana:0.9.0 to kibana-no-security. Change ELASTICSEARCH_URL (docker-compose.yml) or elasticsearch.url (your custom kibana.yml) to http:// rather than https://. Change ELASTICSEARCH_HOSTS or elasticsearch.hosts to http:// rather than https://. Remove all opendistro_security lines from kibana.yml. docker-compose up.  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/security/disable/",
      "relUrl": "/docs/security/disable/"
    },
  
    {
      "id": "15",
      "title": "Docker Security Configuration",
      "content": "Docker security configuration Before deploying to a production environment, you should replace the demo security certificates with your own. With the RPM-based installation, you have direct access to the file system, but the Docker image requires modifying the Docker Compose file to include the replacement files.  Sample Docker Compose file  version: &#39;3&#39; services: odfe-node1:   image: amazon/opendistro-for-elasticsearch:0.9.0   container_name: odfe-node1   environment:   - cluster.name=odfe-cluster   - bootstrap.memory_lock=true # along with the memlock settings below, disables swapping   - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot; # minimum and maximum Java heap size, recommend setting both to 50% of system RAM   - network.host=0.0.0.0 # required if not using the demo Security configuration   ulimits:   memlock:   soft: -1   hard: -1   volumes:   - odfe-data1:/usr/share/elasticsearch/data   - ./root-ca.pem:/usr/share/elasticsearch/config/root-ca.pem   - ./esnode.pem:/usr/share/elasticsearch/config/esnode.pem   - ./esnode-key.pem:/usr/share/elasticsearch/config/esnode-key.pem   - ./kirk.pem:/usr/share/elasticsearch/config/kirk.pem   - ./kirk-key.pem:/usr/share/elasticsearch/config/kirk-key.pem   - ./custom-elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml   - ./internal_users.yml:/usr/share/elasticsearch/plugins/opendistro_security/securityconfig/internal_users.yml   ports:   - 9200:9200   - 9600:9600 # required for Performance Analyzer   networks:   - odfe-net odfe-node2:   image: amazon/opendistro-for-elasticsearch:0.9.0   container_name: odfe-node2   environment:   - cluster.name=odfe-cluster   - bootstrap.memory_lock=true   - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;   - discovery.zen.ping.unicast.hosts=odfe-node1   - network.host=0.0.0.0   ulimits:   memlock:   soft: -1   hard: -1   volumes:   - odfe-data2:/usr/share/elasticsearch/data   - ./root-ca.pem:/usr/share/elasticsearch/config/root-ca.pem   - ./esnode.pem:/usr/share/elasticsearch/config/esnode.pem   - ./esnode-key.pem:/usr/share/elasticsearch/config/esnode-key.pem   - ./kirk.pem:/usr/share/elasticsearch/config/kirk.pem   - ./kirk-key.pem:/usr/share/elasticsearch/config/kirk-key.pem   - ./custom-elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml   - ./internal_users.yml:/usr/share/elasticsearch/plugins/opendistro_security/securityconfig/internal_users.yml   networks:   - odfe-net kibana:   image: amazon/opendistro-for-elasticsearch-kibana:0.9.0   container_name: odfe-kibana   ports:   - 5601:5601   expose:   - &quot;5601&quot;   environment:   ELASTICSEARCH_URL: https://odfe-node1:9200   ELASTICSEARCH_HOSTS: https://odfe-node1:9200   volumes:   - ./custom-kibana.yml:/usr/share/kibana/config/kibana.yml   networks:   - odfe-net  volumes: odfe-data1: odfe-data2:  networks: odfe-net: Then make your changes to elasticsearch.yml. For a full list of settings, see Security. This example adds (extremely) verbose audit logging:  opendistro_security.ssl.transport.pemcert_filepath: esnode.pem opendistro_security.ssl.transport.pemkey_filepath: esnode-key.pem opendistro_security.ssl.transport.pemtrustedcas_filepath: root-ca.pem opendistro_security.ssl.transport.enforce_hostname_verification: false opendistro_security.ssl.http.enabled: true opendistro_security.ssl.http.pemcert_filepath: esnode.pem opendistro_security.ssl.http.pemkey_filepath: esnode-key.pem opendistro_security.ssl.http.pemtrustedcas_filepath: root-ca.pem opendistro_security.allow_unsafe_democertificates: true opendistro_security.allow_default_init_securityindex: true opendistro_security.authcz.admin_dn: - CN=kirk,OU=client,O=client,L=test, C=de  opendistro_security.audit.type: internal_elasticsearch opendistro_security.enable_snapshot_restore_privilege: true opendistro_security.check_snapshot_restore_write_privileges: true opendistro_security.restapi.roles_enabled: [&quot;all_access&quot;, &quot;security_rest_api_access&quot;] cluster.routing.allocation.disk.threshold_enabled: false opendistro_security.audit.config.disabled_rest_categories: NONE opendistro_security.audit.config.disabled_transport_categories: NONE To start the cluster, run docker-compose up.  If you encounter any File /usr/share/elasticsearch/config/elasticsearch.yml has insecure file permissions (should be 0600) messages, you can use chmod to set file permissions before running docker-compose up. Docker Compose passes files to the container as-is.  Change passwords for read-only users  After the cluster starts, change the passwords for the read-only user accounts: admin and kibanaserver.   The admin user has full privileges on the cluster. kibanaserver user has certain permissions to the .kibana index that let it perform management tasks like setting index patterns and retrieving visualizations. This user, or one just like it, is required for Kibana to work properly with the Security plugin. We recommend just using kibanaserver.  Regardless of the authentication method that you choose for other users (e.g. Open ID Connect), the Kibana server user always passes its credentials to Elasticsearch using HTTP basic authentication headers, as set in kibana.yml.  To generate new passwords, run docker ps to find the odfe-node1 container ID. Then run:  $ docker exec &amp;lt;container-id&amp;gt; /bin/sh /usr/share/elasticsearch/plugins/opendistro_security/tools/hash.sh -p newpassword If you encounter a permissions error, run docker exec &amp;lt;container-id&amp;gt; chmod +x /usr/share/elasticsearch/plugins/opendistro_security/tools/hash.sh  The hash script returns a hashed password (e.g. $2y$12$SFNvhLHf7MPCpRCq00o/BuU8GMdcD.7BymhT80YHNISBHsfJwhTou), which you can then copy and paste into internal_users.yml. Repeat the process as necessary for all read-only users. Don’t worry about the other user accounts; you can change (or delete) them in Kibana.  When you’re satisfied, modify elasticsearch.password in custom-kibana.yml to include the new kibanaserver password. Then restart the cluster using docker-compose down -v and docker-compose up. The -v is critical in this case.  internal_users.yml looks like this:  # New password applied admin: readonly: true hash: $2y$12$SFNvhLHf7MPCpRCq00o/BuU8GMdcD.7BymhT80YHNISBHsfJwhTou roles:   - admin attributes:   #no dots allowed in attribute names   attribute1: value1   attribute2: value2   attribute3: value3  # Still using default password: logstash logstash: hash: $2a$12$u1ShR4l4uBS3Uv59Pa2y5.1uQuZBrZtmNfqB3iM/.jL0XoV9sghS2 roles:   - logstash  # New password applied kibanaserver: readonly: true hash: $2a$12$4AcgAt3xwOWadA5s5blL6ev39OXDNhmOesEoo33eZtrq2N0YrU3H.  # Still using default password: kibanaro kibanaro: hash: $2a$12$JJSXNfTowz7Uu5ttXfeYpeYE0arACvcwlPBStB1F.MI7f0U9Z4DGC roles:   - kibanauser   - readall  # Still using default password: readall readall: hash: $2a$12$ae4ycwzwvLtZxwZ82RmiEunBbIPiAmGZduBAjKN0TXdwQFtCwARz2 #password is: readall roles:   - readall  # Still using default password: snapshotrestore snapshotrestore: hash: $2y$12$DpwmetHKwgYnorbgdvORCenv4NAK8cPUg8AI6pxLCuWf/ALc0.v7W roles:   - snapshotrestore Next steps  After the cluster starts, verify the new password:  curl -XGET https://localhost:9200 -u admin:admin -k Unauthorized  curl -XGET https://localhost:9200 -u admin:newpassword -k { ... &quot;tagline&quot; : &quot;You Know, for Search&quot; } Then you can open Kibana at http://localhost:5601, sign in, and perform additional user management in the Security panel.  You can use this same override process to specify new authentication settings in /usr/share/elasticsearch/plugins/opendistro_security/securityconfig/config.yml. ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/install/docker-security/",
      "relUrl": "/docs/install/docker-security/"
    },
  
    {
      "id": "16",
      "title": "Docker",
      "content": "Docker image You can pull the Open Distro for Elasticsearch Docker image just like any other image:  docker pull amazon/opendistro-for-elasticsearch:0.9.0 docker pull amazon/opendistro-for-elasticsearch-kibana:0.9.0 Open Distro for Elasticsearch images use centos:7 as the base image. Run the image Start a cluster Configure Elasticsearch Bash access to containers Important settings Run with custom plugins   Run the image  To run the image for local development:  docker run -p 9200:9200 -p 9600:9600 -e &quot;discovery.type=single-node&quot; amazon/opendistro-for-elasticsearch:0.9.0 Then send requests to the server to verify that Elasticsearch is up and running:  curl -XGET https://localhost:9200 -u admin:admin --insecure curl -XGET https://localhost:9200/_cat/nodes?v -u admin:admin --insecure curl -XGET https://localhost:9200/_cat/plugins?v -u admin:admin --insecure To find the container ID:  docker ps Then you can stop the container using:  docker stop &amp;lt;container-id&amp;gt; Start a cluster  To deploy the image across multiple nodes for a production workload, create a docker-compose.yml file appropriate for your environment and run:  docker-compose up To stop the cluster, run:  docker-compose down To stop the cluster and delete all data volumes, run:  docker-compose down -v Sample Docker Compose file  This sample file starts two data nodes and Kibana. If you’re running Docker locally, we recommend allowing Docker to use at least 4 GB of RAM in Preferences &amp;gt; Advanced.  version: &#39;3&#39; services: odfe-node1:   image: amazon/opendistro-for-elasticsearch:0.9.0   container_name: odfe-node1   environment:   - cluster.name=odfe-cluster   - bootstrap.memory_lock=true # along with the memlock settings below, disables swapping   - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot; # minimum and maximum Java heap size, recommend setting both to 50% of system RAM   ulimits:   memlock:   soft: -1   hard: -1   volumes:   - odfe-data1:/usr/share/elasticsearch/data   ports:   - 9200:9200   - 9600:9600 # required for Performance Analyzer   networks:   - odfe-net odfe-node2:   image: amazon/opendistro-for-elasticsearch:0.9.0   container_name: odfe-node2   environment:   - cluster.name=odfe-cluster   - bootstrap.memory_lock=true   - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;   - discovery.zen.ping.unicast.hosts=odfe-node1   ulimits:   memlock:   soft: -1   hard: -1   volumes:   - odfe-data2:/usr/share/elasticsearch/data   networks:   - odfe-net kibana:   image: amazon/opendistro-for-elasticsearch-kibana:0.9.0   container_name: odfe-kibana   ports:   - 5601:5601   expose:   - &quot;5601&quot;   environment:   ELASTICSEARCH_URL: https://odfe-node1:9200   ELASTICSEARCH_HOSTS: https://odfe-node1:9200   networks:   - odfe-net  volumes: odfe-data1: odfe-data2:  networks: odfe-net: If you override kibana.yml settings using environment variables, as seen above, use all uppercase letters and periods in place of underscores (e.g. for elasticsearch.url, specify ELASTICSEARCH_URL).  Configure Elasticsearch  You can pass a custom elasticsearch.yml file to the Docker container using the -v flag for docker run:  docker run -p 9200:9200 -p 9600:9600 -e &quot;discovery.type=single-node&quot; -v /&amp;lt;full-path-to&amp;gt;/custom-elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml amazon/opendistro-for-elasticsearch:0.9.0 You can perform the same operation in docker-compose.yml using a relative path:  services: odfe-node1:   volumes:   - odfe-data1:/usr/share/elasticsearch/data   - ./custom-elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml odfe-node2:   volumes:   - odfe-data2:/usr/share/elasticsearch/data   - ./custom-elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml kibana:   volumes:   - ./custom-kibana.yml:/usr/share/kibana/config/kibana.yml You can use this same method to pass your own certificates for use with the Security plugin.  Bash access to containers  To create an interactive Bash session in a container, run docker ps to find the container ID. Then run:  docker exec -it &amp;lt;container-id&amp;gt; /bin/bash Important settings  For production workloads, make sure the Linux setting vm.max_map_count is set to at least 262144. On the Open Distro for Elasticsearch Docker image, this setting is the default. To verify, start a Bash session in the container and run:  cat /proc/sys/vm/max_map_count To increase this value, you have to modify the Docker image. On the RPM install, you can add this setting to the host machine’s /etc/sysctl.conf file by adding the following line:  vm.max_map_count=262144 Then run sudo sysctl -p to reload.  The docker-compose.yml file above also contains several key settings: bootstrap.memory_lock=true, ES_JAVA_OPTS=-Xms512m -Xmx512m, and 9600:9600. Respectively, these settings disable memory swapping (along with memlock), set the size of the Java heap (we recommend half of system RAM), and allow you to access Performance Analyzer on port 9600.  Run with custom plugins  To run the image with a custom plugin, first create a Dockerfile:  FROM amazon/opendistro-for-elasticsearch:0.9.0 RUN /usr/share/elasticsearch/bin/elasticsearch-plugin install --batch &amp;lt;plugin-name-or-url&amp;gt; Then run the following commands:  docker build --tag=odfe-custom-plugin . docker run -p 9200:9200 -p 9600:9600 -v /usr/share/elasticsearch/data odfe-custom-plugin You can also use a Dockerfile to pass your own certificates for use with the Security plugin, similar to the -v argument in Configure Elasticsearch:  FROM amazon/opendistro-for-elasticsearch:0.9.0 COPY --chown=elasticsearch:elasticsearch elasticsearch.yml /usr/share/elasticsearch/config/ COPY --chown=elasticsearch:elasticsearch my-key-file.pem /usr/share/elasticsearch/config/ COPY --chown=elasticsearch:elasticsearch my-certificate-chain.pem /usr/share/elasticsearch/config/ COPY --chown=elasticsearch:elasticsearch my-root-cas.pem /usr/share/elasticsearch/config/  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/install/docker/",
      "relUrl": "/docs/install/docker/"
    },
  
    {
      "id": "17",
      "title": "Document-Level Security",
      "content": "Document-level security Document-level security allows for a role to grant a permissions to operate on a subset of documents in an index. The easiest way to get started with document- and field-level security is open Kibana and choose Security. Then choose Roles, create a new role, and choose DLS/FLS.  Simple roles  Use the Elasticsearch query DSL to define which documents a role grants access to. In the REST API, you provide the query as a string, so you have to escape your quotes.  This role allows a user to read any document in any index with the field public set to true:  PUT _opendistro/_security/api/roles/public_data { &quot;cluster&quot; : [ &quot;*&quot; ], &quot;indices&quot; : {   &quot;pub*&quot; : {   &quot;*&quot; : [ &quot;READ&quot; ],   &quot;_dls_&quot;: &quot;{ &quot;term &quot;: {  &quot;public &quot;: true}}&quot;   } } } Parameter substitution  A number of variables exist that you can use to enforce rules based on the properties of a user. For example, ${user.name} is replaced with the name of the current user.  This rule would allow a user to read any document where there user-name was a value of the readable_by field:  PUT _opendistro/_security/api/roles/user_data { &quot;cluster&quot; : [ &quot;*&quot; ], &quot;indices&quot; : {   &quot;pub*&quot; : {   &quot;*&quot; : [ &quot;READ&quot; ],   &quot;_dls_&quot;: &quot;{  &quot;term &quot;: {  &quot;readable_by &quot;:  &quot;${user.name} &quot;}}&quot;   } } } The following substitutions exist:    Term   Replaced with   ${user.name}   Username.    ${user.roles}   A comma-separated, quoted list of user roles.    ${attr.&amp;lt;TYPE&amp;gt;.&amp;lt;NAME&amp;gt;}   An attribute with name &amp;lt;NAME&amp;gt; defined for a user. &amp;lt;TYPE&amp;gt; is internal, jwt or ldap  Attribute-based security  You can use roles and parameter substitution with the terms_set query to enable attribute-based security.  User definition  PUT _opendistro/_security/api/internalusers/user1 { &quot;password&quot;: &quot;asdf&quot;, &quot;roles&quot;: [&quot;abac&quot;],  &quot;attributes&quot;: {  &quot;permissions&quot;: &quot; &quot;att1 &quot;,  &quot;att2 &quot;,  &quot;att3 &quot;&quot;  } } Role definition  PUT _opendistro/_security/api/roles/abac {   &quot;indices&quot; : {   &quot;*&quot; : {   &quot;*&quot; : [&quot;READ&quot;],   &quot;_dls_&quot;: &quot;{ &quot;terms_set &quot;: { &quot;security_attributes &quot;: { &quot;terms &quot;: [${attr.internal.permissions}],  &quot;minimum_should_match_script &quot;: { &quot;source &quot;:  &quot;doc[&#39;security_attributes&#39;].values.length &quot;}}}}&quot;   }   } }  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/security/document-level-security/",
      "relUrl": "/docs/security/document-level-security/"
    },
  
    {
      "id": "18",
      "title": "Encryption at Rest",
      "content": "Encryption at rest The operating system for each Open Distro for Elasticsearch node handles encryption of data at rest. To enable encryption at rest in most Linux distributions, use the cryptsetup command:  cryptsetup luksFormat --key-file &amp;lt;key&amp;gt; &amp;lt;partition&amp;gt; For full documentation on the command, see the Linux man page. ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/install/encryption-at-rest/",
      "relUrl": "/docs/install/encryption-at-rest/"
    },
  
    {
      "id": "19",
      "title": "Full-Text Queries",
      "content": "Full-text queries Although you can use HTTP request parameters to perform simple searches, the Elasticsearch query domain-specific language (DSL) lets you specify the full range of search options. The query DSL uses the HTTP request body. Queries specified in this way have the added advantage of being more explicit in their intent and easier to tune over time.  This page lists all full-text query types and common options. Given the sheer number of options and subtle behaviors, the best method of ensuring useful search results is to test different queries against representative indices and verify the output. Match Multi match Match phrase Common terms Query string Simple query string Match all Match none Options   Match  Creates a boolean query that returns results if the search term is present in the field.  The most basic form of the query provides only a field (title) and a term (wind):  GET _search { &quot;query&quot;: {   &quot;match&quot;: {   &quot;title&quot;: &quot;wind&quot;   } } } For an example that uses curl, try:  curl --insecure -XGET -u admin:admin https://&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;/&amp;lt;index&amp;gt;/_search   -H &quot;content-type: application/json&quot;   -d&#39;{   &quot;query&quot;: {   &quot;match&quot;: {   &quot;title&quot;: &quot;wind&quot;   }   } }&#39; The query accepts the following options. For descriptions of each, see Options.  GET _search { &quot;query&quot;: {   &quot;match&quot;: {   &quot;title&quot;: {   &quot;query&quot;: &quot;wind&quot;,   &quot;fuzziness&quot;: &quot;AUTO&quot;,   &quot;fuzzy_transpositions&quot;: true,   &quot;operator&quot;:  &quot;or&quot;,   &quot;minimum_should_match&quot;: 1,   &quot;analyzer&quot;: &quot;standard&quot;,   &quot;zero_terms_query&quot;: &quot;none&quot;,   &quot;lenient&quot;: false,   &quot;cutoff_frequency&quot;: 0.01,   &quot;prefix_length&quot;: 0,   &quot;max_expansions&quot;: 50,   &quot;boost&quot;: 1   }   } } } Multi match  Like the match query, but searches multiple fields.  The ^ lets you “boost” certain fields. Boosts are multipliers that weigh matches in one field more heavily than matches in other fields. In the following example, a match for “wind” in the title field influences _score four times as much as a match in the plot field. The result is that films like The Wind Rises and Gone with the Wind are near the top of the search results, and films like Twister and Sharknado, which presumably have “wind” in their plot summaries, are near the bottom.  GET _search { &quot;query&quot;: {   &quot;multi_match&quot;: {   &quot;query&quot;: &quot;wind&quot;,   &quot;fields&quot;: [&quot;title^4&quot;, &quot;plot&quot;]   } } } The query accepts the following options. For descriptions of each, see Options.  GET _search { &quot;query&quot;: {   &quot;multi_match&quot;: {   &quot;query&quot;: &quot;wind&quot;,   &quot;fields&quot;: [&quot;title^4&quot;, &quot;description&quot;],   &quot;type&quot;: &quot;most_fields&quot;,   &quot;operator&quot;: &quot;and&quot;,   &quot;minimum_should_match&quot;: 3,   &quot;tie_breaker&quot;: 0.0,   &quot;analyzer&quot;: &quot;standard&quot;,   &quot;boost&quot;: 1,   &quot;fuzziness&quot;: &quot;AUTO&quot;,   &quot;fuzzy_transpositions&quot;: true,   &quot;lenient&quot;: false,   &quot;prefix_length&quot;: 0,   &quot;max_expansions&quot;: 50,   &quot;auto_generate_synonyms_phrase_query&quot;: true,   &quot;cutoff_frequency&quot;: 0.01,   &quot;zero_terms_query&quot;: &quot;none&quot;   } } } Match phrase  Creates a phrase query that matches a sequence of terms.  GET _search { &quot;query&quot;: {   &quot;match_phrase&quot;: {   &quot;title&quot;: &quot;the wind rises&quot;   } } } The query accepts the following options. For descriptions of each, see Options.  GET _search { &quot;query&quot;: {   &quot;match_phrase&quot;: {   &quot;title&quot;: {   &quot;query&quot;: &quot;wind rises the&quot;,   &quot;slop&quot;: 3,   &quot;analyzer&quot;: &quot;standard&quot;,   &quot;zero_terms_query&quot;: &quot;none&quot;   }   } } } Common terms  The common terms query separates the query string into high- and low-frequency terms based on number of occurrences on the shard. Low-frequency terms are weighed more heavily in the results, and high-frequency terms are considered only for documents that already matched one or more low-frequency terms. In that sense, you can think of this query as having a built-in, ever-changing list of stop words.  GET _search { &quot;query&quot;: {   &quot;common&quot;: {   &quot;title&quot;: {   &quot;query&quot;: &quot;the wind rises&quot;   }   } } } The query accepts the following options. For descriptions of each, see Options.  GET _search { &quot;query&quot;: {   &quot;common&quot;: {   &quot;title&quot;: {   &quot;query&quot;: &quot;the wind rises&quot;,   &quot;cutoff_frequency&quot;: 0.002,   &quot;low_freq_operator&quot;: &quot;or&quot;,   &quot;boost&quot;: 1,   &quot;analyzer&quot;: &quot;standard&quot;,   &quot;minimum_should_match&quot;: {  &quot;low_freq&quot; : 2,  &quot;high_freq&quot; : 3   }   }   } } } Query string  The query string query splits text based on operators and analyzes each individually.  If you search using the HTTP request parameters (i.e. _search?q=wind), Elasticsearch creates a query string query.  GET _search { &quot;query&quot;: {   &quot;query_string&quot;: {   &quot;query&quot;: &quot;the wind AND (rises OR rising)&quot;   } } } The query accepts the following options. For descriptions of each, see Options.  GET _search { &quot;query&quot;: {   &quot;query_string&quot;: {   &quot;query&quot;: &quot;the wind AND (rises OR rising)&quot;,   &quot;default_field&quot;: &quot;title&quot;,   &quot;type&quot;: &quot;best_fields&quot;,   &quot;fuzziness&quot;: &quot;AUTO&quot;,   &quot;fuzzy_transpositions&quot;: true,   &quot;fuzzy_max_expansions&quot;: 50,   &quot;fuzzy_prefix_length&quot;: 0,   &quot;minimum_should_match&quot;: 1,   &quot;default_operator&quot;: &quot;or&quot;,   &quot;analyzer&quot;: &quot;standard&quot;,   &quot;lenient&quot;: false,   &quot;boost&quot;: 1,   &quot;allow_leading_wildcard&quot;: true,   &quot;enable_position_increments&quot;: true,   &quot;phrase_slop&quot;: 3,   &quot;max_determinized_states&quot;: 10000,   &quot;time_zone&quot;: &quot;-08:00&quot;,   &quot;quote_field_suffix&quot;: &quot;&quot;,   &quot;quote_analyzer&quot;: &quot;standard&quot;,   &quot;analyze_wildcard&quot;: false,   &quot;auto_generate_synonyms_phrase_query&quot;: true   } } } Simple query string  The simple query string query is like the query string query, but it lets advanced users specify many arguments directly in the query string. The query discards any invalid portions of the query string.  GET _search { &quot;query&quot;: {   &quot;simple_query_string&quot;: {   &quot;query&quot;: &quot; &quot;rises wind the &quot;~4 | *ising~2&quot;,   &quot;fields&quot;: [&quot;title&quot;]   } } }   Special character   Behavior   +   Acts as the and operator.    |   Acts as the or operator.    *   Acts as a wildcard.    &quot;&quot;   Wraps several terms into a phrase.    ()   Wraps a clause for precedence.    ~n   When used after a term (e.g. wnid~3), sets fuzziness. When used after a phrase, sets slop. See Options.    -   Negates the term.  The query accepts the following options. For descriptions of each, see Options.  GET _search { &quot;query&quot;: {   &quot;simple_query_string&quot;: {   &quot;query&quot;: &quot; &quot;rises wind the &quot;~4 | *ising~2&quot;,   &quot;fields&quot;: [&quot;title&quot;],   &quot;flags&quot;: &quot;ALL&quot;,   &quot;fuzzy_transpositions&quot;: true,   &quot;fuzzy_max_expansions&quot;: 50,   &quot;fuzzy_prefix_length&quot;: 0,   &quot;minimum_should_match&quot;: 1,   &quot;default_operator&quot;: &quot;or&quot;,   &quot;analyzer&quot;: &quot;standard&quot;,   &quot;lenient&quot;: false,   &quot;quote_field_suffix&quot;: &quot;&quot;,   &quot;analyze_wildcard&quot;: false,   &quot;auto_generate_synonyms_phrase_query&quot;: true   } } } Match all  Matches all documents. Can be useful for testing.  GET _search { &quot;query&quot;: {   &quot;match_all&quot;: {} } } Match none  Matches no documents. Rarely useful.  GET _search { &quot;query&quot;: {   &quot;match_none&quot;: {} } } Options    Option   Valid values   Description   fields   String array   The list of fields to search (e.g. &quot;fields&quot;: [&quot;title^4&quot;, &quot;description&quot;]). If unspecified, defaults to the index.query.default_field setting, which defaults to [&quot;*&quot;].    fuzziness   AUTO, 0, or a positive integer   The number of character edits (insert, delete, substitute) that it takes to change one word to another when determining whether a term matched a value. For example, the distance between wined and wind is 1. The default, AUTO, chooses a value based on the length of each term and is a good choice for most use cases.    fuzzy_transpositions   Boolean   Setting fuzzy_transpositions to true (default) adds swaps of adjacent characters to the insert, delete, and substitute operations of the fuzziness option. For example, the distance between wind and wnid is 1 if fuzzy_transpositions is true (swap “n” and “i”) and 2 if it is false (delete “n”, insert “n”). If fuzzy_transpositions is false, rewind and wnid have the same distance (2) from wind, despite the more human-centric opinion that wnid is an obvious typo. The default is a good choice for most use cases.    prefix_length   0 (default) or a positive integer   The number of leading characters that are not considered in fuzziness.    max_expansions   Positive integer   Fuzzy queries “expand to” a number of matching terms that are within the distance specified in fuzziness. Then Elasticsearch tries to match those terms against its indices. max_expansions specifies the maximum number of terms that the fuzzy query expands to. The default is 50.    operator   or, and   If the query string contains multiple search terms, whether all terms need to match (and) or only one term needs to match (or) for a document to be considered a match.    minimum_should_match   Positive or negative integer, positive or negative percentage, combination   If the query string contains multiple search terms and you used the or operator, the number of terms that need to match for the document to be considered a match. For example, if minimum_should_match is 2, “wind often rising” does not match “The Wind Rises.” If minimum_should_match is 1, it matches. This option also has low_freq and high_freq properties for Common Terms queries.    analyzer   standard, simple, whitespace, stop, keyword, pattern, &amp;lt;language&amp;gt;, fingerprint   The analyzer you want to use for the query. Different analyzers have different character filters, tokenizers, and token filters. The stop analyzer, for example, removes stop words (e.g. “an,” “but,” “this”) from the query string.    zero_terms_query   none, all   If the analyzer removes all terms from a query string, whether to match no documents (default) or all documents. For example, the stop analyzer removes all terms from the string “an but this.”    lenient   Boolean   Setting lenient to true lets you ignore data type mismatches between the query and the document field. For example, a query string of “8.2” could match a field of type float. The default is false.    cutoff_frequency   Between 0.0 and 1.0 or a positive integer   This value lets you define high and low frequency terms based on number of occurrences in the index. Numbers between 0 and 1 are treated as a percentage. For example, 0.10 is 10%. This value means that if a word occurs within the search field in more than 10% of the documents on the shard, Elasticsearch considers the word “high frequency” and deemphasizes it when calculating search score.  Because this setting is per shard, testing its impact on search results can be challenging unless a cluster has many documents.    auto_generate_synonyms_phrase_query   Boolean   A value of true (default) automatically generates phrase queries for multi-term synonyms. For example, if you have the synonym &quot;ba, batting average&quot; and search for “ba,” Elasticsearch searches for ba OR &quot;batting average&quot; (if this option is true) or ba OR (batting AND average) (if this option is false).    slop   0 (default) or a positive integer   Controls the degree to which words in a query can be misordered and still be considered a match. From the Lucene documentation: “The number of other words permitted between words in query phrase. For example, to switch the order of two words requires two moves (the first move places the words atop one another), so to permit re-orderings of phrases, the slop must be at least two. A value of zero requires an exact match.”    phrase_slop   0 (default) or a positive integer   See slop.    type   best_fields, most_fields, cross-fields, phrase, phrase_prefix   Determines how Elasticsearch executes the query and scores the results. The default is best_fields.    tie_breaker   0.0 (default) to 1.0   Changes the way Elasticsearch scores searches. For example, a type of best_fields typically uses the highest score from any one field. If you specify a tie_breaker value between 0.0 and 1.0, the score changes to highest score + tie_breaker * score for all other matching fields. If you specify a value of 1.0, Elasticsearch adds together the scores for all matching fields (effectively defeating the purpose of best_fields).    rewrite   constant_score, scoring_boolean, constant_score_boolean, top_terms_N, top_terms_boost_N, top_terms_blended_freqs_N   Determines how Elasticsearch rewrites and scores multi-term queries. The default is constant_score.    boost   Floating-point   Boosts the clause by the given multiplier. Useful for weighing clauses in compound queries. The default is 1.0.    low_freq_operator   and, or   The operator for low-frequency terms. The default is or. See Common Terms queries and operator in this table.    analyze_wildcard   Boolean   Whether Elasticsearch should attempt to analyze wildcard terms. Some analyzers do a poor job at this task, so the default is false.    quote_field_suffix   String   This option lets you search different fields depending on whether terms are wrapped in quotes. For example, if quote_field_suffix is &quot;.exact&quot; and you search for &quot;lightly&quot; (in quotes) in the title field, Elasticsearch searches the title.exact field. This second field might use a different type (e.g. keyword rather than text) or a different analyzer. The default is null.    flags   String   A |-delimited string of flags to enable (e.g. AND|OR|NOT). The default is ALL.    time_zone   UTC offset   The time zone to use (e.g. -08:00) if the query string contains a date range (e.g. &quot;query&quot;: &quot;wind rises release_date[2012-01-01 TO 2014-01-01]&quot;). The default is UTC.    max_determinized_states   Positive integer   The maximum number of “states” (a measure of complexity) that Lucene can create for query strings that contain regular expressions (e.g. &quot;query&quot;: &quot;/wind.+?/&quot;). Larger numbers allow for queries that use more memory. The default is 10,000.    enable_position_increments   Boolean   When true, result queries are aware of position increments. This setting is useful when the removal of stop words leaves an unwanted “gap” between terms. The default is true.    allow_leading_wildcard   Boolean   Whether * and ? are allowed as the first character of a search term. The default is true. ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/elasticsearch/full-text/",
      "relUrl": "/docs/elasticsearch/full-text/"
    },
  
    {
      "id": "20",
      "title": "Generate Certificates",
      "content": "Generate certificates If you don’t have access to a certificate authority (CA) for your organization and want to use Open Distro for Elasticsearch for non-demo purposes, you can generate your own self-signed certificates using OpenSSL.  You can probably find OpenSSL in the package manager for your operating system.  On CentOS, use Yum:  sudo yum install openssl On macOS, use Homebrew:  brew install openssl   Generate private key Generate root certificate Generate admin certificate (Optional) Generate node and client certificates  Sample script  Get Distinguished Names Configure certificates Run securityadmin.sh Kibana   Generate private key  The first step in this process is to generate a private key using the genrsa command. As the name suggests, you should keep this file private.  Private keys need to be of sufficient length in order to be secure, so specify 2048:  openssl genrsa -out root-ca-key.pem 2048 If desired, add the -aes256 option to encrypt the key using the AES-256 standard. This option requires a password.  Generate root certificate  Next, use the key to generate a self-signed certificate for the root CA:  openssl req -new -x509 -sha256 -key root-ca-key.pem -out root-ca.pem  The -x509 option specifies that you want a self-signed certificate rather than a certificate request. The -sha256 option sets the hash algorithm to SHA-256. SHA-256 is the default in newer versions of OpenSSL, but older versions might use SHA-1. Optionally, add -days 3650 (10 years) or some other number of days to set an expiration date. Specify details for your organization as prompted. Together, these details form the Distinguished Name (DN) of your CA.  Generate admin certificate  To generate an admin certificate, first create a new key:  openssl genrsa -out admin-key-temp.pem 2048 Then convert that key to PKCS#8 format for use in Java using a PKCS#12-compatible algorithm (3DES):  openssl pkcs8 -inform PEM -outform PEM -in admin-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out admin-key.pem Next, create a certificate signing request (CSR). This file acts as an application to a CA for a signed certificate:  openssl req -new -key admin-key.pem -out admin.csr Fill in the details as prompted. You don’t need to specify a challenge password. As noted in the OpenSSL Cookbook, “Having a challenge password does not increase the security of the CSR in any way.”  Finally, generate the certificate itself:  openssl x509 -req -in admin.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out admin.pem (Optional) Generate node and client certificates  Follow the steps in Generate admin certificates with new file names to generate a new certificate for each node and as many client certificates as you need. Each certificate should use its own private key.  If you generate node certificates and have opendistro_security.ssl.transport.enforce_hostname_verification set to true (default), be sure to specify a Common Name (CN) for the certificate that matches the hostname of the intended node. If you want to use the same node certificate on all nodes (not recommended), set hostname verification to false. To learn more, see Configure TLS certificates.  Sample script  # Root CA openssl genrsa -out root-ca-key.pem 2048 openssl req -new -x509 -sha256 -key root-ca-key.pem -out root-ca.pem # Admin cert openssl genrsa -out admin-key-temp.pem 2048 openssl pkcs8 -inform PEM -outform PEM -in admin-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out admin-key.pem openssl req -new -key admin-key.pem -out admin.csr openssl x509 -req -in admin.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out admin.pem # Node cert openssl genrsa -out node-key-temp.pem 2048 openssl pkcs8 -inform PEM -outform PEM -in node-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out node-key.pem openssl req -new -key node-key.pem -out node.csr openssl x509 -req -in node.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out node.pem # Cleanup rm admin-key-temp.pem rm admin.csr rm node-key-temp.pem rm node.csr Get Distinguished Names  If you created admin and node certificates, you need to specify their DNs in elasticsearch.yml:  opendistro_security.authcz.admin_dn: - &#39;CN=ADMIN,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA&#39; opendistro_security.nodes_dn: - &#39;CN=node1.example.com,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA&#39; But if you look at the subject of the certificate after creating it, you might see different formatting:  subject=/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=node1.example.com If you compare this string to the ones in elasticsearch.yml above, you can see that you need to invert the order of elements and use commas rather than slashes. To get the string you need:  openssl x509 -subject -nameopt RFC2253 -noout -in node.pem Then you can copy and paste the output:  subject= CN=node1.example.com,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA Configure certificates  This process generates many files, but the ones you need to add to your cluster configuration are:   root-ca.pem admin.pem admin-key.pem (Optional) node.pem (Optional) node-key.pem For information on adding and configuring these certificates, see Docker security configuration and Configure TLS certificates.  Run securityadmin.sh  After configuring your certificates and starting Elasticsearch, run securityadmin.sh to initialize the Security plugin:  ./securityadmin.sh -cd ../securityconfig/ -icl -nhnv -cacert ../../../config/root-ca.pem -cert ../../../config/admin.pem -key ../../../config/admin-key.pem For more information about what this command does, see Apply configuration changes and Change passwords for read-only users.  If you’re using Docker, see Bash access to containers.  Kibana  Depending on your settings in kibana.yml, you might need to add root-ca.pem to your Kibana node, as well. You have two options: disable SSL verification or add the root CA. Disable SSL verification:  elasticsearch.ssl.verificationMode: none   Add root CA:  elasticsearch.ssl.certificateAuthorities: [&quot;/usr/share/kibana/config/root-ca.pem&quot;] elasticsearch.ssl.verificationMode: full ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/security/generate-certificates/",
      "relUrl": "/docs/security/generate-certificates/"
    },
  
    {
      "id": "21",
      "title": "Index Data",
      "content": "Index data You index data using the Elasticsearch REST API. Two APIs exist: the index API and the _bulk API.  For situations in which new data arrives incrementally (for example, customer orders from a small business), you might use the index API to add documents individually as they arrive. For situations in which the flow of data is less frequent (for example, weekly updates to a marketing website), you might prefer to generate a file and send it to the _bulk API. For large numbers of documents, lumping requests together and using the _bulk API offers superior performance. If your documents are enormous, however, you might need to index them individually.  Introduction to indexing  A request to the index API looks like the following:  PUT elasticsearch_domain/&amp;lt;index&amp;gt;/_doc/&amp;lt;id&amp;gt; { &quot;A JSON&quot;: &quot;document&quot; } A request to the _bulk API looks a little different, because you specify the index and ID in the bulk data:  POST elasticsearch_domain/_bulk { &quot;index&quot;: { &quot;_index&quot; : &quot;&amp;lt;index&amp;gt;&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;&amp;lt;id&amp;gt;&quot; } } { &quot;A JSON&quot;: &quot;document&quot; }  Bulk data must conform to a specific format, which requires a newline character ( n) at the end of every line, including the last line. This is the basic format:  action_and_metadata&amp;gt; n optional_document n action_and_metadata n optional_document n ... The document is optional, because delete actions do not require a document. The other actions (index, create, and update) all require a document.  Elasticsearch features automatic index creation when you add a document to an index that doesn’t already exist. It also features automatic ID generation if you don’t specify an ID in the request. This simple example automatically creates the movies index, indexes the document, and assigns it a unique ID:  POST elasticsearch_domain/movies/_doc {&quot;title&quot;: &quot;Spirited Away&quot;} Automatic ID generation has a clear downside: because the indexing request didn’t specify a document ID, you can’t easily update the document at a later time. To specify an ID of 1, use the following request, and note the use of PUT instead of POST:  PUT elasticsearch_domain/movies/_doc/1 {&quot;title&quot;: &quot;Spirited Away&quot;} Indices default to five primary shards and one replica. If you want to specify non-default settings, create the index before adding documents:  PUT elasticsearch_domain/more-movies {&quot;settings&quot;: {&quot;number_of_shards&quot;: 6, &quot;number_of_replicas&quot;: 2}} Naming restrictions for indices  Elasticsearch indices have the following naming restrictions:   All letters must be lowercase. Index names can’t begin with _ (underscore) or - (hyphen). Index names can’t contain spaces, commas, or the following characters:  &quot;, *, +, /,  , |, ?, #, &amp;gt;, or &amp;lt;   ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/elasticsearch/index-data/",
      "relUrl": "/docs/elasticsearch/index-data/"
    },
  
    {
      "id": "22",
      "title": "Troubleshoot",
      "content": "Troubleshoot This page contains a list of issues and workarounds.  Multi-tenancy issues in Kibana  If you’re testing multiple users in Kibana and encounter unexpected changes in tenant, use Google Chrome in an Incognito window or Firefox in a Private window.  Beats  If you encounter compatibility issues when attempting to connect Beats to Open Distro for Elasticsearch, make sure you’re using the Apache 2.0 distribution of Beats, not the default distribution, which uses a proprietary license.  As of version 6.7, the default distribution of Beats includes a license check and fails to connect to the Apache 2.0 distribution of Elasticsearch.  Dependency error during upgrade  If you run sudo yum upgrade and receive a dependency error, Elasticsearch likely has a new minor version that the Open Distro for Elasticsearch plugins don’t support yet. You can install a specific, supported version of Elasticsearch to resolve the issue.  A temporary solution is to add the --skip-broken option to upgrade the rest of your system:  sudo yum upgrade --skip-broken Elasticsearch fails to start on Java 8 (RPM install)  If Elasticsearch fails to start and you’re using Java 8, verify that you set the symbolic link (symlink) correctly in step 5 of the RPM installation. If Java is installed to a non-standard path, try looking for tools.jar using the following command:  ls /usr/lib/jvm/java-1.8.0-openjdk-*/lib/tools.jar Then you can delete the old symlink and create a new one to the corrected path:  sudo rm /usr/share/elasticsearch/lib/tools.jar sudo ln -s /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.191.b12-0.amzn2.x86_64/lib/tools.jar /usr/share/elasticsearch/lib/  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/troubleshoot/",
      "relUrl": "/docs/troubleshoot/"
    },
  
    {
      "id": "23",
      "title": "Install and Configure",
      "content": "Install and configure Open Distro for Elasticsearch Open Distro for Elasticsearch has several download options: Docker image, RPM package, and Debian package. ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/install/",
      "relUrl": "/docs/install/"
    },
  
    {
      "id": "24",
      "title": "Elasticsearch",
      "content": "Introduction to Elasticsearch Elasticsearch is a distributed search and analytics engine based on Apache Lucene. Its distributed design means that you interact with Elasticsearch clusters. Each cluster is a collection of one or more nodes, servers that store your data and process search requests.  Indices and documents  Elasticsearch organizes data into indices. Each index is a collection of JSON documents. A simple JSON document might look like this:  { &quot;title&quot;: &quot;The Wind Rises&quot;, &quot;release_date&quot;: &quot;2013-07-20&quot; } When you add the document to an index, Elasticsearch adds some metadata, such as the unique document ID:  { &quot;_index&quot;: &quot;&amp;lt;index-name&amp;gt;&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;&amp;lt;document-id&amp;gt;&quot;, &quot;_version&quot;: 1, &quot;_source&quot;: {   &quot;title&quot;: &quot;The Wind Rises&quot;,   &quot;release_date&quot;: &quot;2013-07-20&quot; } } Indices also contain mappings and settings:   A mapping is the collection of fields that documents in the index have. In this case, those fields are title and release_date. Settings include data like the index name, creation date, and number of shards. Older versions of Elasticsearch used arbitrary document types, but indices created in current versions of Elasticsearch should use a single type named _doc. If you have multiple document types, store them in different indices.  Primary and replica shards  Elasticsearch splits indices into shards so that they can be evenly distributed across nodes in a cluster. For example, a 400 GB index might be too large for any single node in your cluster to handle, but split into ten shards, each one 40 GB, Elasticsearch can distribute the shards across ten nodes and work with each shard individually.  By default, Elasticsearch creates a replica shard for each primary shard. If you split your index into ten shards, for example, Elasticsearch also creates ten replica shards. These replica shards act as backups in the event of a node failure—Elasticsearch distributes replica shards to different nodes than their corresponding primary shards—but they also improve the speed and rate at which the cluster can process search requests. You might specify more than one replica per index for a search-heavy workload.  Despite being a piece of an Elasticsearch index, each shard is actually a full Lucene index—confusing, we know. This detail is important, though, because each instance of Lucene is a running process that consumes CPU and memory. Splitting a 400 GB index into 1,000 shards, for example, would place needless strain on your cluster. A good rule of thumb is to keep shard size between 10–50 GB.  REST API  You interact with Elasticsearch clusters using the REST API, which offers a lot of flexibility. You can use clients like curl or any programming language that can send HTTP requests. To add a JSON document to an Elasticsearch index (i.e. index a document), you send an HTTP request:  PUT https://&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;/&amp;lt;index-name&amp;gt;/_doc/&amp;lt;document-id&amp;gt; { &quot;title&quot;: &quot;The Wind Rises&quot;, &quot;release_date&quot;: &quot;2013-07-20&quot; } To run a search for the document:  GET https://&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;/&amp;lt;index-name&amp;gt;/_search?q=wind To delete the document:  DELETE https://&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;/&amp;lt;index-name&amp;gt;/_doc/&amp;lt;document-id&amp;gt; You can change most Elasticsearch settings using the REST API, modify indices, check the health of the cluster, get statistics—almost everything. ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/elasticsearch/",
      "relUrl": "/docs/elasticsearch/"
    },
  
    {
      "id": "25",
      "title": "About",
      "content": "Open Distro for Elasticsearch Documentation This site contains the technical documentation for Open Distro for Elasticsearch, the community-driven, 100% open source distribution of Elasticsearch with advanced security, alerting, deep performance analysis, and more.  Get started Why use Open Distro for Elasticsearch? Get started Version history About Open Distro for Elasticsearch   Why use Open Distro for Elasticsearch?  Open Distro for Elasticsearch is well-suited to the following use cases:   Log analytics Real-time application monitoring Clickstream analytics Search backend Compared to the open source distribution of Elasticsearch, Open Distro for Elasticsearch offers several extra features:    Component   Purpose   Elasticsearch   Data store and search engine    Kibana   Search frontend and visualizations    Security   Authentication and access control for your cluster    Alerting   Receive alerts when your data meets certain conditions    SQL   Use SQL to query your data    Performance Analyzer   Monitor and optimize your cluster  Get started Docker   Install and start Docker Desktop. docker pull amazon/opendistro-for-elasticsearch:0.9.0 docker pull amazon/opendistro-for-elasticsearch-kibana:0.9.0 docker run -p 9200:9200 -p 9600:9600 -e &quot;discovery.type=single-node&quot; amazon/opendistro-for-elasticsearch:0.9.0 In a new terminal session, run:  curl -XGET --insecure https://localhost:9200 -u admin:admin  To learn more, see Install.  Version history    Open Distro for Elasticsearch version   Release highlights   Elasticsearch version   0.9.0   Bumps Elasticsearch version.   6.7.1    0.8.0   Bumps Elasticsearch version.   6.6.2    0.7.1   Fixes Kibana multitenancy.   6.5.4    0.7.0   Initial release.   6.5.4  For detailed release notes, see these GitHub repositories:   Security Alerting SQL Performance Analyzer   About Open Distro for Elasticsearch  Open Distro for Elasticsearch is supported by Amazon Web Services. All components are available under the Apache License, Version 2.0 on GitHub.  The project welcomes GitHub issues, bug fixes, features, plugins, documentation—anything at all. To get involved, see Contribute on the Open Distro for Elasticsearch website. ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/",
      "relUrl": "/"
    },
  
    {
      "id": "26",
      "title": "Kibana",
      "content": "Kibana Kibana is the default visualization tool for data in Elasticsearch. It also serves as a user interface for the Open Distro for Elasticsearch Security plugin.  Run Kibana using Docker  You can start Kibana using docker run after creating a Docker network and starting Elasticsearch, but the process of connecting Kibana to Elasticsearch is significantly easier with a Docker Compose file. Run docker pull amazon/opendistro-for-elasticsearch-kibana:0.9.0.  Create a docker-compose.yml file appropriate for your environment. A sample file that includes Kibana is available on the Open Distro for Elasticsearch Docker installation page.  Just like elasticsearch.yml, you can pass a custom kibana.yml to the container in the Docker Compose file.  Run docker-compose up.  Wait for the containers to start. Then see Get started with Kibana.  When finished, run docker-compose down.  Run Kibana using the RPM or Debian package   If you haven’t already, add the yum repositories specified in steps 1–2 in RPM or the apt repositories in steps 2–3 of Debian package. sudo yum install opendistroforelasticsearch-kibana or sudo apt install opendistroforelasticsearch-kibana (Optional) Modify /etc/kibana/kibana.yml. sudo systemctl start kibana.service To stop Kibana:  sudo systemctl stop kibana.service  Configuration  To run Kibana when the system starts:  sudo /bin/systemctl daemon-reload sudo /bin/systemctl enable kibana.service You can also modify the values in /etc/kibana/kibana.yml.  Get started with Kibana   After starting Kibana, you can access it at port 5601. For example, http://localhost:5601 Log in with the default username admin and password admin. Choose Try our sample data and add the sample flight data. Choose Discover and search for a few flights. Choose Dashboard, [Flights] Global Flight Dashboard, and wait for the dashboard to load.  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/kibana/",
      "relUrl": "/docs/kibana/"
    },
  
    {
      "id": "27",
      "title": "Security",
      "content": "Security Open Distro for Elasticsearch includes the Security plugin for authentication and access control. This plugin provides features like:   Node-to-node encryption Basic authentication Role-based access control Permission sets Index-, document-, and field-level security Audit logging of read and write operations Support for Active Directory, LDAP, Kerberos, and SAML Cross-cluster search Kibana multi-tenancy The plugin includes demo certificates so that you can get up and running quickly, but you should replace the demo certificates, reconfigure elasticsearch.yml, and change passwords before using Open Distro for Elasticsearch in a production environment.  If you don’t want to use the plugin, see Disable security. ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/security/",
      "relUrl": "/docs/security/"
    },
  
    {
      "id": "28",
      "title": "SQL",
      "content": "SQL Open Distro for Elasticsearch SQL lets you write queries in SQL rather than the Elasticsearch query domain-specific language (DSL). If you’re already familiar with SQL and don’t want to learn the query DSL, this feature is a great option.  To use the feature, send requests to the _opendistro/_sql URI. You can use a request parameter or the request body (recommended).  GET https://&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;/_opendistro/_sql?sql=select * from my-index limit 50 POST https://&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;/_opendistro/_sql { &quot;query&quot;: &quot;SELECT * FROM my-index LIMIT 50&quot; } For a sample curl command, try:  curl -XPOST https://localhost:9200/_opendistro/_sql -u admin:admin -k -d &#39;{&quot;query&quot;: &quot;SELECT * FROM kibana_sample_data_flights LIMIT 10&quot;}&#39; -H &#39;Content-Type: application/json&#39; By default, queries return JSON, but you can also return data in CSV format:  POST _opendistro/_sql?format=csv { &quot;query&quot;: &quot;SELECT * FROM my-index LIMIT 50&quot; } When you return data in CSV format, each row corresponds to a document, and each column corresponds to a field. Conceptually, you might find it useful to think of each Elasticsearch index as a database table.  User interfaces  You can test queries using Dev Tools in Kibana (https://&amp;lt;host&amp;gt;:5601).  Troubleshoot queries  The most common error is the dreaded null pointer exception, which can occur during parsing errors or when using the wrong HTTP method (POST vs. GET and vice versa). The POST method and HTTP request body offer the most consistent results:  POST _opendistro/_sql { &quot;query&quot;: &quot;SELECT * FROM my-index WHERE [&#39;name.firstname&#39;]=&#39;saanvi&#39; LIMIT 5&quot; } If a query isn’t behaving the way you expect, use the _explain API to see the translated query, which you can then troubleshoot. For most operations, _explain returns Elasticsearch query DSL. For UNION, MINUS, and JOIN, it returns something more akin to a SQL execution plan.  Sample request  POST _opendistro/_sql/_explain { &quot;query&quot;: &quot;SELECT * FROM * LIMIT  50&quot; } Sample response  {&quot;from&quot;:0,&quot;size&quot;:50}  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/sql/",
      "relUrl": "/docs/sql/"
    },
  
    {
      "id": "29",
      "title": "Performance Analyzer",
      "content": "Performance Analyzer Performance Analyzer is an agent and REST API that allows you to query numerous performance metrics for your cluster, including aggregations of those metrics, independent of the Java Virtual Machine (JVM). PerfTop is the default command line interface (CLI) for displaying those metrics.  To download PerfTop, see Download on the Open Distro for Elasticsearch website.  You can also install it using npm:  npm install -g @aws/opendistro-for-elasticsearch-perftop   Get started with PerfTop  The basic syntax is:  ./perf-top-&amp;lt;operating_system&amp;gt; --dashboard &amp;lt;dashboard&amp;gt;.json --endpoint &amp;lt;endpoint&amp;gt; If you’re using npm, the syntax is similar:  perf-top --dashboard &amp;lt;dashboard&amp;gt; --endpoint &amp;lt;endpoint&amp;gt; If you’re running PerfTop from a node (i.e. locally), specify port 9600:  ./perf-top-linux --dashboard dashboards/&amp;lt;dashboard&amp;gt;.json --endpoint localhost:9600 Otherwise, just specify the Elasticsearch endpoint:  ./perf-top-macos --dashboard dashboards/&amp;lt;dashboard&amp;gt;.json --endpoint my-cluster.my-domain.com PerfTop has four pre-built dashboards in the dashboards directory, but you can also create your own.  You can also load the pre-built dashboards (ClusterOverview, ClusterNetworkMemoryAnalysis, ClusterThreadAnalysis, or NodeAnalysis) without the JSON files, such as --dashboard ClusterThreadAnalysis.  PerfTop has no interactivity. Start the application, monitor the dashboard, and press esc, q, or Ctrl + C to quit.  Other options   For NodeAnalysis and similar custom dashboards, you can add the --nodename &amp;lt;node_name&amp;gt; argument if you want your dashboard to display metrics for only a single node. For troubleshooting, add the --logfile &amp;lt;log-file&amp;gt;.txt argument.  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/pa/",
      "relUrl": "/docs/pa/"
    },
  
    {
      "id": "30",
      "title": "Alerting",
      "content": "Alerting  Kibana  The alerting feature notifies you when data from one or more Elasticsearch indices meets certain conditions. For example, you might want to notify a Slack channel if your application logs more than five HTTP 503 errors in one hour, or you might want to page a developer if no new documents have been indexed in the past 20 minutes.  To get started, choose Alerting in Kibana. ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/alerting/",
      "relUrl": "/docs/alerting/"
    },
  
    {
      "id": "31",
      "title": "JDBC Driver",
      "content": "JDBC driver The Java Database Connectivity (JDBC) driver lets you integrate Open Distro for Elasticsearch with your favorite business intelligence (BI) applications.  For information on downloading and using the JAR file, see the GitHub repository. ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/sql/jdbc/",
      "relUrl": "/docs/sql/jdbc/"
    },
  
    {
      "id": "32",
      "title": "Joins",
      "content": "Joins Open Distro for Elasticsearch SQL supports inner joins, left outer joins, and cross joins. Joins have a number of constraints:   You can only join two indices. You must use aliases for indices (e.g. people p). Within an ON clause, you can only use AND conditions. In a WHERE statement, don’t combine trees that contain multiple indices. For example, the following statement works:  WHERE (a.type1 &amp;gt; 3 OR a.type1 &amp;lt; 0) AND (b.type2 &amp;gt; 4 OR b.type2 &amp;lt; -1)  The following statement does not:  WHERE (a.type1 &amp;gt; 3 OR b.type2 &amp;lt; 0) AND (a.type1 &amp;gt; 4 OR b.type2 &amp;lt; -1)  You can’t use GROUP BY or ORDER BY for results. LIMIT with OFFSET (e.g. LIMIT 25 OFFSET 25) is not supported.  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/sql/joins/",
      "relUrl": "/docs/sql/joins/"
    },
  
    {
      "id": "33",
      "title": "Active Directory and LDAP",
      "content": "Active Directory and LDAP Active Directory and LDAP can be used for authentication and authorization and thus can be used both in the authc and authz sections of the configuration.  The authc section is used for configuring authentication, which means to check if the user has entered the correct credentials. The authz is used for authorization, which defines how the role(s) for an authenticated user are retrieved and mapped.  In most cases, you want to configure both authentication and authorization. You can also use authentication only and map the users retrieved from LDAP directly to Security plugin roles. Connection settings  Hostname and Port   Bind DN and password   TLS settings   Certificate validation   Client authentication   Enabled ciphers and protocols  Use Active Directory and LDAP for authentication  Configuration summary   Complete authentication example  Use Active Directory and LDAP for authorization  Approach 1: Query the role subtree   Approach 2: Use a user’s attribute as role name   (Advanced) Control LDAP user attributes   (Advanced) Exclude certain users from role lookup   (Advanced) Exclude roles from nested role lookups   Configuration summary   Complete authorization example  Connection settings  To enable LDAP authentication and authorization, add the following lines to plugins/opendistro_security/securityconfig/config.yml:  authc: ldap:   enabled: true   order: 1   http_authenticator:   type: basic   challenge: false   authentication_backend:   type: ldap   config:   ... authz: ldap:   enabled: true authorization_backend:   type: ldap   config:   ... The connection settings are identical for authentication and authorization and are added to the config sections.  Hostname and Port  To configure the hostname and port of your Active Directory server(s):  config: hosts:   - primary.ldap.example.com:389   - secondary.ldap.example.com:389 You can configure more than one server here. If the Security plugin cannot connect to the first server, it the remaining servers sequentially.  Bind DN and password  To configure the bind_dn and password that the Security plugin uses when issuing queries to your server:  config: bind_dn: cn=admin,dc=example,dc=com password: password If your server supports anonymous authentication, both bind_dn and password can be set to null.  TLS settings  Use the following parameters to configure TLS for connecting to your server:  config: enable_ssl: &amp;lt;true|false&amp;gt; enable_start_tls: &amp;lt;true|false&amp;gt; enable_ssl_client_auth: &amp;lt;true|false&amp;gt; verify_hostnames: &amp;lt;true|false&amp;gt;   Name   Description   enable_ssl   Whether to use LDAP over SSL (LDAPS).    enable_start_tls   Whether to use STARTTLS. Can’t be used in combination with LDAPS.    enable_ssl_client_auth   Whether to send the client certificate to the LDAP server.    verify_hostnames   Whether to verify the hostnames of the server’s TLS certificate.  Certificate validation  By default, the Security plugin validates the TLS certificate of the LDAP server(s) against the root CA configured in elasticsearch.yml, either as PEM certificate or a truststore:  opendistro_security.ssl.transport.pemtrustedcas_filepath: ... opendistro_security.ssl.http.truststore_filepath: ... If your server uses a certificate signed by a different CA, import this CA into your truststore or add it to your trusted CA file on each node.  You can also use a separate root CA in PEM format by setting one of the following configuration options:  config: pemtrustedcas_filepath: /full/path/to/trusted_cas.pem config: pemtrustedcas_content: |-   MIID/jCCAuagAwIBAgIBATANBgkqhkiG9w0BAQUFADCBjzETMBEGCgmSJomT8ixk   ARkWA2NvbTEXMBUGCgmSJomT8ixkARkWB2V4YW1wbGUxGTAXBgNVBAoMEEV4YW1w   bGUgQ29tIEluYy4xITAfBgNVBAsMGEV4YW1wbGUgQ29tIEluYy4gUm9vdCBDQTEh   ...   Name   Description   pemtrustedcas_filepath   Absolute path to the PEM file containing the root CA(s) of your Active Directory/LDAP server    pemtrustedcas_content   The root CA content of your Active Directory/LDAP server. Cannot be used when pemtrustedcas _filepath is set.  Client authentication  If you use TLS client authentication, the Security plugin sends the PEM certificate of the node, as configured in elasticsearch.yml. Set one of the following configuration options:  config: pemkey_filepath: /full/path/to/private.key.pem pemkey_password: private_key_password pemcert_filepath: /full/path/to/certificate.pem or  config: pemkey_content: |-   MIID2jCCAsKgAwIBAgIBBTANBgkqhkiG9w0BAQUFADCBlTETMBEGCgmSJomT8ixk   ARkWA2NvbTEXMBUGCgmSJomT8ixkARkWB2V4YW1wbGUxGTAXBgNVBAoMEEV4YW1w   bGUgQ29tIEluYy4xJDAiBgNVBAsMG0V4YW1wbGUgQ29tIEluYy4gU2lnbmluZyBD   ... pemkey_password: private_key_password pemcert_content: |-   MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCHRZwzwGlP2FvL   oEzNeDu2XnOF+ram7rWPT6fxI+JJr3SDz1mSzixTeHq82P5A7RLdMULfQFMfQPfr   WXgB4qfisuDSt+CPocZRfUqqhGlMG2l8LgJMr58tn0AHvauvNTeiGlyXy0ShxHbD   ...   Name   Description   pemkey_filepath   Absolute path to the file containing the private key of your certificate.    pemkey_content   The content of the private key of your certificate. Cannot be used when pemkey _filepath is set.    pemkey_password   The password of your private key, if any.    pemcert_filepath   Absolute path to the the client certificate.    pemcert_content   The content of the client certificate. Cannot be used when pemcert_filepath is set.  Enabled ciphers and protocols  You can limit the allowed ciphers and TLS protocols for the LDAP connection. For example, you can only allow strong ciphers and limit the TLS versions to the most recent ones:  ldap: enabled: true ... authentication_backend:   type: ldap   config:   enabled_ssl_ciphers:   - &quot;TLS_DHE_RSA_WITH_AES_256_CBC_SHA&quot;   - &quot;TLS_DHE_DSS_WITH_AES_128_CBC_SHA256&quot;   enabled_ssl_protocols:   - &quot;TLSv1.1&quot;   - &quot;TLSv1.2&quot;   Name   Description   enabled_ssl_ciphers   Array, enabled TLS ciphers. Only Java format is supported.    enabled_ssl_protocols   Array, enabled TLS protocols. Only Java format is supported.  Use Active Directory and LDAP for authentication  To use Active Directory/LDAP for authentication, first configure a respective authentication domain in the authc section of plugins/opendistro_security/securityconfig/config.yml:  authc: ldap:   enabled: true   order: 1   http_authenticator:   type: basic   challenge: true   authentication_backend:   type: ldap   config:   ... Afterwards add the connection settings for your Active Directory/LDAP server to the config section of the authentication domain:  config: enable_ssl: true enable_start_tls: false enable_ssl_client_auth: false verify_hostnames: true hosts:   - ldap.example.com:8389 bind_dn: cn=admin,dc=example,dc=com password: passw0rd Authentication works by issuing an LDAP query containing the username against the user subtree of the LDAP tree.  The Security plugin first takes the configured LDAP query and replaces the placeholder {0} with the username from the user’s credentials.  usersearch: &#39;(sAMAccountName={0})&#39; Then it issues this query against the user subtree. Currently, the whole subtree beneath the configured userbase is searched:  userbase: &#39;ou=people,dc=example,dc=com&#39; If the query was successful, the Security plugin retrieves the username from the LDAP entry. You can specify which attribute from the LDAP entry the Security plugin should use as the username:  username_attribute: uid If this key is not set or null, then the Distinguished Name (DN) of the LDAP entry is used.  Configuration summary    Name   Description   userbase   Specifies the subtree in the directory where user information is stored.    usersearch   The actual LDAP query that the Security plugin executes when trying to authenticate a user. The variable {0} is substituted with the username.    username_attribute   The Security plugin uses this attribute of the directory entry to look for the user name. If set to null, the DN is used (default).  Complete authentication example  ldap: enabled: false order: 1 http_authenticator:   type: basic   challenge: true authentication_backend:   type: ldap   config:   enable_ssl: true   enable_start_tls: false   enable_ssl_client_auth: false   verify_hostnames: true   hosts:   - ldap.example.com:636   bind_dn: cn=admin,dc=example,dc=com   password: password   userbase: &#39;ou=people,dc=example,dc=com&#39;   usersearch: &#39;(sAMAccountName={0})&#39;   username_attribute: uid   Use Active Directory and LDAP for authorization  To use Active Directory/LDAP for authorization, first configure a respective authorization domain in the authz section of config.yml:  authz: ldap:   enabled: true authorization_backend:   type: ldap   config:   ... Authorization is the process of retrieving backend roles for an authenticated user from an LDAP server. This is typically the same server(s) you use for authentication, but you can also use a different server. The only requirement is that the user to fetch the roles for actually exists on the LDAP server.  Since the Security plugin always checks if a user exists in the LDAP server, you need to configure userbase, usersearch and username_attribute also in the authz section.  Authorization works similarly to authentication. The Security plugin issues an LDAP query containing the username against the role subtree of the LDAP tree.  As an alternative, the Security plugin can also fetch roles that are defined as a direct attribute of the user entry in the user subtree.  Approach 1: Query the role subtree  The Security plugin first takes the LDAP query for fetching roles (“rolesearch”) and substitutes any variables found in the query. For example, for a standard Active Directory installation, you would use the following role search:  rolesearch: &#39;(member={0})&#39; You can use the following variables:   {0} is substituted with the DN of the user. {1} is substituted with the username, as defined by the username_attribute setting. {2} is substituted with an arbitrary attribute value from the authenticated user’s directory entry. The variable {2} refers to an attribute from the user’s directory entry. The attribute you want to use is specified by the userroleattribute setting.  userroleattribute: myattribute The Security plugin then issues the substituted query against the configured role subtree. The whole subtree underneath rolebase is searched.  rolebase: &#39;ou=groups,dc=example,dc=com&#39; If you use nested roles (roles that are members of other roles), you can configure the Security plugin to resolve them:  resolve_nested_roles: false After all roles have been fetched, the Security plugin extracts the final role names from a configurable attribute of the role entries:  rolename: cn If this is not set, the DN of the role entry is used. You can now use this role name for mapping it to one or more the Security plugin roles, as defined in roles_mapping.yml.  Approach 2: Use a user’s attribute as role name  If you store the roles as a direct attribute of the user entries in the user subtree, you only need to configure the attribute name:  userrolename: roles You can configure multiple attribute names:  userrolename: roles, otherroles This approach can be combined with querying the role subtree. The Security plugin fetches the roles from the user’s role attribute and then executes the role search.  If you don’t use or have a role subtree, you can disable the role search completely:  rolesearch_enabled: false (Advanced) Control LDAP user attributes  By default, the Security plugin reads all LDAP user attributes and make them available for index name variable substitution and DLS query variable substitution. If your LDAP entries have a lot of attributes, you might want to control which attributes should be made available. The fewer the attributes, the better the performance.    Name   Description   custom_attr_whitelist   String array. Specifies the LDAP attributes that should be made available for variable substitution.    custom_attr_maxval_len   Integer. Specifies the maximum allowed length of each attribute. All attributes longer than this value are discarded. A value of 0 disables custom attributes altogether. Default is 36.  Example:  authz: ldap:   enabled: true authorization_backend:   type: ldap   config:   custom_attr_whitelist:   - attribute1   - attribute2   custom_attr_maxval_len   ... (Advanced) Exclude certain users from role lookup  If you are using multiple authentication methods, it can make sense to exclude certain users from the LDAP role lookup.  Consider the following scenario for a typical Kibana setup:  All Kibana users are stored in an LDAP/Active Directory server.  However, you also have a Kibana server user. Kibana uses this user to manage stored objects and perform monitoring and maintenance tasks. You do not want to add this user to your Active Directory installation, but rather store it in the the Security plugin internal user database.  In this case, it makes sense to exclude the Kibana server user from the LDAP authorization, since we already know that there is no corresponding entry. You can use the skip_users configuration setting to define which users should be skipped. Wildcards and regular expressions are supported.  skip_users: - kibanaserver - &#39;cn=Michael Jackson,ou*people,o=TEST&#39; - &#39;/ S*/&#39; (Advanced) Exclude roles from nested role lookups  If the users in your LDAP installation have a large number of roles, and you have the requirement to resolve nested roles as well, you might run into performance issues.  In most cases, however, not all user roles are related to Elasticsearch and Kibana. You might only need a couple roles. In this case, you can use the nested role filter feature to define a list of roles that are filtered out from the list of the user’s roles. Wildcards and regular expressions are supported.  This only has an effect if resolve_nested_roles is true.  nested_role_filter: &amp;lt;true|false&amp;gt; - &#39;cn=Michael Jackson,ou*people,o=TEST&#39; - ... Configuration summary    Name   Description   rolebase   Specifies the subtree in the directory where role/group information is stored.    rolesearch   The actual LDAP query that the Security plugin executes when trying to determine the roles of a user. You can use three variables here (see below).    userroleattribute   The attribute in a user entry to use for {2} variable substitution.    userrolename   If the roles/groups of a user are not stored in the groups subtree, but as an attribute of the user’s directory entry, define this attribute name here.    rolename   The attribute of the role entry which should be used as role name.    resolve_nested_roles   Boolean. Whether or not to resolve nested roles. Default is false.    skip_users   Array of users that should be skipped when retrieving roles. Wildcards and regular expressions are supported.    nested_role_filter   Array of role DNs that should be filtered before resolving nested roles. Wildcards and regular expressions are supported.    rolesearch_enabled   Boolean. Enable or disable the role search. Default is true.    custom_attr_whitelist   String array. Specifies the LDAP attributes that should be made available for variable substitution.    custom_attr_maxval_len   Integer. Specifies the maximum allowed length of each attribute. All attributes longer than this value will be discarded. A value of 0 disables custom attributes altogether. Default is 36.  Complete authorization example  authz: ldap:   enabled: true   authorization_backend:   type: ldap   config:   enable_ssl: true   enable_start_tls: false   enable_ssl_client_auth: false   verify_hostnames: true   hosts:  - ldap.example.com:636   bind_dn: cn=admin,dc=example,dc=com   password: password   userbase: &#39;ou=people,dc=example,dc=com&#39;   usersearch: &#39;(uid={0})&#39;   username_attribute: uid   rolebase: &#39;ou=groups,dc=example,dc=com&#39;   rolesearch: &#39;(member={0})&#39;   userroleattribute: null   userrolename: none   rolename: cn   resolve_nested_roles: true   skip_users:  - kibanaserver  - &#39;cn=Michael Jackson,ou*people,o=TEST&#39;  - &#39;/ S*/&#39;  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/security/ldap/",
      "relUrl": "/docs/security/ldap/"
    },
  
    {
      "id": "34",
      "title": "WMS Map Server",
      "content": "Configure WMS map server Due to licensing restrictions, the default installation of Kibana does in Open Distro for Elasticsearch doesn’t include a map server for tile map visualizations. To configure Kibana to use a WMS map server:   Open Kibana at https://&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;. For example, https://localhost:5601. If necessary, log in. Management. Advanced Settings. Locate visualization:tileMap:WMSdefaults. Change enabled to true, and add the URL of a valid WMS map server.  { &quot;enabled&quot;: true, &quot;url&quot;: &quot;&amp;lt;wms-map-server-url&amp;gt;&quot;, &quot;options&quot;: {   &quot;format&quot;: &quot;image/png&quot;,   &quot;transparent&quot;: true } }  Map services often have licensing fees or restrictions. You are responsible for all such considerations on any map server that you specify. ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/kibana/maptiles/",
      "relUrl": "/docs/kibana/maptiles/"
    },
  
    {
      "id": "35",
      "title": "Monitors",
      "content": "Monitors   Key terms Create destinations Create monitors Create triggers  Visual graph   Extraction query  Add actions Work with alerts   Key terms    Term   Definition   Monitor   A job that runs on a defined schedule and queries Elasticsearch. The results of these queries are then used as input for one or more triggers.    Trigger   Conditions that, if met, generate alerts and can perform some action.    Alert   A notification that a monitor’s trigger condition has been met.    Action   The information that you want the monitor to send out after being triggered. Actions have a destination, a message subject, and a message body.    Destination   A reusable location for an action, such as Amazon Chime, Slack, or a webhook URL.  Create destinations   Choose Alerting, Destinations, Add destination. Specify a name for the destination so that you can identify it later. For Type, choose Slack, Amazon Chime, or custom webhook. Specify the webhook URL. These requests use the HTTP POST method.  For more information about webhooks, see the documentation for Slack and Chime.  For custom webhooks, you must specify more information: parameters, authentication, and headers. This information is stored in plain text in the Elasticsearch cluster. We will improve this design in the future, but for now, credentials might be visible to other Elasticsearch users. Create monitors   Choose Alerting, Monitors, Create monitor. Specify a name and schedule for the monitor. Choose one or more indices. You can also use * as a wildcard to specify an index pattern. Define the monitor in one of two ways: visually or using a query.  Visual definition works well for monitors that you can define as “some value is above or below some threshold for some amount of time.”   Query definition gives you flexibility in terms of what you query for (using the Elasticsearch query DSL) and how you evaluate the results of that query (Painless scripting).   To define a monitor visually, choose Define using visual graph. Then choose an aggregation (for example, count() or average()), a set of documents, and a timeframe. Visual definition works well for most monitors.  To use a query, choose Define using extraction query, add your query (using the Elasticsearch query DSL), and test it using the Run button.  The monitor makes this query to Elasticsearch as often as the schedule dictates; check the Query Performance section and make sure you’re comfortable with the performance implications.  Choose Create.   Create triggers  The next step in creating a monitor is to create a trigger. These steps differ depending on whether you chose Define using visual graph or Define using extraction query when you created the monitor.  Either way, you begin by specifying a name and severity level for the trigger. Severity levels help you manage alerts. A trigger with a high severity level (e.g. 1) might page a specific individual, whereas a trigger with a low severity level might message a chat room.  Visual graph  For Trigger condition, specify a threshold for the aggregation and timeframe you chose earlier, such as “is below 1,000” or “is exactly 10.”  The line moves up and down as you increase and decrease the threshold. Once this line is crossed, the trigger evaluates to true.  Extraction query  For Trigger condition, specify a Painless script that returns true or false. Painless is the default Elasticsearch scripting language and has a syntax similar to Groovy.  Trigger condition scripts revolve around the ctx.results[0] variable, which corresponds to the extraction query response. For example, your script might reference ctx.results[0].hits.total or ctx.results[0].hits.hits[i]._source.error_code.  A return value of true means the trigger condition has been met, and the trigger should execute its actions. Test your script using the Run button.  The Info link next to Trigger condition contains a useful summary of the variables and results available to your query.  Sample scripts  // Evaluates to true if the query returned any documents ctx.results[0].hits.total &amp;gt; 0 // Performs some crude custom scoring and returns true if that score exceeds a certain value int score = 0; for (int i = 0; i &amp;lt; ctx.results[0].hits.hits.length; i++) { // Weighs 500 errors 10 times as heavily as 503 errors if (ctx.results[0].hits.hits[i]._source.http_status_code == &quot;500&quot;) {   score += 10; } else if (ctx.results[0].hits.hits[i]._source.http_status_code == &quot;503&quot;) {   score += 1; } } if (score &amp;gt; 99) { return true; } else { return false; } Available variables    Variable   Description   ctx.results   An array with one element (i.e. ctx.results[0]). Contains the query results. This variable is empty if the trigger was unable to retrieve results. See ctx.error.    ctx.monitor   Includes ctx.monitor.name, ctx.monitor.type, ctx.monitor.enabled, ctx.monitor.enabled_time, ctx.monitor.schedule, ctx.monitor.inputs, triggers and ctx.monitor.last_update_time.    ctx.trigger   Includes ctx.trigger.name, ctx.trigger.severity, ctx.trigger.condition, and ctx.trigger.actions.    ctx.periodStart   Unix timestamp for the beginning of the period during which the alert triggered. For example, if a monitor runs every ten minutes, a period might begin at 10:40 and end at 10:50.    ctx.periodEnd   The end of the period during which the alert triggered.    ctx.error   The error message if the trigger was unable to retrieve results or unable to evaluate the trigger, typically due to a compile error or null pointer exception. Null otherwise.    ctx.alert   The current, active alert (if it exists). Includes ctx.alert.id, ctx.alert.version, and ctx.alert.isAcknowledged. Null if no alert is active.  Add actions  The final step in creating a monitor is to add one or more actions. Actions send notifications when trigger conditions are met and support Slack, Amazon Chime, and webhooks.  If you don’t want to receive notifications for alerts, you don’t have to add actions to your triggers. Instead, you can periodically check Kibana.   Specify a name for the action. Choose a destination. Add a subject and body for the message.  You can add variables to your messages using Mustache templates. You have access to ctx.action.name, the name of the current action, as well as all trigger variables.  Choose Create. After an action sends a message, the content of that message has left the purview of the Security plugin. Securing access to the message (e.g. access to the Slack channel) is your responsibility.  Sample message  Monitor {{ctx.monitor.name}} just entered an alert state. Please investigate the issue. - Trigger: {{ctx.trigger.name}} - Severity: {{ctx.trigger.severity}} - Period start: {{ctx.periodStart}} - Period end: {{ctx.periodEnd}} If you want to use the ctx.results variable in a message, use {{ctx.results.0}} rather than {{ctx.results[0]}}. This difference is due to how Mustache handles bracket notation.  Work with alerts  Alerts persist until you resolve the root cause and have the following states:    State   Description   Active   The alert is ongoing and unacknowledged. Alerts remain in this state until you acknowledge them, delete the trigger associated with the alert, or delete the monitor entirely.    Acknowledged   Someone has acknowledged the alert, but not fixed the root cause.    Completed   The alert is no longer ongoing. Alerts enter this state after the corresponding trigger evaluates to false.    Error   An error occurred while executing the trigger—usually the result of a a bad trigger or destination.    Deleted   Someone deleted the monitor or trigger associated with this alert while the alert was ongoing. ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/alerting/monitors/",
      "relUrl": "/docs/alerting/monitors/"
    },
  
    {
      "id": "36",
      "title": "OpenID Connect",
      "content": "OpenID Connect The Security plugin can integrate with identify providers that use the OpenID Connect standard. This feature enables: Automatic configuration  Point the Security plugin to the metadata of your identity provider (IdP), and the Security plugin uses that data for configuration.  Automatic key fetching  The Security plugin automatically retrieves the public key for validating the JSON web tokens (JWTs) from the JSON web key set (JWKS) endpoint of your IdP. You don’t have to configure keys or shared secrets in config.yml.  Key rollover  You can change the keys used for signing the JWTs directly in your IdP. If the Security plugin detects an unknown key, it tries to retrieve it from the IdP, transparent to the user.  Kibana single sign-on Configure OpenID Connect integration OpenID Connect URL Fetching public keys Key rollover and multiple public keys TLS settings  Enabling TLS   Certificate validation   TLS client authentication   Enabled ciphers and protocols  (Advanced) DoS protection Kibana single sign-on  Configuration   Configuration parameters   Configuration example   Elasticsearch configuration  Configure OpenID Connect integration  To integrate with an OpenID IdP, set up an authentication domain and choose openid as HTTP authentication type. JSON web tokens already contain all required information to verify the request, so set challenge to false and authentication_backend to noop.  Minimal configuration:  openid_auth_domain: http_enabled: true transport_enabled: true order: 0 http_authenticator:   type: openid   challenge: false   config:   subject_key: preferred_username   roles_key: roles   openid_connect_url: https://keycloak.example.com:8080/auth/realms/master/.well-known/openid-configuration authentication_backend:   type: noop Configuration parameters:    Name   Description   openid_connect_url   The URL of your IdP where the Security plugin can find the OpenID Connect metadata/configuration settings. This URL differs between IdPs. Required.    jwt_header   The HTTP header that stores the token. Typically the Authorization header with the Bearer schema: Authorization: Bearer &amp;lt;token&amp;gt;. Optional. Default is Authorization.    jwt_url_parameter   If the token is not transmitted in the HTTP header, but as an URL parameter, define the name of the parameter here. Optional.    subject_key   The key in the JSON payload that stores the user’s name. If not defined, the subject registered claim is used. Most IdP providers use the preferred_username claim. Optional.    roles_key   The key in the JSON payload that stores the user’s roles. The value of this key must be a comma-separated list of roles. Required only if you want to use roles in the JWT.  OpenID Connect URL  OpenID Connect specifies various endpoints for integration purposes. The most important endpoint is well-known, which lists endpoints and other configuration options for the Security plugin.  The URL differs between IdPs, but usually ends in /.well-known/openid-configuration.  Keycloak example:  http(s)://&amp;lt;server&amp;gt;:&amp;lt;port&amp;gt;/auth/realms/&amp;lt;realm&amp;gt;/.well-known/openid-configuration The main information that the Security plugin needs is jwks_uri. This URI specifies where the IdP’s public key(s) in JWKS format can be found. For example:  jwks_uri: &quot;https://keycloak.example.com:8080/auth/realms/master/protocol/openid-connect/certs&quot; {  keys:[   { kid:&quot;V-diposfUJIk5jDBFi_QRouiVinG5PowskcSWy5EuCo&quot;, kty:&quot;RSA&quot;, alg:&quot;RS256&quot;, use:&quot;sig&quot;, n:&quot;rI8aUrAcI_auAdF10KUopDOmEFa4qlUUaNoTER90XXWADtKne6VsYoD3ZnHGFXvPkRAQLM5d65ScBzWungcbLwZGWtWf5T2NzQj0wDyquMRwwIAsFDFtAZWkXRfXeXrFY0irYUS9rIJDafyMRvBbSz1FwWG7RTQkILkwiC4B8W1KdS5d9EZ8JPhrXvPMvW509g0GhLlkBSbPBeRSUlAS2Kk6nY5i3m6fi1H9CP3Y_X-TzOjOTsxQA_1pdP5uubXPUh5YfJihXcgewO9XXiqGDuQn6wZ3hrF6HTlhNWGcSyQPKh1gEcmXWQlRENZMvYET-BuJEE7eKyM5vRhjNoYR3w&quot;, e:&quot;AQAB&quot;   }  ] } To find more information about IdP endpoints:   Okta Keycloak Auth0 Connect2ID Salesforce IBM OpenID Connect Fetching public keys  When an IdP generates and signs a JSON web token, it must add the ID of the key to the JWT header. For example:  { &quot;alg&quot;: &quot;RS256&quot;, &quot;typ&quot;: &quot;JWT&quot;, &quot;kid&quot;: &quot;V-diposfUJIk5jDBFi_QRouiVinG5PowskcSWy5EuCo&quot; } As per the OpenID Connect specification, the kid (key ID) is mandatory. Token verification does not work if an IdP fails to add the kid field to the JWT.  If the Security plugin receives a JWT with an unknown kid, it visits the IdP’s jwks_uri and retrieves all available, valid keys. These keys are used and cached until a refresh is triggered by retrieving another unknown key ID.  Key rollover and multiple public keys  The Security plugin can maintain multiple valid public keys at once. The OpenID specification does not allow for a validity period of public keys, so a key is valid until it has been removed from the list of valid keys in your IdP and the list of valid keys has been refreshed.  If you want to roll over a key in your IdP, best practice is to: Create a new key pair in your IdP, and give the new key a higher priority than the currently used key.  Your IdP uses this new key over the old key.  Upon first appearance of the new kid in a JWT, the Security plugin refreshes the key list.  At this point, both the old key and the new key are valid. Tokens signed with the old key are also still valid.  The old key can be removed from your IdP when the last JWT signed with this key has timed out.  If you have to immediately change your public key, you can also delete the old key first and then create a new one. In this case, all JWTs signed with the old key become invalid immediately.  TLS settings  In order to prevent man-in-the-middle attacks, you should secure the connection between the Security plugin and your IdP with TLS.  Enabling TLS  Use the following parameters to enable TLS for connecting to your IdP:  config: enable_ssl: &amp;lt;true|false&amp;gt; verify_hostnames: &amp;lt;true|false&amp;gt;   Name   Description   enable_ssl   Whether to use TSL. Default is false.    verify_hostnames   Whether to verify the hostnames of the IdP’s TLS certificate. Default is true.  Certificate validation  To validate the TLS certificate of your IdP, configure either the path to the IdP’s root CA or the root certificates content:  config: pemtrustedcas_filepath: /path/to/trusted_cas.pem config: pemtrustedcas_content: |-   MIID/jCCAuagAwIBAgIBATANBgkqhkiG9w0BAQUFADCBjzETMBEGCgmSJomT8ixk   ARkWA2NvbTEXMBUGCgmSJomT8ixkARkWB2V4YW1wbGUxGTAXBgNVBAoMEEV4YW1w   bGUgQ29tIEluYy4xITAfBgNVBAsMGEV4YW1wbGUgQ29tIEluYy4gUm9vdCBDQTEh   ...   Name   Description   pemtrustedcas_filepath   Absolute path to the PEM file containing the root CA(s) of your IdP.    pemtrustedcas_content   The root CA content of your IdP. Cannot be used if pemtrustedcas_filepath is set.  TLS client authentication  To use TLS client authentication, configure the PEM certificate and private key the Security plugin should send for TLS client authentication (or its content):  config: pemkey_filepath: /path/to/private.key.pem pemkey_password: private_key_password pemcert_filepath: /path/to/certificate.pem config: pemkey_content: |-   MIID2jCCAsKgAwIBAgIBBTANBgkqhkiG9w0BAQUFADCBlTETMBEGCgmSJomT8ixk   ARkWA2NvbTEXMBUGCgmSJomT8ixkARkWB2V4YW1wbGUxGTAXBgNVBAoMEEV4YW1w   bGUgQ29tIEluYy4xJDAiBgNVBAsMG0V4YW1wbGUgQ29tIEluYy4gU2lnbmluZyBD   ... pemkey_password: private_key_password pemcert_content: |-   MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCHRZwzwGlP2FvL   oEzNeDu2XnOF+ram7rWPT6fxI+JJr3SDz1mSzixTeHq82P5A7RLdMULfQFMfQPfr   WXgB4qfisuDSt+CPocZRfUqqhGlMG2l8LgJMr58tn0AHvauvNTeiGlyXy0ShxHbD   ...   Name   Description   enable_ssl_client_auth   Whether to send the client certificate to the IdP server. Default is false.    pemcert_filepath   Absolute path to the the client certificate.    pemcert_content   The content of the client certificate. Cannot be used when pemcert_filepath is set.    pemkey_filepath   Absolute path to the file containing the private key of the client certificate.    pemkey_content   The content of the private key of your client certificate. Cannot be used when pemkey_filepath is set.    pemkey_password   The password of your private key, if any.  Enabled ciphers and protocols  You can limit the allowed ciphers and TLS protocols by using the following keys:    Name   Description   enabled_ssl_ciphers   Array. Enabled TLS cipher suites. Only Java format is supported.    enabled_ssl_protocols   Array. Enabled TLS protocols. Only Java format is supported.  (Advanced) DoS protection  To help protect against denial-of-service (DoS) attacks, the Security plugin only allows a maximum number of new key IDs in a certain span of time. If the number of new key IDs exceeds this threshold, the Security plugin returns HTTP status code 503 (Service Unavailable) and refuses to query the IdP. By default, the Security plugin does not allow for more than 10 unknown key IDs within 10 seconds. To modify these settings:    Name   Description   refresh_rate_limit_count   The maximum number of unknown key IDs in the time frame. Default is 10.    refresh_rate_limit_time_window_ms   The time frame to use when checking the maximum number of unknown key IDs, in milliseconds. Default is 10000 (10 seconds).  Kibana single sign-on  Activate OpenID Connect by adding the following to kibana.yml:  opendistro_security.auth.type: &quot;openid&quot; Configuration  OpenID Connect providers usually publish their configuration in JSON format under the metadata url. Therefore most settings can be pulled in automatically, so the Kibana configuration becomes minimal. The most important settings are:   Connect URL Client ID  Every IdP can host multiple clients (sometimes called applications) with different settings and authentication protocols. When enabling OpenID Connect, you should create a new client for Kibana in your IdP. The client ID uniquely identifies Kibana.  Client secret  Beyond the ID, each client also has a client secret assigned. The client secret is usually generated when the client is created. Applications can only obtain an identity token when they provide a client secret. You can find this secret in the settings of the client on your IdP.  Configuration parameters    Name   Description   opendistro_security.openid.connect_url   The URL where the IdP publishes the OpenID metadata. Required.    opendistro_security.openid.client_id   The ID of the OpenID Connect client configured in your IdP. Required.    opendistro_security.openid.client_secret   The client secret of the OpenID Connect client configured in your IdP. Required.    opendistro_security.openid.scope   The scope of the identity token issued by the IdP. Optional. Default is openid profile email address phone.    opendistro_security.openid.header   HTTP header name of the JWT token. Optional. Default is Authorization.    opendistro_security.openid.logout_url   The logout URL of your IdP. Optional. Only necessary if your IdP does not publish the logout URL in its metadata.    opendistro_security.openid.base_redirect_url   The base of the redirect URL that will be sent to your IdP. Optional. Only necessary when Kibana is behind a reverse proxy, in which case it should be different than server.host and server.port in kibana.yml.  Configuration example  # Enable OpenID authentication opendistro_security.auth.type: &quot;openid&quot;  # The IdP metadata endpoint opendistro_security.openid.connect_url: &quot;http://keycloak.example.com:8080/auth/realms/master/.well-known/openid-configuration&quot;  # The ID of the OpenID Connect client in your IdP opendistro_security.openid.client_id: &quot;kibana-sso&quot;  # The client secret of the OpenID Connect client opendistro_security.openid.client_secret: &quot;a59c51f5-f052-4740-a3b0-e14ba355b520&quot;  # Use HTTPS instead of HTTP elasticsearch.url: &quot;https://&amp;lt;hostname&amp;gt;.com:&amp;lt;http port&amp;gt;&quot;  # Configure the Kibana internal server user elasticsearch.username: &quot;kibanaserver&quot; elasticsearch.password: &quot;kibanaserver&quot;  # Disable SSL verification when using self-signed demo certificates elasticsearch.ssl.verificationMode: none  # Whitelist basic headers and multi-tenancy header elasticsearch.requestHeadersWhitelist: [&quot;Authorization&quot;, &quot;security_tenant&quot;] Elasticsearch configuration  Because Kibana requires that the internal Kibana server user can authenticate via HTTP basic authentication, you must configure two authentication domains. For OpenID Connect, the HTTP basic domain has to be placed first in the chain. Make sure you set the challenge flag to false.  basic_internal_auth_domain: enabled: true order: 0 http_authenticator:   type: basic   challenge: false authentication_backend:   type: internal openid_auth_domain: enabled: true order: 1 http_authenticator:   type: openid   challenge: false   config:   subject_key: preferred_username   roles_key: roles   openid_connect_url: https://keycloak.example.com:8080/auth/realms/master/.well-known/openid-configuration authentication_backend:   type: noop  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/security/openid-connect/",
      "relUrl": "/docs/security/openid-connect/"
    },
  
    {
      "id": "37",
      "title": "Supported Operations",
      "content": "Supported operations Open Distro for Elasticsearch supports the following SQL operations. Statements Conditions Aggregations Include and exclude fields Functions Joins Show   Statements    Statement   Example   Select   SELECT * FROM my-index    Delete   DELETE FROM my-index WHERE _id=1    Where   SELECT * FROM my-index WHERE [&#39;field&#39;]=&#39;value&#39;    Order by   SELECT * FROM my-index ORDER BY _id asc    Group by   SELECT * FROM my-index GROUP BY range(age, 20,30,39)    Limit   SELECT * FROM my-index LIMIT 50 (default is 200)    Union   SELECT * FROM my-index1 UNION SELECT * FROM my-index2    Minus   SELECT * FROM my-index1 MINUS SELECT * FROM my-index2  Like any complex query, large UNION and MINUS statements can strain or even crash your cluster.  Conditions    Condition   Example   Like   SELECT * FROM my-index WHERE name LIKE &#39;j%&#39;    And   SELECT * FROM my-index WHERE name LIKE &#39;j%&#39; AND age &amp;gt; 21    Or   SELECT * FROM my-index WHERE name LIKE &#39;j%&#39; OR age &amp;gt; 21    Count distinct   SELECT count(distinct age) FROM my-index    In   SELECT * FROM my-index WHERE name IN (&#39;alejandro&#39;, &#39;carolina&#39;)    Not   SELECT * FROM my-index WHERE name NOT IN (&#39;jane&#39;)    Between   SELECT * FROM my-index WHERE age BETWEEN 20 AND 30    Aliases   SELECT avg(age) AS Average_Age FROM my-index    Date   SELECT * FROM my-index WHERE birthday=&#39;1990-11-15&#39;    Null   SELECT * FROM my-index WHERE name IS NULL  Aggregations    Aggregation   Example   avg()   SELECT avg(age) FROM my-index    count()   SELECT count(age) FROM my-index    max()   SELECT max(age) AS Highest_Age FROM my-index    min()   SELECT min(age) AS Lowest_Age FROM my-index    sum()   SELECT sum(age) AS Age_Sum FROM my-index  Include and exclude fields    Pattern   Example   include()   SELECT include(&#39;a*&#39;), exclude(&#39;age&#39;) FROM my-index    exclude()   SELECT exclude(&#39;*name&#39;) FROM my-index  Functions  You must enable fielddata in the document mapping for most string functions to work properly.    Function   Example   floor   SELECT floor(number) AS Rounded_Down FROM my-index    trim   SELECT trim(name) FROM my-index    log   SELECT log(number) FROM my-index    log10   SELECT log10(number) FROM my-index    substring   SELECT substring(name, 2,5) FROM my-index    round   SELECT round(number) FROM my-index    sqrt   SELECT sqrt(number) FROM my-index    concat_ws   SELECT concat_ws(&#39; &#39;, age, height) AS combined FROM my-index    /   SELECT number / 100 FROM my-index    %   SELECT number % 100 FROM my-index    date_format   SELECT date_format(date, &#39;Y&#39;) FROM my-index  Joins  See Joins for constraints and limitations.    Join   Example   Inner join   SELECT p.firstname, p.lastname, p.gender, dogs.name FROM people p JOIN dogs d ON d.holdersName = p.firstname WHERE p.age &amp;gt; 12 AND d.age &amp;gt; 1    Left outer join   SELECT p.firstname, p.lastname, p.gender, dogs.name FROM people p LEFT JOIN dogs d ON d.holdersName = p.firstname    Cross join   SELECT p.firstname, p.lastname, p.gender, dogs.name FROM people p CROSS JOIN dogs d  Show  Show commands, well, show you indices and mappings that match an index pattern. You can use * or % for wildcards.    Show   Example   Show tables like   SHOW TABLES LIKE logs-* ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/sql/operations/",
      "relUrl": "/docs/sql/operations/"
    },
  
    {
      "id": "38",
      "title": "Other Components",
      "content": "Other components Open Distro for Elasticsearch has a number of other components that you might want to use:   Java Database Connectivity (JDBC) driver PerfTop client for Performance Analyzer Job Scheduler plugin, an extensible plugin for running periodic jobs  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/install/other-components/",
      "relUrl": "/docs/install/other-components/"
    },
  
    {
      "id": "39",
      "title": "Permissions",
      "content": "Permissions This page is a complete list of available permissions in the Security plugin. Each permission controls access to a data type or API. For more information about permissions, see Configuration.  Rather than creating new action groups from individual permissions, you can often achieve your desired security posture using some combination of the default action groups. To learn more, see Default Action Groups.  Cluster   cluster:admin/ingest/pipeline/delete cluster:admin/ingest/pipeline/get cluster:admin/ingest/pipeline/put cluster:admin/ingest/pipeline/simulate cluster:admin/ingest/processor/grok/get cluster:admin/reindex/rethrottle cluster:admin/repository/delete cluster:admin/repository/get cluster:admin/repository/put cluster:admin/repository/verify cluster:admin/reroute cluster:admin/script/delete cluster:admin/script/get cluster:admin/script/put cluster:admin/settings/update cluster:admin/snapshot/create cluster:admin/snapshot/delete cluster:admin/snapshot/get cluster:admin/snapshot/restore cluster:admin/snapshot/status cluster:admin/snapshot/status* cluster:admin/tasks/cancel cluster:admin/tasks/test cluster:admin/tasks/testunblock cluster:monitor/allocation/explain cluster:monitor/health cluster:monitor/main cluster:monitor/nodes/hot_threads cluster:monitor/nodes/info cluster:monitor/nodes/liveness cluster:monitor/nodes/stats cluster:monitor/nodes/usage cluster:monitor/remote/info cluster:monitor/state cluster:monitor/stats cluster:monitor/task cluster:monitor/task/get cluster:monitor/tasks/list Indices   indices:admin/aliases indices:admin/aliases/exists indices:admin/aliases/get indices:admin/analyze indices:admin/cache/clear indices:admin/close indices:admin/create indices:admin/delete indices:admin/exists indices:admin/flush indices:admin/flush* indices:admin/forcemerge indices:admin/get indices:admin/mapping/put indices:admin/mappings/fields/get indices:admin/mappings/fields/get* indices:admin/mappings/get indices:admin/open indices:admin/refresh indices:admin/refresh* indices:admin/rollover indices:admin/seq_no/global_checkpoint_sync indices:admin/settings/update indices:admin/shards/search_shards indices:admin/shrink indices:admin/synced_flush indices:admin/template/delete indices:admin/template/get indices:admin/template/put indices:admin/types/exists indices:admin/upgrade indices:admin/validate/query indices:data/read/explain indices:data/read/field_caps indices:data/read/field_caps* indices:data/read/get indices:data/read/mget indices:data/read/mget* indices:data/read/msearch indices:data/read/msearch/template indices:data/read/mtv indices:data/read/mtv* indices:data/read/scroll indices:data/read/scroll/clear indices:data/read/search indices:data/read/search* indices:data/read/search/template indices:data/read/tv indices:data/write/bulk indices:data/write/bulk* indices:data/write/delete indices:data/write/delete/byquery indices:data/write/index indices:data/write/reindex indices:data/write/update indices:data/write/update/byquery indices:monitor/recovery indices:monitor/segments indices:monitor/settings/get indices:monitor/shard_stores indices:monitor/stats indices:monitor/upgrade  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/security/permissions/",
      "relUrl": "/docs/security/permissions/"
    },
  
    {
      "id": "40",
      "title": "Standalone Elasticsearch Plugin Install",
      "content": "Standalone Elasticsearch plugin install If you don’t want to use the all-in-one Open Distro for Elasticsearch installation options, you can install the Security, Alerting, SQL, and Performance Analyzer plugins on a compatible Elasticsearch cluster just like any other Elasticsearch plugin.  In addition to their Elasticsearch plugins, Security and Alerting have corresponding Kibana plugins that you probably want to install, as well.  Navigate to the Elasticsearch home directory (likely /usr/share/elasticsearch) and run the install command for each plugin.  Security  sudo bin/elasticsearch-plugin install https://d3g5vo6xdbdb9a.cloudfront.net/downloads/elasticsearch-plugins/opendistro-security/opendistro_security-0.9.0.0.zip Alerting  sudo bin/elasticsearch-plugin install https://d3g5vo6xdbdb9a.cloudfront.net/downloads/elasticsearch-plugins/opendistro-alerting/opendistro_alerting-0.9.0.0.zip SQL  sudo bin/elasticsearch-plugin install https://d3g5vo6xdbdb9a.cloudfront.net/downloads/elasticsearch-plugins/opendistro-sql/opendistro_sql-0.9.0.0.zip Performance Analyzer  sudo bin/elasticsearch-plugin install https://d3g5vo6xdbdb9a.cloudfront.net/downloads/elasticsearch-plugins/performance-analyzer/opendistro_performance_analyzer-0.9.0.0.zip Compatibility  See Version history for the versions of Elasticsearch that Open Distro for Elasticsearch supports. You must have the exact compatible version installed (e.g. 6.6.2 and not 6.6.1). To get a list of available Elasticsearch versions on CentOS 7 and Amazon Linux 2:  sudo yum --showduplicates list elasticsearch Then you can specify the version you need:  sudo yum install elasticsearch-6.6.2 After installing the Security plugin, you can run sudo sh /usr/share/elasticsearch/plugins/opendistro_security/tools/install_demo_configuration.sh to quickly get started with demo certificates or configure it manually for production workloads.  List installed plugins  To check your installed plugins:  sudo bin/elasticsearch-plugin list Remove plugins  sudo bin/elasticsearch-plugin remove &amp;lt;plugin-name&amp;gt; Then restart Elasticsearch on the node.  Update plugins  Elasticsearch doesn’t update plugins. Instead, you have to remove and reinstall them:  sudo bin/elasticsearch-plugin remove &amp;lt;plugin-name&amp;gt; sudo bin/elasticsearch-plugin install &amp;lt;plugin-name&amp;gt;  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/install/plugins/",
      "relUrl": "/docs/install/plugins/"
    },
  
    {
      "id": "41",
      "title": "Standalone Kibana Plugin Install",
      "content": "Standalone Kibana plugin install If you don’t want to use the all-in-one Open Distro for Elasticsearch installation options, you can install the Security and Alerting plugins for Kibana individually.  Prerequisites   An Elasticsearch cluster that uses a compatible version The Security and/or Alerting plugins installed on the cluster The corresponding version of Kibana (e.g. Kibana 6.7.1 works with Elasticsearch 6.7.1) Install  Navigate to the Kibana home directory (likely /usr/share/kibana) and run the install command for each plugin.  Security  sudo bin/kibana-plugin install https://d3g5vo6xdbdb9a.cloudfront.net/downloads/kibana-plugins/opendistro-security/opendistro_security_kibana_plugin-0.9.0.0.zip Alerting  sudo bin/kibana-plugin install https://d3g5vo6xdbdb9a.cloudfront.net/downloads/kibana-plugins/opendistro-alerting/opendistro-alerting-0.9.0.0.zip List installed plugins  To check your installed plugins:  sudo bin/kibana-plugin list Remove plugins  sudo bin/kibana-plugin remove &amp;lt;plugin-name&amp;gt; Then restart Kibana. After the removal of any plugin, Kibana performs an “optimize” operation the next time you start it. This operation takes several minutes even on fast machines, so be patient.  Update plugins  Kibana doesn’t update plugins. Instead, you have to remove and reinstall them:  sudo bin/kibana-plugin remove &amp;lt;plugin-name&amp;gt; sudo bin/kibana-plugin install &amp;lt;plugin-name&amp;gt;  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/kibana/plugins/",
      "relUrl": "/docs/kibana/plugins/"
    },
  
    {
      "id": "42",
      "title": "Metrics Reference",
      "content": "Metrics reference This page contains all Performance Analyzer metrics. All metrics support the avg, sum, min, and max aggregations, although certain metrics measure only one thing, making the choice of aggregation irrelevant.  For information on dimensions, see the dimensions reference.  This list is extensive. We recommend Ctrl + F to find what you’re looking for.    Metric   Dimensions   Description   CPU_Utilization   ShardID, IndexName, Operation, ShardRole   CPU usage ratio. CPU time (in milliseconds) used by the associated thread(s) in the past five seconds, divided by 5000 milliseconds.  Paging_MajfltRate   The number of major faults per second in the past five seconds. A major fault requires the process to load a memory page from disk.  Paging_MinfltRate   The number of minor faults per second in the past five seconds. A minor fault does not requires the process to load a memory page from disk.  Paging_RSS   The number of pages the process has in real memorythe pages that count towards text, data, or stack space. This number does not include pages that have not been demand-loaded in or swapped out.  Sched_Runtime   Time (seconds) spent executing on the CPU per context switch.  Sched_Waittime   Time (seconds) spent waiting on a run queue per context switch.  Sched_CtxRate   Number of times run on the CPU per second in the past five seconds.  Heap_AllocRate   An approximation of the heap memory allocated, in bytes, per second in the past five seconds  IO_ReadThroughput   Number of bytes read per second in the last five seconds.  IO_WriteThroughput   Number of bytes written per second in the last five seconds.  IO_TotThroughput   Number of bytes read or written per second in the last five seconds.  IO_ReadSyscallRate   Read system calls per second in the last five seconds.  IO_WriteSyscallRate   Write system calls per second in the last five seconds.  IO_TotalSyscallRate   Read and write system calls per second in the last five seconds.  Thread_Blocked_Time   Average time (seconds) that the associated thread(s) blocked to enter or reenter a monitor.  Thread_Blocked_Event   The total number of times that the associated thread(s) blocked to enter or reenter a monitor (i.e. the number of times a thread has been in the blocked state).  Indexing_ThrottleTime   Time (milliseconds) that the index has been under merge throttling control in the past five seconds.  Cache_Query_Hit   The number of successful lookups in the query cache in the past five seconds.  Cache_Query_Miss   The number of lookups in the query cache that failed to retrieve a `DocIdSet` in the past five seconds. `DocIdSet` is a set of document IDs in Lucene.  Cache_Query_Size   Query cache memory size in bytes.  Cache_FieldData_Eviction   The number of times Elasticsearch has evicted data from the fielddata heap space (occurs when the heap space is full) in the past five seconds.  Cache_FieldData_Size   Fielddata memory size in bytes.  Cache_Request_Hit   The number of successful lookups in the shard request cache in the past five seconds.  Cache_Request_Miss   The number of lookups in the request cache that failed to retrieve the results of search requests in the past five seconds.  Cache_Request_Eviction   The number of times Elasticsearch evicts data from shard request cache (occurs when the request cache is full) in the past five seconds.  Cache_Request_Size   Shard request cache memory size in bytes.  Refresh_Event   The total number of refreshes executed in the past five seconds.  Refresh_Time   The total time (milliseconds) spent executing refreshes in the past five seconds  Flush_Event   The total number of flushes executed in the past five seconds.  Flush_Time   The total time (milliseconds) spent executing flushes in the past five seconds.  Merge_Event   The total number of merges executed in the past five seconds.  Merge_Time   The total time (milliseconds) spent executing merges in the past five seconds.  Merge_CurrentEvent   The current number of merges executing.  Indexing_Buffer   Index buffer memory size in bytes.  Segments_Total   The number of segments.  Segments_Memory   Estimated memory usage of segments in bytes.  Terms_Memory   Estimated memory usage of terms dictionaries in bytes.  StoredFields_Memory   Estimated memory usage of stored fields in bytes.  TermVectors_Memory   Estimated memory usage of term vectors in bytes.  Norms_Memory   Estimated memory usage of norms (normalization factors) in bytes.  Points_Memory   Estimated memory usage of points in bytes.  DocValues_Memory   Estimated memory usage of doc values in bytes.  IndexWriter_Memory   Estimated memory usage by the index writer in bytes.  Bitset_Memory   Estimated memory usage for the cached bit sets in bytes.  VersionMap_Memory   Estimated memory usage of the version map in bytes.  ShardEvents   The total number of events executed on a shard in the past five seconds.  ShardBulkDocs   The total number of documents indexed in the past five seconds.  Latency   Operation, Exception, Indices, HTTPRespCode, ShardID, IndexName, ShardRole   Latency (milliseconds) of a request.  GC_Collection_Event   MemType   The number of garbage collections that have occurred in the past five seconds.  GC_Collection_Time   The approximate accumulated time (milliseconds) of all garbage collections that have occurred in the past five seconds.  Heap_Committed   The amount of memory (bytes) that is committed for the JVM to use.  Heap_Init   The amount of memory (bytes) that the JVM initially requests from the operating system for memory management.  Heap_Max   The maximum amount of memory (bytes) that can be used for memory management.  Heap_Used   The amount of used memory in bytes.  Disk_Utilization   DiskName   Disk utilization rate: percentage of disk time spent reading and writing by the Elasticsearch process in the past five seconds.  Disk_WaitTime   Average duration (milliseconds) of read and write operations in the past five seconds.  Disk_ServiceRate   Service rate: MB read or written per second in the past five seconds. This metric assumes that each disk sector stores 512 bytes.  Net_TCP_NumFlows   DestAddr   Number of samples collected. Performance Analyzer collects one sample every five seconds.  Net_TCP_TxQ   Average number of TCP packets in the send buffer.  Net_TCP_RxQ   Average number of TCP packets in the receive buffer.  Net_TCP_Lost   Average number of unrecovered recurring timeouts. This number is reset when the recovery finishes or `SND.UNA` is advanced. `SND.UNA` is the sequence number of the first byte of data that has been sent, but not yet acknowledged.  Net_TCP_SendCWND   Average size (bytes) of the sending congestion window.  Net_TCP_SSThresh   Average size (bytes) of the slow start size threshold.  Net_PacketRate4   Direction   The total number of IPv4 datagrams transmitted/received from/by interfaces per second, including those transmitted or received in error  Net_PacketDropRate4   The total number of IPv4 datagrams transmitted or received in error per second.  Net_PacketRate6   The total number of IPv6 datagrams transmitted or received from or by interfaces per second, including those transmitted or received in error.  Net_PacketDropRate6   The total number of IPv6 datagrams transmitted or received in error per second.  Net_Throughput   The number of bytes of data transmitted or received per second by all network interfaces.  ThreadPool_QueueSize   ThreadPoolType   The size of the task queue.  ThreadPool_RejectedReqs   The number of rejected executions.  ThreadPool_TotalThreads   The current number of threads in the pool.  ThreadPool_ActiveThreads   The approximate number of threads that are actively executing tasks.  Master_PendingQueueSize   N/A   The current number of pending tasks in the cluster state update thread. Each node has a cluster state update thread that submits cluster state update tasks (create index, update mapping, allocate shard, fail shard, etc.).  HTTP_RequestDocs   Operation, Exception, Indices, HTTPRespCode   The number of items in the request (only for `_bulk` request type).  HTTP_TotalRequests   The number of finished requests in the past five seconds.  CB_EstimatedSize   CBType   The current number of estimated bytes.  CB_TrippedEvents   The number of times the circuit breaker has tripped.  CB_ConfiguredSize   The limit (bytes) for how much memory operations can use.  Master_Task_Queue_Time   MasterTaskInsertOrder, MasterTaskPriority, MasterTaskType, MasterTaskMetadata   The time (milliseconds) that a master task spent in the queue.  Master_Task_Run_Time   The time (milliseconds) that a master task has been executed.   Dimensions reference    Dimension   Return values   ShardID   ID for the shard (e.g. 1).    IndexName   Name of the index (e.g. my-index).    Operation   Type of operation (e.g. shardbulk).    ShardRole   primary, replica    Exception   Elasticsearch exceptions (e.g. org.elasticsearch.index_not_found_exception).    Indices   The list of indices in the request URI.    HTTPRespCode   Response code from Elasticsearch (e.g. 200).    MemType   totYoungGC, totFullGC, Survivor, PermGen, OldGen, Eden, NonHeap, Heap    DiskName   Name of the disk (e.g. sda1).    DestAddr   Destination address (e.g. 010015AC).    Direction   in, out    ThreadPoolType   The Elasticsearch thread pools (e.g. index, search,snapshot).    CBType   accounting, fielddata, in_flight_requests, parent, request    MasterTaskInsertOrder   The order in which the task was inserted (e.g. 3691).    MasterTaskPriority   Priority of the task (e.g. URGENT). Elasticsearch executes higher priority tasks before lower priority ones, regardless of insert_order.    MasterTaskType   shard-started, create-index, delete-index, refresh-mapping, put-mapping, CleanupSnapshotRestoreState, Update snapshot state    MasterTaskMetadata   Metadata for the task (if any). ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/pa/reference/",
      "relUrl": "/docs/pa/reference/"
    },
  
    {
      "id": "43",
      "title": "RPM",
      "content": "RPM package Installing and running Open Distro for Elasticsearch from an RPM package is a more manual process than the Docker image. We recommend CentOS 7 and Amazon Linux 2, but any RPM-based distribution that uses systemd should work. These steps assume you’re using CentOS 7. cd /etc/yum.repos.d/  sudo curl https://d3g5vo6xdbdb9a.cloudfront.net/yum/opendistroforelasticsearch-artifacts.repo -o opendistroforelasticsearch-artifacts.repo  You can also create the file manually. It looks like this:  [elasticsearch-6.x] name=Elasticsearch repository for 6.x packages baseurl=https://artifacts.elastic.co/packages/oss-6.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md  [opendistroforelasticsearch-artifacts-repo] name=Release RPM artifacts of OpenDistroForElasticsearch baseurl=https://d3g5vo6xdbdb9a.cloudfront.net/yum/noarch/ enabled=1 gpgkey=https://d3g5vo6xdbdb9a.cloudfront.net/GPG-KEY-opendistroforelasticsearch gpgcheck=1 repo_gpgcheck=1 autorefresh=1 type=rpm-md   Open Distro for Elasticseach requires the full Java Development Kit (JDK), not just the Java Runtime Environment (JRE). If you don’t have the JDK installed, install either version 8 or version 11:  # Java 11 sudo yum install java-11-openjdk-devel # Java 8 sudo yum install java-1.8.0-openjdk-devel  If you’re using Amazon Linux 2, you might need to use Java 8.  sudo yum install opendistroforelasticsearch  If you installed Java 8, run the following command:  sudo ln -s /usr/lib/jvm/java-1.8.0/lib/tools.jar /usr/share/elasticsearch/lib/   To start Open Distro for Elasticsearch:  sudo systemctl start elasticsearch.service   Send requests to the server to verify that Elasticsearch is up and running:  curl -XGET https://localhost:9200 -u admin:admin --insecure curl -XGET https://localhost:9200/_cat/nodes?v -u admin:admin --insecure curl -XGET https://localhost:9200/_cat/plugins?v -u admin:admin --insecure   For instructions on installing and running Kibana, see Kibana.  To check the status of the service:  systemctl status elasticsearch.service  You might notice some errors if you are using Java 8. If the service is still active (running), you can safely ignore them:  elasticsearch[3969]: java.security.policy: error adding Entry: elasticsearch[3969]: java.net.MalformedURLException: unknown protocol: jrt elasticsearch[3969]: java.security.policy: error adding Entry: elasticsearch[3969]: java.net.MalformedURLException: unknown protocol: jrt   To stop Open Distro for Elasticsearch:  sudo systemctl stop elasticsearch.service  Configuration  To run Open Distro for Elasticsearch when the system starts:  sudo /bin/systemctl daemon-reload sudo /bin/systemctl enable elasticsearch.service You can also modify the values in /etc/sysconfig/elasticsearch (JAVA_HOME, most notably), /etc/elasticsearch/elasticsearch.yml, and /etc/elasticsearch/jvm.options (to set the heap size, most notably). To learn more, see Elasticsearch configuration and Important Settings on the Docker page.  Where are the files?  The RPM package installs files to the following locations:    File type   Location   Elasticsearch home, management scripts, and plugins   /usr/share/elasticsearch/    Configuration files   /etc/elasticsearch    Environment variables   /etc/sysconfig/elasticsearch    Logs   /var/log/elasticsearch    Shard data   /var/lib/elasticsearch ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/install/rpm/",
      "relUrl": "/docs/install/rpm/"
    },
  
    {
      "id": "44",
      "title": "SAML",
      "content": "SAML The Security plugin supports user authentication via SAML single sign-on. The Security plugin implements the web browser SSO profile of the SAML 2.0 protocol.  This profile is meant for use with web browsers. It is not a general-purpose way of authenticating users against the Security plugin, so its primary use case is to support Kibana single sign-on. Activating SAML Running multiple authentication domains Identity provider metadata Idp and service provider entity ID Kibana settings Username and Role attributes Request signing Logout Exchange key settings TLS settings  Certificate validation   Client authentication   Enabled ciphers and protocols  Minimal configuration example Kibana configuration   Activating SAML  To use SAML for authentication, you need to configure a respective authentication domain in the authc section of plugins/opendistro_security/securityconfig/config.yml. Since SAML works solely on the HTTP layer, you do not need any authentication_backend and can set it to noop. Place all SAML specific configuration options in this chapter in the config section of the SAML HTTP authenticator:  authc: saml_auth_domain:   enabled: true   order: 1   http_authenticator:   type: saml   challenge: true   config:   idp:  metadata_file: okta.xml  ...   authentication_backend:   type: noop Once you have configured SAML in config.yml, you need to also activate it in Kibana.  Running multiple authentication domains  We recommend adding at least one other authentication domain, such as LDAP or the internal user database, to support API access to Elasticsearch without SAML. For Kibana and the internal Kibana server user, you also need to add another authentication domain that supports basic authentication. This authentication domain should be placed first in the chain, and the challenge flag must be set to false:  authc: basic_internal_auth_domain:   enabled: true   order: 0   http_authenticator:   type: basic   challenge: false   authentication_backend:   type: internal saml_auth_domain:  enabled: true  order: 1  http_authenticator:   type: &#39;saml&#39;   challenge: true   config:  ...  authentication_backend:   type: noop Identity provider metadata  A SAML identity provider (IdP) provides a SAML 2.0 metadata file describing the IdP’ss capabilities and configuration. The Security plugin can read IdP metadata either from a URL or a file. Which way to choose depends on your IdP and your preferences. The SAML 2.0 metadata file is required.    Name   Description   idp.metadata_file   The path to the SAML 2.0 metadata file of your IdP. Place the metadata file in the config directory of Open Distro for Elasticsearch. The path has to be specified relative to the config directory. Required if idp.metadata_url is not set.    idp.metadata_url   The SAML 2.0 metadata URL of your IdP. Required if idp.metadata_file is not set.  Idp and service provider entity ID  An entity ID is a globally unique name for a SAML entity, either an IdP or a service provider (SP). The IdP entity ID is usually provided by your IdP. The SP entity ID is the name of the configured application or client in your IdP. We recommend adding a new application for Kibana and using the URL of your Kibana installation as SP entity ID.    Name   Description   idp.entity_id   The entity ID of your IdP. Required.    sp.entity_id   The entity ID of the service provider. Required.  Kibana settings  The Web Browser SSO profile works by exchanging information via HTTP GET or POST. For example, after you log in to your IdP, it sends an HTTP POST back to Kibana containing the SAML response. You must configure the base URL of your Kibana installation where the HTTP requests are being sent to.    Name   Description   kibana_url   The Kibana base URL. Required.  Username and Role attributes  Subjects (i.e. usernames) are usually stored in the NameID element of a SAML response:  &amp;lt;saml2:Subject&amp;gt; &amp;lt;saml2:NameID&amp;gt;admin&amp;lt;/saml2:NameID&amp;gt; ... &amp;lt;/saml2:Subject&amp;gt; If your IdP is compliant with the SAML 2.0 specification, you do not need to set anything special. If your IdP uses a different element name, you can also specify its name explicitly.  Role attributes are optional. However, most IdPs can be configured to add roles in the SAML assertions as well. If present, you can use these roles in your role mappings:  &amp;lt;saml2:Attribute Name=&#39;Role&#39;&amp;gt; &amp;lt;saml2:AttributeValue &amp;gt;Everyone&amp;lt;/saml2:AttributeValue&amp;gt; &amp;lt;saml2:AttributeValue &amp;gt;Admins&amp;lt;/saml2:AttributeValue&amp;gt; &amp;lt;/saml2:Attribute&amp;gt; If you want to extract roles from the SAML response, you need to specify the element name that contains the roles.    Name   Description   subject_key   The attribute in the SAML response where the subject is stored. Optional. If not configured, the NameID attribute is used.    roles_key   The attribute in the SAML response where the roles are stored. Optional. If not configured, no roles are used.  Request signing  Requests from the Security plugin to the IdP can optionally be signed. Use the following settings to configure request signing.    Name   Description   sp.signature_private_key   The private key used to sign the requests. Optional. If not provided, requests are not signed.    sp.signature_algorithm   The algorithm used to sign the requests. See below for possible values.  The Security plugin supports the following signature algorithms:    Algorithm   Value   DSA_SHA1   http://www.w3.org/2000/09/xmldsig#dsa-sha1;    RSA_SHA1   http://www.w3.org/2000/09/xmldsig#rsa-sha1;    RSA_SHA256   http://www.w3.org/2001/04/xmldsig-more#rsa-sha256;    RSA_SHA384   http://www.w3.org/2001/04/xmldsig-more#rsa-sha384;    RSA_SHA512   http://www.w3.org/2001/04/xmldsig-more#rsa-sha512;  Logout  Usually, IdPs provide information about their individual logout URL in their SAML 2.0 metadata. If this is the case, the Security plugin uses them to render the correct logout link in Kibana. If your IdP does not support an explicit logout, you can force a re-login when the user visits Kibana again.    Name   Description   sp.forceAuthn   Force a re-login even if the user has an active session with the IdP.  At the moment the Security plugin only supports the HTTP-Redirect logout binding. Please make sure this is configured correctly in your IdP.  Exchange key settings  SAML, unlike other protocols, is not meant to be used for exchanging user credentials with each request. The Security plugin trades the SAML response for a lightweight JSON web token that stores the validated user attributes. This token is signed by an exchange key that you can choose freely. Note that when you change this key, all tokens signed with it will become invalid immediately.    Name   Description   exchange_key   The key to sign the token. The algorithm is HMAC256, so it should have at least 32 characters.  TLS settings  If you are loading the IdP metadata from a URL, we recommend to use SSL/TLS. If you use an external IdP like Okta or Auth0 that uses a trusted certificate, you usually do not need to configure anything. If you host the IdP yourself and use your own root CA, you can customize the TLS settings as follows. These settings are only used for loading SAML metadata over HTTPS.    Name   Description   idp.enable_ssl   Whether to enable the custom TLS configuration. Default is false (JDK settings are used).    idp.verify_hostnames   Whether to verify the hostnames of the server’s TLS certificate.  Example:  authc: saml_auth_domain:   enabled: true   order: 1   http_authenticator:   type: saml   challenge: true   config:   idp:  enable_ssl: true  verify_hostnames: true  ...   authentication_backend:   type: noop Certificate validation  Configure the root CA used for validating the IdP TLS certificate by setting one of the following configuration options:  config: idp:   pemtrustedcas_filepath: path/to/trusted_cas.pem config: idp:   pemtrustedcas_content: |-   MIID/jCCAuagAwIBAgIBATANBgkqhkiG9w0BAQUFADCBjzETMBEGCgmSJomT8ixk   ARkWA2NvbTEXMBUGCgmSJomT8ixkARkWB2V4YW1wbGUxGTAXBgNVBAoMEEV4YW1w   bGUgQ29tIEluYy4xITAfBgNVBAsMGEV4YW1wbGUgQ29tIEluYy4gUm9vdCBDQTEh   ...   Name   Description   idp.pemtrustedcas_filepath   Path to the PEM file containing the root CA(s) of your IdP. The files must be placed under the Open Distro for Elasticsearch config directory, and you must specify the path relative to that same directory.    idp.pemtrustedcas_content   The root CA content of your IdP server. Cannot be used when pemtrustedcas_filepath is set.  Client authentication  The Security plugin can use TLS client authentication when fetching the IdP metadata. If enabled, the Security plugin sends a TLS client certificate to the IdP for each metadata request. Use the following keys to configure client authentication:    Name   Description   idp.enable_ssl_client_auth   Whether to send a client certificate to the IdP server. Default is false.    idp.pemcert_filepath   Path to the PEM file containing the client certificate. The file must be placed under the Open Distro for Elasticsearch config directory, and the path must be specified relative to the config directory.    idp.pemcert_content   The content of the client certificate. Cannot be used when pemcert_filepath is set.    idp.pemkey_filepath   Path to the private key of the client certificate. The file must be placed under the Open Distro for Elasticsearch config directory, and the path must be specified relative to the config directory.    idp.pemkey_content   The content of the private key of your certificate. Cannot be used when pemkey_filepath is set.    idp.pemkey_password   The password of your private key, if any.  Enabled ciphers and protocols  You can limit the allowed ciphers and TLS protocols for the IdP connection. For example, you can only enable strong ciphers and limit the TLS versions to the most recent ones.    Name   Description   idp.enabled_ssl_ciphers   Array of enabled TLS ciphers. Only Java format is supported.    idp.enabled_ssl_protocols   Array of enabled TLS protocols. Only Java format is supported.  Minimal configuration example  authc: saml_auth_domain:   enabled: true   order: 1   http_authenticator:   type: saml   challenge: true   config:   idp:  metadata_file: metadata.xml  entity_id: http://idp.example.com/   sp:  entity_id: https://kibana.example.com   kibana_url: https://kibana.example.com:5601/   roles_key: Role   exchange_key: &#39;peuvgOLrjzuhXf ...&#39;   authentication_backend:   type: noop Kibana configuration  Since most of the SAML specific configuration is done in the Security plugin, just activate SAML in your kibana.yml by adding:  opendistro_security.auth.type: &quot;saml&quot; In addition, the Kibana endpoint for validating the SAML assertions must be whitelisted:  server.xsrf.whitelist: [&quot;/_opendistro/_security/saml/acs&quot;] If you use the logout POST binding, you also need to whitelist the logout endpoint:  server.xsrf.whitelist: [&quot;/_opendistro/_security/saml/acs&quot;, &quot;/_opendistro/_security/saml/logout&quot;]  ",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/security/saml/",
      "relUrl": "/docs/security/saml/"
    },
  
    {
      "id": "45",
      "title": "Apply Configuration Changes",
      "content": "Apply configuration changes  The Security plugin stores its configuration—including users, roles, and permissions—in an index on the Elasticsearch cluster (.opendistro_security). Storing these settings in an index lets you change settings without restarting the cluster and eliminates the need to place configuration files on any node.  After changing any of the configuration files in plugins/opendistro_security/securityconfig, however, you must run plugins/opendistro_security/tools/securityadmin.sh to load these new settings into the index. This script identifies itself against a cluster through an admin TLS certificate, in .pem, .jks, .p12, or .pfx format.  If the .opendistro_security index is already initialized, you can also use Kibana to change users, roles, and permissions. However, you need to run securityadmin.sh at least once to initialize the index and configure your authentication and authorization methods. —    1. TOC {:toc} —   Configure the admin certificate  You can configure all certificates that should have admin privileges in elasticsearch.yml stating their respective Distinguished Names (DNs). If you use the demo certificates, for example, you can use the kirk certificate:  yml opendistro_security.authcz.admin_dn: - CN=kirk,OU=client,O=client,L=test,C=DE  Do not use node certificates as admin certificates. Best practice is to keep these certificates separate. Also, do not use any whitespace between the parts of the DN. {: .warning }  Basic usage  securityadmin.sh can be run from any machine that has access to the transport port of your Elasticsearch cluster (default is 9300); you can change the Security plugin configuration without having to access your nodes via SSH.  Each node also includes the tool at plugins/opendistro_security/tools/securityadmin.sh. You might need to make the script executable before running it:  bash chmod +x plugins/opendistro_security/tools/securityadmin.sh  To print all available command line options, run the script with no arguments:  bash ./plugins/opendistro_security/tools/securityadmin.sh  To load configuration changes to Security plugin, you need to provide your admin certificate to the tool:  bash ./securityadmin.sh -cd ../securityconfig/ -icl -nhnv  -cacert ../../../config/root-ca.pem  -cert ../../../config/kirk.pem  -key ../../../config/kirk-key.pem  - The -cd option speficies where the Security plugin configuration files to upload to the cluster can be found. - The -icl (--ignore-clustername) option tells the Security plugin to upload the configuration regardless of the cluster name. As an alternative, you can also specify the cluster name with the -cn (--clustername) option. - Because the demo certificates are self-signed, we also disable hostname verification with the -nhnv (--disable-host-name-verification) option. - The -cacert, -cert and -key options define the location of your root CA certificate, the admin certificate, and the private key for the admin certificate. If the private key has a password, specify it with the -keypass option.  All PEM options:  Name | Description — | — -cert | The location of the PEM file containing the admin certificate and all intermediate certificates, if any. You can use an absolute or relative path. Relative paths are resolved relative to the execution directory of . -key | The location of the PEM file containing the private key of the admin certificate. You can use an absolute or relative path. Relative paths are resolved relative to the execution directory of securityadmin.sh. The key must be in PKCS#8 format. -keypass | The password of the private key of the admin certificate, if any. -cacert | The location of the PEM file containing the root certificate. You can use an absolute or relative path. Relative paths are resolved relative to the execution directory of securityadmin.sh.  Sample commands  Apply configuration in securityconfig using PEM certificates:  bash /usr/share/elasticsearch/plugins/opendistro_security/tools/securityadmin.sh -cacert /etc/elasticsearch/root-ca.pem -cert /etc/elasticsearch/kirk.pem -key /etc/elasticsearch/kirk-key.pem -cd /usr/share/elasticsearch/plugins/opendistro_security/securityconfig/  Apply configuration from a single file (config.yml) using PEM certificates:  bash ./securityadmin.sh -f ../securityconfig/config.yml -icl -nhnv -cert /etc/elasticsearch/kirk.pem -cacert /etc/elasticsearch/root-ca.pem -h -key /etc/elasticsearch/kirk-key.pem -t config  Apply configuration in securityconfig with keystore and truststore files:  bash ./securityadmin.sh  -cd /usr/share/elasticsearch/plugins/opendistro_security/securityconfig/  -ks /path/to/keystore.jks  -kspass changeit  -ts /path/to/truststore.jks  -tspass changeit  -nhnv  -icl  Using securityadmin with Keystore and Truststore files  You can also use keystore files in JKS format in conjunction with securityadmin.sh:  bash ./securityadmin.sh -cd ../securityconfig -icl -nhnv -ts &amp;lt;path/to/truststore&amp;gt; -tspass &amp;lt;truststore password&amp;gt; -ks &amp;lt;path/to/keystore&amp;gt; -kspass &amp;lt;keystore password&amp;gt;  Use the following options to control the key and truststore settings:  Name | Description — | — -ks | The location of the keystore containing the admin certificate and all intermediate certificates, if any. You can use an absolute or relative path. Relative paths are resolved relative to the execution directory of securityadmin.sh. -kspass | The password for the keystore. -kst | The key store type, either JKS or PKCS#12/PFX. If not specified, the Security plugin tries to determine the type from the file extension. -ksalias | The alias of the admin certificate, if any. -ts | The location of the truststore containing the root certificate. You can use an absolute or relative path. Relative paths are resolved relative to the execution directory of securityadmin.sh. -tspass | The password for the truststore. -tst | The truststore type, either JKS or PKCS#12/PFX. If not specified, the Security plugin tries to determine the type from the file extension. -tsalias | The alias for the root certificate, if any.  Elasticsearch settings  If you run a default Elasticsearch installation, which listens on transport port 9300, and uses elasticsearch as cluster name, you can omit the following settings altogether. Otherwise, specify your Elasticsearch settings by using the following switches:  Name | Description — | — -h | Elasticsearch hostname. Default is localhost. -p | Elasticsearch port. Default is 9300—not the HTTP port. -cn | Cluster name. Default is elasticsearch. -icl | Ignore cluster name. -sniff | Sniff cluster nodes. Sniffing detects available nodes using the Elasticsearch _cluster/state API. -arc,–accept-red-cluster | Execute securityadmin.sh even if the cluster state is red. Default is false, which means the script will not execute on a red cluster.  Certificate validation settings  Use the following options to control certificate validation:  Name | Description — | — -nhnv | Do not validate hostname. Default is false. -nrhn | Do not resolve hostname. Only relevant if -nhnv is not set. -noopenssl | Do not use OpenSSL, even if available. Default is to use OpenSSL if it is available.  Configuration files settings  The following switches define which configuration file(s) you want to push to the Security plugin. You can either push a single file or specify a directory containing one or more configuration files.  Name | Description — | — -cd | Directory containing multiple Security plugin configuration files. -f | Single configuration file. Can’t be used with -cd. -t | File type. -rl | Reload the current configuration and flush the internal cache.  To upload all configuration files in a directory, use:  bash ./securityadmin.sh -cd ../securityconfig -ts ... -tspass ... -ks ... -kspass ...  If you want to push a single configuration file, use:  bash ./securityadmin.sh -f ../securityconfig/internal_users.yml -t internalusers -ts ... -tspass ... -ks ... -kspass ...  The file type must be one of:  * config * roles * rolesmapping * internalusers * actiongroups  Cipher settings  You probably won’t need to change cipher settings. If you need to, use the following options:  Name | Description — | — -ec | Ccomma-separated list of enabled TLS ciphers. -ep | Comma-separated list of enabled TLS protocols.  Backup and Restore  You can download all current configuration files from your cluster with the following command:  bash ./securityadmin.sh -r -ts ... -tspass ... -ks ... -kspass ...  This command dumps the current Security plugin configuration from your cluster to individual files in the working directory. You can then use these files as backups or to load the configuration into a different cluster. This command is useful when moving a proof-of-concept to production.  You can specify the download location with the -cd option:  bash ./securityadmin.sh -r -h staging.example.com -p 9300 -cd /etc/backup/ -ts ... -tspass ... -ks ... -kspass ...  To upload the dumped files to another cluster:  bash ./securityadmin.sh -h production.example.com -p 9301 -cd /etc/backup/ -ts ... -tspass ... -ks ... -kspass ...  Name | Description — | — -r | Retrieve the current Security plugin configuration from a running cluster and dump it to the working directory. -cd | Specify the directory to store the files to. You can use an absolute or relative path. Relative paths are resolved relative to the execution directory of securityadmin.sh.  Other options  Name | Description — | — -dci | Delete the Security plugin configuration index and exit. This option is useful if the cluster state is red due to a corrupted Security plugin index. -esa | Enable shard allocation and exit. This option is useful if you disabled shard allocation while performing a full cluster restart and need to recreate the Security plugin index. -si | Displays the currently active Security plugin license -w | Displays information about the used admin certificate -rl | By default, the Security plugin caches authenticated users, along with their roles and permissions, for one hour. This option reloads the current Security plugin configuration stored in your cluster, invalidating any cached users, roles and permissions. -i | The Security plugin index name. Default is .opendistro_security. -er | Set explicit number of replicas or auto-expand expression for the opendistro_security index. -era | Enable replica auto-expand. -dra | Disable replica auto-expand. -us | Update the replica settings.",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/security/security-admin/",
      "relUrl": "/docs/security/security-admin/"
    },
  
    {
      "id": "46",
      "title": "Security Roles",
      "content": "Security roles  If you use the Security plugin alongside alerting, you might want to limit certain users to certain permissions. For example, you might want some users to only be able to view and acknowledge alerts, while others can create monitors and destinations. This page contains some sample roles for common use cases.  Mix and match these roles to give users the permissions they need to use the alerting feature.  Monitors run as the admin user, which means that monitors can query all documents in all indices and do not consider the roles of the user who created the monitor. Please keep this fact in mind while working with sensitive data. {: .warning }  View and acknowledge alerts  1. In Kibana, choose Security, Roles. 1. Create a new security role named alerting_alerts. 1. In the Index Permissions tab, choose Add new index and document type. 1. Specify the .opendistro-alerting-alerts index and * document type and Save. 1. Choose the CRUD action group and Save role definition. 1. Choose Role Mappings. 1. Map the alerting_acknowledge role to the desired users or backend roles.  Create, update, and delete monitors and destinations  1. In Kibana, choose Security, Roles. 1. Create a new security role named alerting_monitors. 1. In the Index Permissions tab, choose Add new index and document type. 1. Specify the .opendistro-alerting-config index and * document type and Save. 1. Choose the CRUD action group and Save role definition. 1. Choose Role Mappings. 1. Map the alerting_monitors role to the desired users or backend roles.  Read-only  1. In Kibana, choose Security, Roles. 1. Create a new security role named alerting_read_only. 1. In the Index Permissions tab, choose Add new index and document type. 1. Specify the .opendistro-alerting-alerts index and * document type and Save. 1. Choose the READ action group and Add new index and document type. 1. Specify the .opendistro-alerting-config index and * document type and Save. 1. Choose the READ action group and Save role definition. 1. Choose Role Mappings. 1. Map the alerting_read_only role to the desired users or backend roles.",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/alerting/security-roles/",
      "relUrl": "/docs/alerting/security-roles/"
    },
  
    {
      "id": "47",
      "title": "Management",
      "content": "Management  Alerting indices  The alerting feature creates several indices and one alias. Don’t delete these or modify their contents without using the alerting APIs.  Index | Purpose :— | :— .opendistro-alerting-alerts | Stores ongoing alerts. .opendistro-alerting-alert-history-&amp;lt;date&amp;gt; | Stores a history of completed alerts. .opendistro-alerting-config | Stores monitors, triggers, and destinations. .opendistro-alerting-alert-history-write (alias) | Provides a consistent URI for the .opendistro-alerting-alert-history-&amp;lt;date&amp;gt; index.  Alerting settings  We don’t recommend changing these settings; the defaults should work well for most use cases.  All settings are available using the Elasticsearch _cluster/settings API. None require a restart, and all can be marked persistent or transient.  Setting | Default | Description :— | :— | :— opendistro.alerting.enabled | true | Whether the alerting plugin is enabled or not. If disabled, all monitors immediately stop running. opendistro.alerting.input_timeout | 30s | How long the monitor can take to issue the search request. opendistro.alerting.bulk_timeout | 120s | How long the monitor can write alerts to the alert index. opendistro.alerting.alert_backoff_count | 2 | The number of retries for writing alerts before the operation fails. opendistro.alerting.alert_backoff_millis | 50ms | The amount of time to wait between retries—increases exponentially after each failed retry. opendistro.alerting.alert_history_rollover_period | 12h | How often completed alerts are rolled over from the .opendistro-alerts index to .opendistro-alert-history-&amp;lt;date&amp;gt;. opendistro.alerting.move_alerts_backoff_millis | 250 | The amount of time to wait between retries—increases exponentially after each failed retry. opendistro.alerting.move_alerts_backoff_count | 3 | The number of retries for moving alerts to a deleted state after their monitor or trigger has been deleted. opendistro.alerting.monitor.max_monitors | 1000 | The maximum number of monitors users can create. opendistro.alerting.alert_history_max_age | 24h | The oldest document the .opendistro-alert-history-&amp;lt;date&amp;gt; index should keep. opendistro.alerting.alert_history_max_docs | 1000 | The maximum number of documents the .opendistro-alert-history-&amp;lt;date&amp;gt; index should keep. opendistro.scheduled_jobs.sweeper.period | 5m | The alerting feature uses its “job sweeper” component to periodically check for new or updated jobs. This setting is the rate at which the sweeper checks to see if any jobs (monitors) have changed and need to be rescheduled. opendistro.scheduled_jobs.sweeper.page_size | 100 | The page size for the sweeper. You shouldn’t need to change this value. opendistro.scheduled_jobs.sweeper.backoff_millis | 50ms | The amount of time the sweeper waits between retries—increases exponentially after each failed retry.",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/alerting/settings/",
      "relUrl": "/docs/alerting/settings/"
    },
  
    {
      "id": "48",
      "title": "Take and Restore Snapshots",
      "content": "Take and restore snapshots  Snapshots are backups of a cluster’s indices and state. State includes cluster settings, node information, index settings, and shard allocation.  Snapshots have two main uses:  - Recovering from failure  For example, if cluster health goes red, you might restore the red indices from a snapshot.  - Migrating from one cluster to another  For example, if you are moving from a proof-of-concept to a production cluster, you might take a snapshot of the former and restore it on the latter. —    1. TOC {:toc} —   About snapshots  Snapshots are not instantaneous; they take time to complete and do not represent perfect point-in-time views of the cluster. While a snapshot is in-progress, you can still index documents and make other requests to the cluster, but new documents (and updates to existing documents) generally aren’t included in the snapshot. The snapshot includes primary shards as they existed when Elasticsearch initiated the snapshot. Depending on the size of your snapshot thread pool, different shards might be included in the snapshot at slightly different times.  Elasticsearch snapshots are incremental, meaning that they only store data that has changed since the last successful snapshot. The difference in disk usage between frequent and infrequent snapshots is often minimal.  In other words, taking hourly snapshots for a week (for a total of 168 snapshots) might not use much more disk space than taking a single snapshot at the end of the week. Also, the more frequently you take snapshots, the less time they take to complete. Some Elasticsearch users take snapshots as often as every half hour.  Register repository  Before you can take a snapshot, you have to “register” a snapshot repository. A snapshot repository is really just a storage location: a shared file system, Amazon S3, Hadoop Distributed File System (HDFS), Azure Storage, etc.  Shared file system  1. To use a shared file system as a snapshot repository, add it to elasticsearch.yml:   yml  path.repo: [&quot;/mnt/snapshots&quot;] On the RPM install, you can then mount the file system. If you’re using the Docker install, add the file system to each node in docker-compose.yml before starting the cluster:   yml  volumes:  - /Users/jdoe/snapshots:/mnt/snapshots   1. Then register the repository using the REST API:   json  PUT _snapshot/my-fs-repository  {  &quot;type&quot;: &quot;fs&quot;,  &quot;settings&quot;: { &quot;location&quot;: &quot;/mnt/snapshots&quot;  }  } If the request is successful, the response from Elasticsearch is minimal:   json  {  &quot;acknowledged&quot;: true  }   You probably only need to specify location, but to summarize the options:  Setting | Description :— | :— location | The shared file system for snapshots. Required. chunk_size | Breaks large files into chunks during snapshot operations (e.g. 64mb, 1gb), which is important for cloud storage providers and far less important for shared file systems. Default is null (unlimited). Optional. compress | Whether to compress metadata files. This setting does not affect data files, which might already be compressed (depending on your index settings). Default is false. Optional. max_restore_bytes_per_sec | The maximum rate at which snapshots restore. Default is 40 MB per second (40m). Optional. max_snapshot_bytes_per_sec | The maximum rate at which snapshots take. Default is 40 MB per second (40m). Optional. readonly | Whether the repository is read-only. Useful when migrating from one cluster (&quot;readonly&quot;: false when registering) to another cluster (&quot;readonly&quot;: true when registering). Optional.  Amazon S3  1. To use an Amazon S3 bucket as a snapshot repository, install the repository-s3 plugin on all nodes:   bash  sudo ./bin/elasticsearch-plugin install repository-s3 If you’re using the Docker installation, see Run with custom plugins. Your Dockerfile should look something like this: FROM amazon/opendistro-for-elasticsearch:0.9.0   ENV AWS_ACCESS_KEY_ID &amp;lt;access-key&amp;gt;  ENV AWS_SECRET_ACCESS_KEY &amp;lt;secret-key&amp;gt;   # Optional  ENV AWS_SESSION_TOKEN &amp;lt;optional-session-token&amp;gt;   RUN /usr/share/elasticsearch/bin/elasticsearch-plugin install --batch repository-s3  RUN /usr/share/elasticsearch/bin/elasticsearch-keystore create   RUN echo $AWS_ACCESS_KEY_ID | /usr/share/elasticsearch/bin/elasticsearch-keystore add --stdin s3.client.default.access_key  RUN echo $AWS_SECRET_ACCESS_KEY | /usr/share/elasticsearch/bin/elasticsearch-keystore add --stdin s3.client.default.secret_key   # Optional  RUN echo $AWS_SESSION_TOKEN | /usr/share/elasticsearch/bin/elasticsearch-keystore add --stdin s3.client.default.session_token After the Docker cluster starts, skip to step 7.  1. Add your AWS access and secret keys to the Elasticsearch keystore:   bash  sudo ./bin/elasticsearch-keystore add s3.client.default.access_key  sudo ./bin/elasticsearch-keystore add s3.client.default.secret_key   1. (Optional) If you’re using temporary credentials, add your session token:   bash  sudo ./bin/elasticsearch-keystore add s3.client.default.session_token   1. (Optional) If you connect to the internet through a proxy, add those credentials:   bash  sudo ./bin/elasticsearch-keystore add s3.client.default.proxy.username  sudo ./bin/elasticsearch-keystore add s3.client.default.proxy.password   1. (Optional) Add other settings to elasticsearch.yml:   yml  endpoint: s3.amazonaws.com # S3 has alternate endpoints, but you probably don&#39;t need to change this value.  protocol: https # http or https  proxy.host: my-proxy-host # the hostname for your proxy server  proxy.port: 8080 # port for your proxy server  read_timeout: 50s # the S3 connection timeout  max_retries: 3 # number of retries if a request fails  use_throttle_retries: true # whether the client should wait a progressively longer amount of time (exponential backoff) between each successive retry   1. If you changed elasticsearch.yml, you must restart each node in the cluster. Otherwise, you only need to reload secure cluster settings: POST _nodes/reload_secure_settings   1. Create an S3 bucket if you don’t already have one.  1. Register the repository using the REST API:   json  PUT _snapshot/my-s3-repository  {  &quot;type&quot;: &quot;s3&quot;,  &quot;settings&quot;: { &quot;bucket&quot;: &quot;my-s3-bucket&quot;, &quot;base_path&quot;: &quot;my/snapshot/directory&quot;  }  }   You probably don’t need to specify anything but bucket and base_path, but to summarize the options:  Setting | Description :— | :— base_path | The path within the bucket where you want to store snapshots (e.g. my/snapshot/directory). Optional. If not specified, snapshots are stored in the bucket root. bucket | Name of the S3 bucket. Required. buffer_size | The threshold beyond which chunks (of chunk_size) should be broken into pieces (of buffer_size) and sent to S3 using a different API. Default is the smaller of two values: 100 MB or 5% of the Java heap. Valid values are between 5mb and 5gb. We don’t recommend changing this option. canned_acl | S3 has several canned ACLs that the repository-s3 plugin can add to objects as it creates them in S3. Default is private. Optional. chunk_size | Breaks files into chunks during snapshot operations (e.g. 64mb, 1gb), which is important for cloud storage providers and far less important for shared file systems. Default is 1gb. Optional. client | When specifying client settings (e.g. s3.client.default.access_key), you can use a string other than default (e.g. s3.client.backup-role.access_key). If you used an alternate name, change this value to match. Default and recommended value is default. Optional. compress | Whether to compress metadata files. This setting does not affect data files, which depending on your index settings, might already be compressed. Default is false. Optional. max_restore_bytes_per_sec | The maximum rate at which snapshots restore. Default is 40 MB per second (40m). Optional. max_snapshot_bytes_per_sec | The maximum rate at which snapshots take. Default is 40 MB per second (40m). Optional. readonly | Whether the repository is read-only. Useful when migrating from one cluster (&quot;readonly&quot;: false when registering) to another cluster (&quot;readonly&quot;: true when registering). Optional. server_side_encryption | Whether to encrypt snapshot files in the S3 bucket. This setting uses AES-256 with S3-managed keys. See Protecting Data Using Server-Side Encryption. Default is false. Optional. storage_class | Specifies the S3 storage class for the snapshots files. Default is standard. Do not use the glacier and deep_archive storage classes. Optional.  Take snapshots  You specify two pieces of information when you create a snapshot:  - Name of your snapshot repository - Name for the snapshot  The following snapshot includes all indices and the cluster state:  json PUT _snapshot/my-repository/1  You can also add a request body to include or exclude certain indices or specify some other settings:  json PUT _snapshot/my-repository/2 { &quot;indices&quot;: &quot;kibana*,my-index*,-my-index-2016&quot;, &quot;ignore_unavailable&quot;: true, &quot;include_global_state&quot;: true, &quot;partial&quot;: false }  Setting | Description :— | :— indices | The indices that you want to include in the snapshot. You can use , to create a list of indices, * to specify an index pattern, and - to exclude certain indices. Don’t put spaces between items. Default is all indices. ignore_unavailable | If an index from the indices list doesn’t exist, whether to ignore it rather than fail the snapshot. Default is false. include_global_state | Whether to include cluster state in the snapshot. Default is true. partial | Whether to allow partial snapshots. Default is false, which fails the entire snapshot if one or more shards fails to store.  If you request the snapshot immediately after taking it, you might see something like:  json GET _snapshot/my-repository/2 { &quot;snapshots&quot;: [{   &quot;snapshot&quot;: &quot;2&quot;,   &quot;version&quot;: &quot;6.5.4&quot;,   &quot;indices&quot;: [   &quot;kibana_sample_data_ecommerce&quot;,   &quot;my-index&quot;,   &quot;kibana_sample_data_logs&quot;,   &quot;kibana_sample_data_flights&quot;   ],   &quot;include_global_state&quot;: true,   &quot;state&quot;: &quot;IN_PROGRESS&quot;,   ... }] }  Note that the snapshot is still in progress. If you want to wait for the snapshot to finish before continuing, add the wait_for_completion parameter to your request. Snapshots can take a while to complete, though, so consider whether or not this option fits your use case:  PUT _snapshot/my-repository/3?wait_for_completion=true  Snapshots have the following states:  State | Description :— | :— SUCCESS | The snapshot successfully stored all shards. IN_PROGRESS | The snapshot is currently running. PARTIAL | At least one shard failed to store successfully. Can only occur if you set partial to true when taking the snapshot. FAILED | The snapshot encountered an error and stored no data. INCOMPATIBLE | The snapshot is incompatible with the version of Elasticsearch running on this cluster. See Conflicts and compatibility.  Restore snapshots  The first step in restoring a snapshot is retrieving existing snapshots. To see all snapshot repositories:  GET _snapshot/_all  To see all snapshots in a repository:  GET _snapshot/my-repository/_all  Then you can restore a snapshot:  POST _snapshot/my-repository/2/_restore  Just like when taking a snapshot, you can add a request body to include or exclude certain indices or specify some other settings:  json POST _snapshot/my-repository/2/_restore { &quot;indices&quot;: &quot;kibana*,my-index*&quot;, &quot;ignore_unavailable&quot;: true, &quot;include_global_state&quot;: true, &quot;include_aliases&quot;: false, &quot;partial&quot;: false, &quot;rename_pattern&quot;: &quot;kibana(.+)&quot;, &quot;rename_replacement&quot;: &quot;restored-kibana$1&quot; }  Setting | Description :— | :— indices | The indices that you want to restore. You can use , to create a list of indices, * to specify an index pattern, and - to exclude certain indices. Don’t put spaces between items. Default is all indices. ignore_unavailable | If an index from the indices list doesn’t exist, whether to ignore it rather than fail the restore operation. Default is false. include_global_state | Whether to restore the cluster state. Default is false. include_aliases | Whether to restore aliases alongside their associated indices. Default is true. partial | Whether to allow the restoration of partial snapshots. Default is false. rename_pattern | If you want to rename indices as you restore them, use this option to specify a regular expression that matches all indices you want to restore. Use capture groups (()) to reuse portions of the index name. rename_replacement | If you want to rename indices as you restore them, use this option to specify the replacement pattern. Use $0 to include the entire matching index name, $1 to include the content of the first capture group, etc.  Conflicts and compatibility  One way to avoid naming conflicts when restoring indices is to use the rename_pattern and rename_replacement options. Then, if necessary, you can use the _reindex API to combine the two. The simpler way is to delete existing indices prior to restoring from a snapshot.  You can use the _close API to close existing indices prior to restoring from a snapshot, but the index in the snapshot has to have the same number of shards as the existing index.  We recommend ceasing write requests to a cluster before restoring from a snapshot, which helps avoid scenarios such as:  - You delete an index, which also deletes its alias. - A write request to the now-deleted alias creates a new index with the same name as the alias. - The alias from the snapshot fails to restore due to a naming conflict with the new index.  Snapshots are only forward-compatible, and only by one major version. For example, snapshots taken on a 2.x cluster can’t be restored on a 1.x cluster nor a 6.x cluster, but they can be restored on a 2.x or 5.x cluster.  If you have an old snapshot, you can sometimes restore it into an intermediate cluster, reindex all indices, take a new snapshot, and repeat until you arrive at your desired version, but you might find it easier to just manually index your data on the new cluster.",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/elasticsearch/snapshot-restore/",
      "relUrl": "/docs/elasticsearch/snapshot-restore/"
    },
  
    {
      "id": "49",
      "title": "TLS Certificates",
      "content": "Configure TLS certificates  TLS is configured in elasticsearch.yml. There are two main configuration sections: transport layer and REST layer. TLS is optional for the REST layer and mandatory for the transport layer.  You can find an example configuration template with all options on GitHub. {: .note } —    1. TOC {:toc} —   X.509 PEM certificates and PKCS #8 keys  The following tables contain the settings you can use to configure the location of your PEM certificates and private keys.  Transport layer TLS  Name | Description :— | :— opendistro_security.ssl.transport.pemkey_filepath | Path to the certificate’s key file (PKCS #8), which must be under the config/ directory, specified using a relative path. Required. opendistro_security.ssl.transport.pemkey_password | Key password. Omit this setting if the key has no password. Optional. opendistro_security.ssl.transport.pemcert_filepath | Path to the X.509 node certificate chain (PEM format), which must be under the config/ directory, specified using a relative path. Required. opendistro_security.ssl.transport.pemtrustedcas_filepath | Path to the root CA(s) (PEM format), which must be under the config/ directory, specified using a relative path . Required.  REST layer TLS  Name | Description :— | :— opendistro_security.ssl.http.pemkey_filepath | Path to the certificate’s key file (PKCS #8), which must be under the config/ directory, specified using a relative path. Required. opendistro_security.ssl.http.pemkey_password | Key password. Omit this setting if the key has no password. Optional. opendistro_security.ssl.http.pemcert_filepath | Path to the X.509 node certificate chain (PEM format), which must be under the config/ directory, specified using a relative path. Required. opendistro_security.ssl.http.pemtrustedcas_filepath | Path to the root CA(s) (PEM format), which must be under the config/ directory, specified using a relative path . Required.  Keystore and truststore files  As an alternative to certificates and private keys in PEM format, you can instead use keystore and truststore files in JKS or PKCS12/PFX format. The following settings configure the location and password of your keystore and truststore files. If desired, you can use different keystore and truststore files for the REST and the transport layer.  Transport layer TLS  Name | Description :— | :— opendistro_security.ssl.transport.keystore_type | The type of the keystore file, JKS or PKCS12/PFX. Optional. Default is JKS. opendistro_security.ssl.transport.keystore_filepath | Path to the keystore file, which must be under the config/ directory, specified using a relative path. Required. opendistro_security.ssl.transport.keystore_alias: my_alias | Alias name. Optional. Default is the first alias. opendistro_security.ssl.transport.keystore_password | Keystore password. Default is changeit. opendistro_security.ssl.transport.truststore_type | The type of the truststore file, JKS or PKCS12/PFX. Default is JKS. opendistro_security.ssl.transport.truststore_filepath | Path to the truststore file, which must be under the config/ directory, specified using a relative path. Required. opendistro_security.ssl.transport.truststore_alias | Alias name. Optional. Default is all certificates. opendistro_security.ssl.transport.truststore_password | Truststore password. Default is changeit.  REST layer TLS  Name | Description :— | :— opendistro_security.ssl.http.enabled | Whether to enable TLS on the REST layer. If enabled, only HTTPS is allowed. Optional. Default is false. opendistro_security.ssl.http.keystore_type | The type of the keystore file, JKS or PKCS12/PFX. Optional. Default is JKS. opendistro_security.ssl.http.keystore_filepath | Path to the keystore file, which must be under the config/ directory, specified using a relative path. Required. opendistro_security.ssl.http.keystore_alias | Alias name. Optional. Default is the first alias. opendistro_security.ssl.http.keystore_password | Keystore password. Default is changeit. opendistro_security.ssl.http.truststore_type | The type of the truststore file, JKS or PKCS12/PFX. Default is JKS. opendistro_security.ssl.http.truststore_filepath | Path to the truststore file, which must be under the config/ directory, specified using a relative path. Required. opendistro_security.ssl.http.truststore_alias | Alias name. Optional. Default is all certificates. opendistro_security.ssl.http.truststore_password | Truststore password. Default is changeit.  Configure node certificates  The Security plugin needs to identify inter-cluster requests (i.e. requests between the nodes). The simplest way of configuring node certificates is to list the Distinguished Names (DNs) of these certificates in elasticsearch.yml. The Security plugin supports wildcards and regular expressions:  yml opendistro_security.nodes_dn: - &#39;CN=node.other.com,OU=SSL,O=Test,L=Test,C=DE&#39; - &#39;CN=*.example.com,OU=SSL,O=Test,L=Test,C=DE&#39; - &#39;CN=elk-devcluster*&#39; - &#39;/CN=.*regex/&#39;  If your node certificates have an OID identifier in the SAN section, you can omit this configuration.  Configure admin certificates  Admin certificates are regular client certificates that have elevated rights to perform administrative tasks. You need an admin certificate to change the the Security plugin configuration using plugins/opendistro_security/tools/securityadmin.sh or the REST API. Admin certificates are configured in elasticsearch.yml by stating their DN(s):  yml opendistro_security.authcz.admin_dn: - CN=admin,OU=SSL,O=Test,L=Test,C=DE  For security reasons, you cannot use wildcards nor regular expressions here.  OpenSSL  The Security plugin supports OpenSSL. We recommend OpenSSL in production for enhanced performance and a wider range of modern cipher suites. In order to use OpenSSL, you need to install OpenSSL, the Apache Portable Runtime, and a Netty version with OpenSSL support matching your platform on all nodes.  If OpenSSL is enabled, but for one reason or another the installation does not work, the Security plugin falls back to the Java JCE as the security engine.  Name | Description :— | :— opendistro_security.ssl.transport.enable_openssl_if_available | Enable OpenSSL on the transport layer if available. Optional. Default is true. opendistro_security.ssl.http.enable_openssl_if_available | Enable OpenSSL on the REST layer if available. Optional. Default is true.  1. Install OpenSSL 1.1.0 on every node. 1. Install Apache Portable Runtime  on every node:  sudo yum install apr  1. Download netty-tcnative for RPM-based distributions (_linux-x86_64-fedora.jar_) and place it into plugins/opendistro_security/ on every node.  (Advanced) Hostname verification and DNS lookup  In addition to verifying the TLS certificates against the root CA and/or intermediate CA(s), the Security plugin can apply additional checks on the transport layer.  With enforce_hostname_verification enabled, the Security plugin verifies that the hostname of the communication partner matches the hostname in the certificate. The hostname is taken from the subject or SAN entries of your certificate. For example, if the hostname of your node is node-0.example.com, then the hostname in the TLS certificate has to be set to node-0.example.com, as well. Otherwise, errors are thrown:  [ERROR][c.a.o.s.s.t.OpenDistroSecuritySSLNettyTransport] [WX6omJY] SSL Problem No name matching &amp;lt;hostname&amp;gt; found [ERROR][c.a.o.s.s.t.OpenDistroSecuritySSLNettyTransport] [WX6omJY] SSL Problem Received fatal alert: certificate_unknown  In addition, when resolve_hostnames is enabled, the Security plugin resolves the (verified) hostname against your DNS. If the hostname does not resolve, errors are thrown: Name | Description :— | :— opendistro_security.ssl.transport.enforce_hostname_verification | Whether to verify hostnames on the transport layer. Optional. Default is true. opendistro_security.ssl.transport.resolve_hostname | Whether to resolve hostnames against DNS on the transport layer. Optional. Default is true. Only works if hostname verification is also enabled.  (Advanced) Client authentication  With TLS client authentication enabled, REST clients can send a TLS certificate with the HTTP request to provide identity information to the Security plugin. There are three main usage scenarios for TLS client authentication:  - Providing an admin certificate when using the REST management API. - Configuring roles and permissions based on a client certificate. - Providing identity information for tools like Kibana, Logstash or Beats.  TLS client authentication has three modes:  * NONE: The Security plugin does not accept TLS client certificates. If one is sent, it is discarded. * OPTIONAL: The Security plugin accepts TLS client certificates if they are sent, but does not require them. * REQUIRE: The Security plugin only accepts REST requests when a valid client TLS certificate is sent.  For the REST management API, the client authentication modes has to be OPTIONAL at a minimum.  You can configure the client authentication mode by using the following setting:  Name | Description :— | :— opendistro_security.ssl.http.clientauth_mode | The TLS client authentication mode to use. Can be one of NONE, OPTIONAL (default) or REQUIRE. Optional.  (Advanced) Enabled ciphers and protocols  You can limit the allowed ciphers and TLS protocols for the REST layer. For example, you can only allow strong ciphers and limit the TLS versions to the most recent ones.  If this setting is not enabled, the ciphers and TLS versions are negotiated between the browser and the Security plugin automatically, which in some cases can lead to a weaker cipher suite being used. You can configure the ciphers and protocols using the following settings.  Name | Description :— | :— opendistro_security.ssl.http.enabled_ciphers | Array, enabled TLS cipher suites for the REST layer. Only Java format is supported. opendistro_security.ssl.http.enabled_protocols | Array, enabled TLS protocols for the REST layer. Only Java format is supported. opendistro_security.ssl.transport.enabled_ciphers | Array, enabled TLS cipher suites for the transport layer. Only Java format is supported. opendistro_security.ssl.transport.enabled_protocols | Array, enabled TLS protocols for the transport layer. Only Java format is supported.   Example settings  yml opendistro_security.ssl.http.enabled_ciphers: - &quot;TLS_DHE_RSA_WITH_AES_256_CBC_SHA&quot; - &quot;TLS_DHE_DSS_WITH_AES_128_CBC_SHA256&quot; opendistro_security.ssl.http.enabled_protocols: - &quot;TLSv1.1&quot; - &quot;TLSv1.2&quot;  Because it is insecure, the Security plugin disables TLSv1 by default. If you need to use TLSv1 and accept the risks, you can still enable it:  yml opendistro_security.ssl.http.enabled_protocols: - &quot;TLSv1&quot; - &quot;TLSv1.1&quot; - &quot;TLSv1.2&quot;  (Advanced) Disable client initiated renegotiation for Java 8  Set -Djdk.tls.rejectClientInitiatedRenegotiation=true to disable secure client initiated renegotiation, which is enabled by default. This can be set via ES_JAVA_OPTS in config/jvm.options.",
      "url": "https://opendistro.github.io/for-elasticsearch-docs/old/0.9.0/docs/security/tls-configuration/",
      "relUrl": "/docs/security/tls-configuration/"
    }
  
];
